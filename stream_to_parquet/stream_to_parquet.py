# Generated by nuclio.export.NuclioExporter

import os
import pandas as pd
import numpy as np
import json
import datetime
import mlrun


def record_to_features(record):
    features = record["request"]["instances"][0]
    timestamp = record["when"]
    prediction = record["resp"]

    record = {"timestamp": timestamp, **features, "predictions": prediction}

    return record


def init_context(context):
    setattr(context, "batch", [])
    setattr(context, "window", int(os.getenv("window", 10)))
    setattr(context, "save_to", os.getenv("save_to", "/bigdata/inference_pq/"))
    os.makedirs(context.save_to, exist_ok=True)

    mlrun.mlconf.dbpath = mlrun.mlconf.dbpath or "http://mlrun-api:8080"
    artifact_path = os.getenv("artifact_path", None)
    if artifact_path:
        mlrun.mlconf.artifact_path = artifact_path
    if "hub_url" in os.environ:
        mlrun.mlconf.hub_url = os.environ["hub_url"]
    virtual_drift_fn = mlrun.import_function("hub://virtual_drift")
    virtual_drift_fn.apply(mlrun.auto_mount())
    setattr(context, "virtual_drift_fn", virtual_drift_fn)

    predictions_col = os.getenv("predictions", None)
    label_col = os.getenv("label_col", None)
    setattr(context, "base_dataset", os.getenv("base_dataset", ""))
    setattr(context, "indexes", json.loads(os.environ.get("indexes", "[]")))
    setattr(context, "predictions_col", predictions_col)
    setattr(context, "label_col", label_col)
    setattr(
        context, "results_tsdb_container", os.getenv("results_tsdb_container", None)
    )
    setattr(context, "results_tsdb_table", os.getenv("results_tsdb_table", None))


def handler(context, event):

    context.logger.info(f"Adding {event.body}")
    context.batch.append(record_to_features(json.loads(event.body)))

    if len(context.batch) > context.window:
        context.logger.info(context.batch[:1])
        context.logger.info(context.indexes)
        df = pd.DataFrame(context.batch)
        context.logger.info(f"df example: {df.head(1)}")
        if context.indexes:
            df = df.set_index(context.indexes)
        df_path = os.path.join(
            context.save_to,
            f"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}.pq",
        )
        df.to_parquet(df_path,index=False)

        task = mlrun.NewTask(
            name="drift_magnitude",
            handler="drift_magnitude",
            params={
                "label_col": context.label_col,
                "prediction_col": context.predictions_col,
                "results_tsdb_container": context.results_tsdb_container,
                "results_tsdb_table": context.results_tsdb_table,
            },
            inputs={"t": context.base_dataset, "u": df_path},
            artifact_path=mlrun.mlconf.artifact_path,
        )

        context.virtual_drift_fn.run(task, watch=False)

        context.batch = []
