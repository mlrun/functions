{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a Serverless XGBoost Model Server\n",
    "  --------------------------------------------------------------------\n",
    "\n",
    "The following notebook demonstrates how to deploy an XGBoost model server (a.k.a <b>Nuclio-serving</b>)\n",
    "\n",
    "#### **notebook how-to's**\n",
    "* Write and test model serving class in a notebook.\n",
    "* Deploy the model server function.\n",
    "* Invoke and test the serving function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "#### **steps**\n",
    "**[define a new function and its dependencies](#define-function)**<br>\n",
    "**[test the model serving class locally](#test-locally)**<br>\n",
    "**[deploy our serving class using as a serverless function](#deploy)**<br>\n",
    "**[test our model server using HTTP request](#test-model-server)**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='define-function'></a>\n",
    "### **define a new function and its dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting spec.build.baseImage to 'mlrun/ml-serving:0.4.7'\n"
     ]
    }
   ],
   "source": [
    "%nuclio config spec.build.baseImage = \"mlrun/ml-serving:0.4.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nuclio cmd -c\n",
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200426 13:21:44 utils:129] Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[I 200426 13:21:44 utils:141] NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import kfserving\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Serving Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostModel(kfserving.KFModel):\n",
    "    def __init__(self, name: str, model_dir: str):\n",
    "        self.name = name\n",
    "        self.model_filepath = model_dir\n",
    "        self._booster = None\n",
    "        self.ready = None\n",
    "\n",
    "    def load(self):\n",
    "        self._booster = xgb.Booster(model_file=self.model_filepath)\n",
    "        self.ready = True\n",
    "    \n",
    "    def predict(self, body):\n",
    "        try:\n",
    "            # Use of list as input for XGBoost.DMatrix is deprecated \n",
    "            # see https://github.com/dmlc/xgboost/pull/3970\n",
    "            # dtype should be ....\n",
    "            np_array_2d = np.asarray(body['instances'], dtype=np.float32).reshape(-1,1)\n",
    "            dmatrix = xgb.DMatrix(np_array_2d)\n",
    "            result: xgb.DMatrix = self._booster.predict(dmatrix)\n",
    "            return result.tolist()\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Failed to predict %s\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following end-code annotation tells ```nuclio``` to stop parsing the notebook from this cell. _**Please do not remove this cell**_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test-locally'></a>\n",
    "## Test the function locally\n",
    "\n",
    "The class above can be tested locally. Just instantiate the class, `.load()` will load the model to a local dir.\n",
    "\n",
    "> **Verify there is a `model.bst` file in the model_dir path (generated by the training notebook)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/User/artifacts/xgb_serving/model.bst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_server = XGBoostModel('my-model', model_dir=model_dir)\n",
    "my_server.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can use the `.predict(body)` method to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:21:50] WARNING: /workspace/src/learner.cc:979: Number of columns does not match number of features in booster. Columns: 1 Features: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.7837269902229309,\n",
       "  0.025536756962537766,\n",
       "  0.03395244851708412,\n",
       "  0.02239767648279667,\n",
       "  0.02239767648279667,\n",
       "  0.02239767648279667,\n",
       "  0.02239767648279667,\n",
       "  0.02239767648279667,\n",
       "  0.02239767648279667,\n",
       "  0.02239767648279667],\n",
       " [0.7837269902229309,\n",
       "  0.025536756962537766,\n",
       "  0.03395244851708412,\n",
       "  0.02239767648279667,\n",
       "  0.02239767648279667,\n",
       "  0.02239767648279667,\n",
       "  0.02239767648279667,\n",
       "  0.02239767648279667,\n",
       "  0.02239767648279667,\n",
       "  0.02239767648279667]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "my_server.predict({\"instances\":[[5], [10]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import mlconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://mlrun-api:8080'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlconf.dbpath = mlconf.dbpath or './'\n",
    "mlconf.dbpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/User/repos/functions/{name}/function.yaml'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vcs_branch = 'development'\n",
    "base_vcs = f'https://raw.githubusercontent.com/mlrun/functions/{vcs_branch}/'\n",
    "\n",
    "mlconf.hub_url = mlconf.hub_url or base_vcs + f'{name}/function.yaml'\n",
    "mlconf.hub_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/User/artifacts'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "mlconf.artifact_path = mlconf.artifact_path or f'{os.environ[\"V3IO_HOME\"]}/artifacts'\n",
    "mlconf.artifact_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deploy'></a>\n",
    "### **deploy our serving class using as a serverless function**\n",
    "in the following section we create a new model serving function which wraps our class , and specify model and other resources.\n",
    "\n",
    "the `models` dict store model names and the assosiated model **dir** URL (the URL can start with `S3://` and other blob store options), the faster way is to use a shared file volume, we use `.apply(mount_v3io())` to attach a v3io (iguazio data fabric) volume to our function. By default v3io will mount the current user home into the `\\User` function path.\n",
    "\n",
    "**verify the model dir does contain a valid `model.bst` file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import new_model_server, mount_v3io\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-04-26 13:22:02,033 saving function: iris-srv, tag: latest\n",
      "[mlrun] 2020-04-26 13:22:02,055 function spec saved to path: function.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7f32da19fbb0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = new_model_server('iris-srv',\n",
    "                      model_class='XGBoostModel',\n",
    "                      models={'iris_v1': '/User/artifacts/xgb_serving/model.bst'})\n",
    "fn.spec.description = \"xgboost iris classification server\"\n",
    "fn.metadata.categories = ['serving', 'models']\n",
    "fn.metadata.labels = {'author': 'yaronh'}\n",
    "fn.spec.imagePullPolicy = \"Always\"\n",
    "\n",
    "fn.save()\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"V3IO_HOME\" in list(os.environ):\n",
    "    from mlrun import mount_v3io\n",
    "    fn.apply(mount_v3io())\n",
    "else:\n",
    "    # is you set up mlrun using the instructions at\n",
    "    # https://github.com/mlrun/mlrun/blob/master/hack/local/README.md\n",
    "    from mlrun.platforms import mount_pvc\n",
    "    fn.apply(mount_pvc('nfsvol', 'nfsvol', '/home/joyan/data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-04-26 13:22:02,102 deploy started\n",
      "[nuclio] 2020-04-26 13:22:05,190 (info) Build complete\n",
      "[nuclio] 2020-04-26 13:22:11,261 (info) Function deploy complete\n",
      "[nuclio] 2020-04-26 13:22:11,266 done updating iris-srv, function address: 18.221.173.138:32234\n"
     ]
    }
   ],
   "source": [
    "addr = fn.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"test-model-server\"></a>\n",
    "### **test our model server using HTTP request**\n",
    "\n",
    "\n",
    "We invoke our model serving function using test data, the data vector is specified in the `instances` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFServing protocol event\n",
    "event_data = {\"instances\": [[5], [10]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7837269902229309, 0.025536756962537766, 0.03395244851708412, 0.02239767648279667, 0.02239767648279667, 0.02239767648279667, 0.02239767648279667, 0.02239767648279667, 0.02239767648279667, 0.02239767648279667], [0.7837269902229309, 0.025536756962537766, 0.03395244851708412, 0.02239767648279667, 0.02239767648279667, 0.02239767648279667, 0.02239767648279667, 0.02239767648279667, 0.02239767648279667, 0.02239767648279667]]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "resp = requests.put(addr + '/iris_v1/predict', json=json.dumps(event_data))\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[back to top](#top)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-4.7",
   "language": "python",
   "name": "mlrun-4.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
