{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import importlib\n",
    "from cloudpickle import load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from mlrun.execution import MLClientCtx\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun.artifacts import TableArtifact, PlotArtifact\n",
    "\n",
    "from mlutils.models import get_model_configs\n",
    "from mlutils.plots import plot_roc, plot_importance, gcf_clear\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "def _gcf_clear(plt):\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()        \n",
    "\n",
    "def test_classifier(\n",
    "    context: MLClientCtx,\n",
    "    models_dir: str, \n",
    "    test_set: str,\n",
    "    label_column: str,\n",
    "    score_method: str = 'micro',\n",
    "    key: str = \"\",\n",
    "    plots_dest: str = \"plots\"\n",
    ") -> None:\n",
    "    \"\"\"Test one or more classifier models against held-out dataset\n",
    "    \n",
    "    Using held-out test features, evaluates the peformance of the estimated model\n",
    "    \n",
    "    Can be part of a kubeflow pipeline as a test step that is run post EDA and \n",
    "    training/validation cycles\n",
    "    \n",
    "    :param context:         the function context\n",
    "    :param models_dir:      artifact models representing a folder or a folder\n",
    "    :param test_set:        test features and labels\n",
    "    :param label_column:    column name for ground truth labels\n",
    "    :param score_method:    for multiclass classification\n",
    "    :param key:             key for results artifact (maybe just a dir of artifacts for test like plots_dest)\n",
    "    :param plots_dest:       dir for test plots\n",
    "    \"\"\"\n",
    "    xtest = pd.read_parquet(str(test_set))\n",
    "    ytest = xtest.pop(label_column)\n",
    "    \n",
    "    context.header = list(xtest.columns.values)\n",
    "    \n",
    "    def _eval_model(model):\n",
    "        # enclose all except model\n",
    "        ytestb = label_binarize(ytest, classes=ytest.unique())\n",
    "        clf = load(open(os.path.join(str(models_dir), \"model\")+\".pkl\", \"rb\"))\n",
    "        if callable(getattr(clf, \"predict_proba\")):\n",
    "            y_score = clf.predict_proba(xtest.values)\n",
    "            ypred = clf.predict(xtest.values)\n",
    "            context.logger.info(f\"y_score.shape {y_score.shape}\")\n",
    "            context.logger.info(f\"ytestb.shape {ytestb.shape}\")\n",
    "            plot_roc(context, ytestb, y_score, key=f\"roc\", plots_dir=plots_dest)\n",
    "        else:\n",
    "            ypred = clf.predict(xtest.values) # refactor\n",
    "            y_score = None\n",
    "            \n",
    "        gcf_clear(plt)\n",
    "        # use sklearn >= v0.22 built in:\n",
    "       \n",
    "        metrics.plot_confusion_matrix(clf, xtest, ytest, \n",
    "                                      labels=ytest.unique(), normalize='true') \n",
    "        \n",
    "        context.log_artifact(PlotArtifact(\"confusion\", body=plt.gcf()), \n",
    "                             local_path=f\"{plots_dest}/confusion.html\")        \n",
    "    \n",
    "        if hasattr(clf, \"feature_importances_\"):\n",
    "            plot_importance(context, clf, key=f\"featimp\")\n",
    "        average_precision = metrics.average_precision_score(ytestb[:,:-1], y_score, average=score_method)\n",
    "        context.log_result(f\"accuracy\", float(clf.score(xtest.values, ytest.values)))\n",
    "        context.log_result(f\"rocauc\", metrics.roc_auc_score(ytestb, y_score))\n",
    "        context.log_result(f\"f1_score\", metrics.f1_score(ytest.values, ypred, average=score_method))\n",
    "        context.log_result(f\"avg_precscore\", average_precision)\n",
    "    \n",
    "    best_model = None\n",
    "    for model in os.listdir(str(models_dir)):\n",
    "        if model.endswith('.pkl'):\n",
    "            _eval_model(model)\n",
    "            # HACK: there is only one model here\n",
    "            best_model = model\n",
    "\n",
    "    # log 'best model' as artifact\n",
    "    context.log_artifact('TODAYS-MODELS-TEST-REPORT', local_path=best_model)\n",
    "    context.log_artifact('DEPLOY', body=b'true', local_path='DEPLOY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function \n",
    "# create job function object from notebook code\n",
    "fn = code_to_function(\"test_classifier\", kind=\"job\", with_doc=True,\n",
    "                      handler=test_classifier, image=\"mlrun/ml-models\")\n",
    "\n",
    "# add metadata (for templates and reuse)\n",
    "fn.spec.default_handler = \"test_classifier\"\n",
    "fn.spec.description = \"test a classifier using held-out or new data\"\n",
    "fn.metadata.categories = [\"models\", \"testing\"]\n",
    "fn.spec.image_pull_policy = \"Always\"\n",
    "fn.metadata.labels = {\"author\": \"yjb\"}\n",
    "\n",
    "fn.save()\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import import_function, mount_v3io\n",
    "\n",
    "func = import_function(\"hub://test_classifier\").apply(mount_v3io())\n",
    "# func = import_function(\"function.yaml\").apply(mlrun.mount_v3io())\n",
    "\n",
    "task_params = {\n",
    "    \"name\" : \"tasks - test classifier\",\n",
    "    \"params\": {\n",
    "        # Ina pipeline setting, the models_dir parameter would be the output of a training step\n",
    "        \"models_dir\"    : \"/User/artifacts/models\",\n",
    "        \"test_set\"      : \"/User/artifacts/test_set.parquet\",\n",
    "        \"label_column\"  : \"labels\"}}\n",
    "\n",
    "from mlrun import NewTask\n",
    "run = func.run(NewTask(**task_params), artifact_path=\"/User/artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-v0.4.6",
   "language": "python",
   "name": "mlrun-v0.4.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
