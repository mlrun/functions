{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aecdfc37-4b22-4bb1-b052-930bcf7ec109",
   "metadata": {},
   "source": [
    "#### Configure mlrun project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be42e7c5-b2af-476f-8041-c17be56edb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-12-03 07:17:36,530 [info] Project loaded successfully: {\"project_name\":\"langchain-example-10\"}\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import mlrun\n",
    "from mlrun import get_or_create_project\n",
    "\n",
    "image = \"mlrun/mlrun\"\n",
    "project_name = \"langchain-example\"\n",
    "project = get_or_create_project(project_name, context=\"./\", allow_cross_project=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c136d-7b21-4191-81a9-7bd353cd2c9f",
   "metadata": {},
   "source": [
    "#### Create openai secret:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a47d7789-2ea2-493e-8905-f53b978e2abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create project secrets for project\n",
    "secrets = {\"OPENAI_API_KEY\": \"\", # add your OpenAI API key here\n",
    "          \"OPENAI_BASE_URL\": \"\" # add your OpenAI base url here if needed\n",
    "          }\n",
    "project.set_secrets(secrets=secrets, provider=\"kubernetes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0eaf4a-fb29-4a50-97de-8b60b26ae34b",
   "metadata": {},
   "source": [
    "#### Write your python file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cbd982-86de-43b5-91ef-24fc60b2d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile langchain_model.py\n",
    "\n",
    "# Langchain impoets for Agent initialization and use:\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
    "from langchain_core.tracers.langchain import wait_for_all_tracers\n",
    "\n",
    "\n",
    "# mlrun imports:\n",
    "import mlrun\n",
    "from mlrun.serving import Model\n",
    "\n",
    "# General imports:\n",
    "from typing import Any\n",
    "import asyncio\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Use this tool to evaluate a mathematical expression.\n",
    "    It can handle addition, mutliplication, subtraction, division and exponents.\n",
    "    Example: `calculator(\"2 + 2\")` or `calculator('3**4')`\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except Exception as e:\n",
    "        return f\"Error evaluating expression: {e}\"\n",
    "\n",
    "\n",
    "\n",
    "class LangchainWrapper(Model):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            *args,\n",
    "            prompt_template:str,\n",
    "            **kwargs\n",
    "    ) -> None:\n",
    "        self.prompt_template = prompt_template\n",
    "        super().__init__(**kwargs)\n",
    "        self.executor = None\n",
    "\n",
    "    def load(self):\n",
    "        if not self.executor:\n",
    "            self.openai_cb = UsageMetadataCallbackHandler()\n",
    "            model = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=mlrun.get_secret_or_env(\"OPENAI_API_KEY\"),\n",
    "                               openai_api_base=mlrun.get_secret_or_env(\"OPENAI_BASE_URL\"), temperature=0, model_kwargs={\n",
    "                    \"stream_options\": {\"include_usage\": True}\n",
    "                }\n",
    "                              )\n",
    "            agent = create_tool_calling_agent(llm=model, tools=[calculator], prompt=PromptTemplate.from_template(self.prompt_template))\n",
    "            self.executor = AgentExecutor(\n",
    "                agent=agent,\n",
    "                tools=[calculator],\n",
    "                handle_parsing_errors=True,\n",
    "                handle_intermediate_steps=True,\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "\n",
    "    def predict(self, body: Any, **kwargs) -> Any:\n",
    "        if not self.executor:\n",
    "            raise RuntimeError(\"Model not loaded. Call load() before predict().\")\n",
    "\n",
    "        print(f\"Invoking Agent with {body}\")\n",
    "\n",
    "        result = self.executor.invoke(\n",
    "                {\"input\": body},\n",
    "                config={\n",
    "                    \"max_iterations\": 100,\n",
    "                    \"callbacks\": [self.openai_cb],\n",
    "                    \"run_name\": \"my_run\",\n",
    "                    \"metadata\": {\"request_id\": \"123\"},\n",
    "                    },\n",
    "                return_intermediate_steps=True\n",
    "                )\n",
    "\n",
    "        print(f\"Extracting metrics {self.openai_cb.usage_metadata}\")\n",
    "\n",
    "        # Get usage from callback after invoke\n",
    "        tokens_dict = self.openai_cb.usage_metadata\n",
    "        tokens_metrics = {}\n",
    "        # Extract for specific model or all models\n",
    "        if tokens_dict:\n",
    "            token_metrics = self.extract_token_metrics(tokens_dict)\n",
    "\n",
    "        wait_for_all_tracers()\n",
    "\n",
    "        result_body = {\"output\": result[\"output\"]}\n",
    "        result_body.update(token_metrics)\n",
    "\n",
    "        return {\"outputs\": result_body}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_token_metrics(usage_metadata: dict) -> dict:\n",
    "        result = {\n",
    "            \"total_tokens\": 0,\n",
    "            \"prompt_tokens\": 0,\n",
    "            \"completion_tokens\": 0,\n",
    "            \"successful_requests\": 0,\n",
    "            \"total_cost_usd\": 0.0,\n",
    "        }\n",
    "        if not usage_metadata:\n",
    "            return result\n",
    "\n",
    "        # Get first (and only) model's metrics\n",
    "        metrics = next(iter(usage_metadata.values()))\n",
    "\n",
    "        input_tokens = metrics.get('input_tokens', 0)\n",
    "        output_tokens = metrics.get('output_tokens', 0)\n",
    "        total = metrics.get('total_tokens', 0)\n",
    "\n",
    "        # gpt-4o-mini pricing: $0.15 per 1M input, $0.60 per 1M output\n",
    "        input_cost = input_tokens * (0.15 / 1_000_000)\n",
    "        output_cost = output_tokens * (0.60 / 1_000_000)\n",
    "\n",
    "        result[\"total_tokens\"] = total\n",
    "        result[\"prompt_tokens\"] = input_tokens\n",
    "        result[\"completion_tokens\"] = output_tokens\n",
    "        result[\"successful_requests\"] = 1\n",
    "        result[\"total_cost_usd\"] = input_cost + output_cost\n",
    "        return result\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e6b930-4692-4ba3-b9fd-3ba3a4aa064b",
   "metadata": {},
   "source": [
    "#### Import the module from the hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "691e9068-ec9c-40d6-9ac8-e6c3e605b44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-12-03 10:55:46,194 [info] Project loaded successfully: {\"project_name\":\"langchain-example-10\"}\n",
      "> 2025-12-03 10:55:46,463 [info] Model monitoring credentials were set successfully. Please keep in mind that if you already had model monitoring functions / model monitoring infra / tracked model server deployed on your project, you will need to redeploy them. For redeploying the model monitoring infra, first disable it using `project.disable_model_monitoring()` and then enable it using `project.enable_model_monitoring()`.\n",
      "details: MLRunConflictError(\"The following model-montioring infrastructure functions are already deployed, aborting: ['model-monitoring-controller', 'model-monitoring-writer']\\nIf you want to redeploy the model-monitoring controller (maybe with different base-period), use update_model_monitoring_controller.If you want to redeploy all of model-monitoring infrastructure, call disable_model_monitoringbefore calling enable_model_monitoring again.\")\n"
     ]
    }
   ],
   "source": [
    "module = mlrun.import_module(\"hub://agent_deployer\")\n",
    "\n",
    "agent = module.AgentDeployer(\n",
    "            agent_name=\"langchain_agent\",\n",
    "            model_class_name=\"LangchainWrapper\",\n",
    "            function=\"langchain_model.py\",\n",
    "            result_path=\"outputs\",\n",
    "            output_schema=[\"output\" ,\"total_tokens\", \"prompt_tokens\", \"completion_tokens\", \"successful_requests\", \"total_cost_usd\"],\n",
    "            requirements=[\"langchain==0.3.7\", \"langchain-openai==0.2.5\", \"langchain-community==0.3.3\"],\n",
    "            set_model_monitoring=True,\n",
    "            prompt_template= \"\"\"\n",
    "            Answer the following questions as best you can.\n",
    "            You have access to the following tools:\n",
    "            {tools}\n",
    "            Use the following format:\n",
    "            Question: the input question you must answer\n",
    "            Thought: you should always think about what to do\n",
    "            Action: the action to take, should be one of [{tool_names}]\n",
    "            Action Input: the input to the action\n",
    "            Observation: the result of the action\n",
    "            ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "            Thought: I now know the final answer\n",
    "            Final Answer: the final answer to the original input question\n",
    "\n",
    "            Begin!\n",
    "            Question: {input}\n",
    "            Thought:{agent_scratchpad}\n",
    "            \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0bb1c4d1-5d7c-4d1c-bf51-8f53b319e91f",
   "metadata": {},
   "outputs": [],
   "source": "func = agent.deploy_function(enable_tracking=True)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Invoke the deployed agent:",
   "id": "f7b87b6ebed2ce57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "func.invoke(\"./\", {\"question\" : \"If a pizza costs $18.75 and I want to buy 3, plus a 15% tip, what is the total cost?\"})",
   "id": "ac5c3ba174d2cf8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Write your monitoring application:\n",
    "This part is optional. It allows you to see detailed monitoring metrics about the LLM usage in the MLRun UI."
   ],
   "id": "de55715ab419841d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%writefile monitoring_application.py\n",
    "\n",
    "\n",
    "from typing import Any, Union\n",
    "import pandas as pd\n",
    "import mlrun.model_monitoring.applications.context as mm_context\n",
    "from mlrun.common.schemas.model_monitoring.constants import (\n",
    "    ResultKindApp,\n",
    "    ResultStatusApp,\n",
    ")\n",
    "from mlrun.model_monitoring.applications import (\n",
    "    ModelMonitoringApplicationBase,\n",
    "    ModelMonitoringApplicationResult,\n",
    "    ModelMonitoringApplicationMetric,\n",
    ")\n",
    "\n",
    "\n",
    "class ModelMonitoringApplication(ModelMonitoringApplicationBase):\n",
    "    name = \"LLModelMonitoringApplication\"\n",
    "\n",
    "    def do_tracking(\n",
    "        self,\n",
    "        monitoring_context: mm_context.MonitoringApplicationContext,\n",
    "    ) -> list[Union[ModelMonitoringApplicationResult,ModelMonitoringApplicationMetric]]:\n",
    "        \"\"\"\"\"\"\n",
    "        df = monitoring_context.sample_df\n",
    "        if df.empty:\n",
    "            monitoring_context.logger.warning(\n",
    "                \"Empty dataframe received, skipping tracking\"\n",
    "            )\n",
    "            return [], []\n",
    "        # Example of processing the dataframe and creating results\n",
    "        results = []\n",
    "        metrics = []\n",
    "        # Calculate max, min, avg, and std for the 'usage'\n",
    "        for column in [\"completion_tokens\", \"prompt_tokens\", \"total_tokens\"]:\n",
    "            if column in df.columns:\n",
    "                stats = self._calculate_max_min_avg_std(df, column)\n",
    "                results.append(\n",
    "                    self._create_result(\n",
    "                        name=f\"{column}_stats\",\n",
    "                        value=stats[\"avg\"],\n",
    "                        kind=ResultKindApp.model_performance,\n",
    "                        threshold=1000,  # Example threshold\n",
    "                    )\n",
    "                )\n",
    "                metrics.append(\n",
    "                    self._create_metric(\n",
    "                        name=f\"{column}_max\",\n",
    "                        value=stats[\"max\"],\n",
    "                    )\n",
    "                )\n",
    "                metrics.append(\n",
    "                    self._create_metric(\n",
    "                        name=f\"{column}_min\",\n",
    "                        value=stats[\"min\"],\n",
    "                    )\n",
    "                )\n",
    "                metrics.append(\n",
    "                    self._create_metric(\n",
    "                        name=f\"{column}_std\",\n",
    "                        value=stats[\"std\"],\n",
    "                    )\n",
    "                )\n",
    "        return results + metrics\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_max_min_avg_std(df: pd.DataFrame, column: str) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate max, min, avg, and std for a given column in the dataframe.\n",
    "        \"\"\"\n",
    "        if column not in df.columns:\n",
    "            raise ValueError(f\"Column '{column}' does not exist in the dataframe.\")\n",
    "\n",
    "        return {\n",
    "            \"max\": df[column].max(),\n",
    "            \"min\": df[column].min(),\n",
    "            \"avg\": df[column].mean(),\n",
    "            \"std\": df[column].std(),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_result(\n",
    "        name: str,\n",
    "        value: float,\n",
    "        kind: ResultKindApp,\n",
    "        threshold: float,\n",
    "    ) -> ModelMonitoringApplicationResult:\n",
    "        status = ResultStatusApp.no_detection\n",
    "        if value > threshold:\n",
    "            status = ResultStatusApp.detected\n",
    "        return ModelMonitoringApplicationResult(\n",
    "            name=name,\n",
    "            value=value,\n",
    "            kind=kind,\n",
    "            status=status,\n",
    "            extra_data={\n",
    "                \"threshold\": threshold,\n",
    "                \"value\": value,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_metric(\n",
    "        name: str,\n",
    "        value: float,\n",
    "    ) -> ModelMonitoringApplicationMetric:\n",
    "        return ModelMonitoringApplicationMetric(\n",
    "            name=name,\n",
    "            value=value,\n",
    "        )\n"
   ],
   "id": "377487422f5ed289"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Deploy the monitoring application:",
   "id": "f76998dba7ac301a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "llm_monitoring_app = project.set_model_monitoring_function(\n",
    "    func=\"monitoring_application.py\",\n",
    "    application_class=\"ModelMonitoringApplication\",\n",
    "    name=\"llm-monitoring\",\n",
    "    image=image,\n",
    ")\n",
    "\n",
    "project.deploy_function(llm_monitoring_app)"
   ],
   "id": "9d6ad2a4a47a44bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-base-py311",
   "language": "python",
   "name": "conda-env-mlrun-base-py311-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
