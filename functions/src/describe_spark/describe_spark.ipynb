{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting kind to 'job'\n",
      "%nuclio: setting spec.image to 'iguazio/shell:3.0_b5565_20201026062233_wsdf'\n"
     ]
    }
   ],
   "source": [
    "%nuclio config kind = \"job\"\n",
    "%nuclio config spec.image = \"iguazio/shell:3.0_b5565_20201026062233_wsdf\" # docker image available on idan707/spark_shell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "from mlrun.platforms.iguazio import mount_v3io, mount_v3iod\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun.execution import MLClientCtx\n",
    "\n",
    "import os\n",
    "from subprocess import run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Spark Describe Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import base64 as b64\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from itertools import product\n",
    "import matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from pkg_resources import resource_filename\n",
    "import six\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql.functions import (abs as df_abs, col, count, countDistinct,\n",
    "                                   max as df_max, mean, min as df_min,\n",
    "                                   sum as df_sum, when\n",
    "                                   )\n",
    "from pyspark.sql.functions import variance, stddev, kurtosis, skewness\n",
    "\n",
    "\n",
    "def describe(df, bins, corr_reject, config, **kwargs):\n",
    "    if not isinstance(df, SparkDataFrame):\n",
    "        raise TypeError(\"df must be of type pyspark.sql.DataFrame\")\n",
    "\n",
    "    # Number of rows:\n",
    "    table_stats = {\"n\": df.count()}\n",
    "    if table_stats[\"n\"] == 0:\n",
    "        raise ValueError(\"df cannot be empty\")\n",
    "\n",
    "    try:\n",
    "        matplotlib.style.use(\"default\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Function to \"pretty name\" floats:\n",
    "    def pretty_name(x):\n",
    "        x *= 100\n",
    "        if x == int(x):\n",
    "            return '%.0f%%' % x\n",
    "        else:\n",
    "            return '%.1f%%' % x\n",
    "\n",
    "    def corr_matrix(df, columns=None):\n",
    "        if columns is None:\n",
    "            columns = df.columns\n",
    "        combinations = list(product(columns,columns))\n",
    "\n",
    "        def separate(l, n):\n",
    "            for i in range(0, len(l), n):\n",
    "                yield l[i:i+n]\n",
    "\n",
    "        grouped = list(separate(combinations,len(columns)))\n",
    "        df_cleaned = df.select(*columns).na.drop(how=\"any\")\n",
    "\n",
    "        for i in grouped:\n",
    "            for j in enumerate(i):\n",
    "                i[j[0]] = i[j[0]] + (df_cleaned.corr(str(j[1][0]), str(j[1][1])),)\n",
    "\n",
    "        df_pandas = pd.DataFrame(grouped).applymap(lambda x: x[2])\n",
    "        df_pandas.columns = columns\n",
    "        df_pandas.index = columns\n",
    "        \n",
    "        return df_pandas\n",
    "\n",
    "    def create_hist_data(df, column, minim, maxim, bins=10):\n",
    "\n",
    "        def create_all_conditions(current_col, column, left_edges, count=1):\n",
    "            \"\"\"\n",
    "            Recursive function that exploits the\n",
    "            ability to call the Spark SQL Column method\n",
    "            .when() in a recursive way.\n",
    "            \"\"\"\n",
    "            left_edges = left_edges[:]\n",
    "            if len(left_edges) == 0:\n",
    "                return current_col\n",
    "            if len(left_edges) == 1:\n",
    "                next_col = current_col.when(col(column) >= float(left_edges[0]), count)\n",
    "                left_edges.pop(0)\n",
    "                return create_all_conditions(next_col, column, left_edges[:], count+1)\n",
    "            next_col = current_col.when((float(left_edges[0]) <= col(column))\n",
    "                                        & (col(column) < float(left_edges[1])), count)\n",
    "            left_edges.pop(0)\n",
    "            return create_all_conditions(next_col, column, left_edges[:], count+1)\n",
    "\n",
    "        num_range = maxim - minim\n",
    "        bin_width = num_range / float(bins)\n",
    "        left_edges = [minim]\n",
    "        for _bin in range(bins):\n",
    "            left_edges = left_edges + [left_edges[-1] + bin_width]\n",
    "        left_edges.pop()\n",
    "        expression_col = when((float(left_edges[0]) <= col(column))\n",
    "                              & (col(column) < float(left_edges[1])), 0)\n",
    "        left_edges_copy = left_edges[:]\n",
    "        left_edges_copy.pop(0)\n",
    "        bin_data = (df.select(col(column))\n",
    "                    .na.drop()\n",
    "                    .select(col(column),\n",
    "                            create_all_conditions(expression_col,\n",
    "                                                  column,\n",
    "                                                  left_edges_copy\n",
    "                                                 ).alias(\"bin_id\")\n",
    "                           )\n",
    "                    .groupBy(\"bin_id\").count()\n",
    "                   ).toPandas()\n",
    "\n",
    "        bin_data.index = bin_data[\"bin_id\"]\n",
    "        new_index = list(range(bins))\n",
    "        bin_data = bin_data.reindex(new_index)\n",
    "        bin_data[\"bin_id\"] = bin_data.index\n",
    "        bin_data = bin_data.fillna(0)\n",
    "\n",
    "        bin_data[\"left_edge\"] = left_edges\n",
    "        bin_data[\"width\"] = bin_width\n",
    "        \n",
    "\n",
    "        return bin_data\n",
    "\n",
    "\n",
    "    def describe_integer_1d(df, column, current_result, nrows):\n",
    "        \n",
    "        stats_df = df.select(column).na.drop().agg(mean(col(column)).alias(\"mean\"),\n",
    "                                                       df_min(col(column)).alias(\"min\"),\n",
    "                                                       df_max(col(column)).alias(\"max\"),\n",
    "                                                       variance(col(column)).alias(\"variance\"),\n",
    "                                                       kurtosis(col(column)).alias(\"kurtosis\"),\n",
    "                                                       stddev(col(column)).alias(\"std\"),\n",
    "                                                       skewness(col(column)).alias(\"skewness\"),\n",
    "                                                       df_sum(col(column)).alias(\"sum\")\n",
    "                                                       ).toPandas()\n",
    "\n",
    "\n",
    "        for x in np.array([0.05, 0.25, 0.5, 0.75, 0.95]):\n",
    "            stats_df[pretty_name(x)] = (df.select(column)\n",
    "                                        .na.drop()\n",
    "                                        .selectExpr(\"percentile(`{col}`,CAST({n} AS DOUBLE))\"\n",
    "                                                    .format(col=column, n=x)).toPandas().iloc[:,0]\n",
    "                                        )\n",
    "        stats = stats_df.iloc[0].copy()\n",
    "        stats.name = column\n",
    "        stats[\"range\"] = stats[\"max\"] - stats[\"min\"]\n",
    "        stats[\"iqr\"] = stats[pretty_name(0.75)] - stats[pretty_name(0.25)]\n",
    "        stats[\"cv\"] = stats[\"std\"] / float(stats[\"mean\"])\n",
    "        stats[\"mad\"] = (df.select(column)\n",
    "                        .na.drop()\n",
    "                        .select(df_abs(col(column)-stats[\"mean\"]).alias(\"delta\"))\n",
    "                        .agg(df_sum(col(\"delta\"))).toPandas().iloc[0,0] / float(current_result[\"count\"]))\n",
    "        stats[\"type\"] = \"NUM\"\n",
    "        stats['n_zeros'] = df.select(column).where(col(column)==0.0).count()\n",
    "        stats['p_zeros'] = stats['n_zeros'] / float(nrows)\n",
    "\n",
    "        hist_data = create_hist_data(df, column, stats[\"min\"], stats[\"max\"], bins)\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def describe_float_1d(df, column, current_result, nrows):\n",
    "        stats_df = df.select(column).na.drop().agg(mean(col(column)).alias(\"mean\"),\n",
    "                                                       df_min(col(column)).alias(\"min\"),\n",
    "                                                       df_max(col(column)).alias(\"max\"),\n",
    "                                                       variance(col(column)).alias(\"variance\"),\n",
    "                                                       kurtosis(col(column)).alias(\"kurtosis\"),\n",
    "                                                       stddev(col(column)).alias(\"std\"),\n",
    "                                                       skewness(col(column)).alias(\"skewness\"),\n",
    "                                                       df_sum(col(column)).alias(\"sum\")\n",
    "                                                       ).toPandas()\n",
    "\n",
    "        for x in np.array([0.05, 0.25, 0.5, 0.75, 0.95]):\n",
    "            stats_df[pretty_name(x)] = (df.select(column)\n",
    "                                        .na.drop()\n",
    "                                        .selectExpr(\"percentile_approx(`{col}`,CAST({n} AS DOUBLE))\"\n",
    "                                                    .format(col=column, n=x)).toPandas().iloc[:,0]\n",
    "                                        )\n",
    "        stats = stats_df.iloc[0].copy()\n",
    "        stats.name = column\n",
    "        stats[\"range\"] = stats[\"max\"] - stats[\"min\"]\n",
    "        stats[\"iqr\"] = stats[pretty_name(0.75)] - stats[pretty_name(0.25)]\n",
    "        stats[\"cv\"] = stats[\"std\"] / float(stats[\"mean\"])\n",
    "        stats[\"mad\"] = (df.select(column)\n",
    "                        .na.drop()\n",
    "                        .select(df_abs(col(column)-stats[\"mean\"]).alias(\"delta\"))\n",
    "                        .agg(df_sum(col(\"delta\"))).toPandas().iloc[0,0] / float(current_result[\"count\"]))\n",
    "        stats[\"type\"] = \"NUM\"\n",
    "        stats['n_zeros'] = df.select(column).where(col(column)==0.0).count()\n",
    "        stats['p_zeros'] = stats['n_zeros'] / float(nrows)\n",
    "\n",
    "        hist_data = create_hist_data(df, column, stats[\"min\"], stats[\"max\"], bins)\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def describe_date_1d(df, column):\n",
    "        stats_df = df.select(column).na.drop().agg(df_min(col(column)).alias(\"min\"),\n",
    "                                                   df_max(col(column)).alias(\"max\")\n",
    "                                                  ).toPandas()\n",
    "        stats = stats_df.iloc[0].copy()\n",
    "        stats.name = column\n",
    "\n",
    "        if isinstance(stats[\"max\"], pd.Timestamp):\n",
    "            stats = stats.astype(object)\n",
    "            stats[\"max\"] = str(stats[\"max\"].to_pydatetime())\n",
    "            stats[\"min\"] = str(stats[\"min\"].to_pydatetime())\n",
    "\n",
    "        else:\n",
    "            stats[\"range\"] = stats[\"max\"] - stats[\"min\"]\n",
    "        stats[\"type\"] = \"DATE\"\n",
    "        return stats\n",
    "\n",
    "    def guess_json_type(string_value):\n",
    "        try:\n",
    "            obj = json.loads(string_value)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        return type(obj)\n",
    "\n",
    "    def describe_categorical_1d(df, column):\n",
    "        value_counts = (df.select(column).na.drop()\n",
    "                        .groupBy(column)\n",
    "                        .agg(count(col(column)))\n",
    "                        .orderBy(\"count({c})\".format(c=column),ascending=False)\n",
    "                       ).cache()\n",
    "\n",
    "        # Get the most frequent class:\n",
    "        stats = (value_counts\n",
    "                 .limit(1)\n",
    "                 .withColumnRenamed(column, \"top\")\n",
    "                 .withColumnRenamed(\"count({c})\".format(c=column), \"freq\")\n",
    "                ).toPandas().iloc[0]\n",
    "\n",
    "        top_50 = value_counts.limit(50).toPandas().sort_values(\"count({c})\".format(c=column),\n",
    "                                                               ascending=False)\n",
    "        top_50_categories = top_50[column].values.tolist()\n",
    "\n",
    "        others_count = pd.Series([df.select(column).na.drop()\n",
    "                        .where(~(col(column).isin(*top_50_categories)))\n",
    "                        .count()\n",
    "                        ], index=[\"***Other Values***\"])\n",
    "        others_distinct_count = pd.Series([value_counts\n",
    "                                .where(~(col(column).isin(*top_50_categories)))\n",
    "                                .count()\n",
    "                                ], index=[\"***Other Values Distinct Count***\"])\n",
    "\n",
    "        top = top_50.set_index(column)[\"count({c})\".format(c=column)]\n",
    "        top = top.append(others_count)\n",
    "        top = top.append(others_distinct_count)\n",
    "        stats[\"value_counts\"] = top\n",
    "        stats[\"type\"] = \"CAT\"\n",
    "        value_counts.unpersist()\n",
    "        unparsed_valid_jsons = df.select(column).na.drop().rdd.map(\n",
    "            lambda x: guess_json_type(x[column])).filter(\n",
    "            lambda x: x).distinct().collect()\n",
    "        stats[\"unparsed_json_types\"] = unparsed_valid_jsons\n",
    "        return stats\n",
    "\n",
    "    def describe_constant_1d(df, column):\n",
    "        stats = pd.Series(['CONST'], index=['type'], name=column)\n",
    "        stats[\"value_counts\"] = (df.select(column)\n",
    "                                 .na.drop()\n",
    "                                 .limit(1)).toPandas().iloc[:,0].value_counts()\n",
    "        return stats\n",
    "\n",
    "    def describe_unique_1d(df, column):\n",
    "        stats = pd.Series(['UNIQUE'], index=['type'], name=column)\n",
    "        stats[\"value_counts\"] = (df.select(column)\n",
    "                                 .na.drop()\n",
    "                                 .limit(50)).toPandas().iloc[:,0].value_counts()\n",
    "        return stats\n",
    "\n",
    "    def describe_1d(df, column, nrows, lookup_config=None):\n",
    "        column_type = df.select(column).dtypes[0][1]\n",
    "        if (\"array\" in column_type) or (\"stuct\" in column_type) or (\"map\" in column_type):\n",
    "            raise NotImplementedError(\"Column {c} is of type {t} and cannot be analyzed\".format(c=column, t=column_type))\n",
    "\n",
    "        distinct_count = df.select(column).agg(countDistinct(col(column)).alias(\"distinct_count\")).toPandas()\n",
    "        non_nan_count = df.select(column).na.drop().select(count(col(column)).alias(\"count\")).toPandas()\n",
    "        results_data = pd.concat([distinct_count, non_nan_count],axis=1)\n",
    "        results_data[\"p_unique\"] = results_data[\"distinct_count\"] / float(results_data[\"count\"])\n",
    "        results_data[\"is_unique\"] = results_data[\"distinct_count\"] == nrows\n",
    "        results_data[\"n_missing\"] = nrows - results_data[\"count\"]\n",
    "        results_data[\"p_missing\"] = results_data[\"n_missing\"] / float(nrows)\n",
    "        results_data[\"p_infinite\"] = 0\n",
    "        results_data[\"n_infinite\"] = 0\n",
    "        result = results_data.iloc[0].copy()\n",
    "        result[\"memorysize\"] = 0\n",
    "        result.name = column\n",
    "\n",
    "        if result[\"distinct_count\"] <= 1:\n",
    "            result = result.append(describe_constant_1d(df, column))\n",
    "        elif column_type in {\"tinyint\", \"smallint\", \"int\", \"bigint\"}:\n",
    "            result = result.append(describe_integer_1d(df, column, result, nrows))\n",
    "        elif column_type in {\"float\", \"double\", \"decimal\"}:\n",
    "            result = result.append(describe_float_1d(df, column, result, nrows))\n",
    "        elif column_type in {\"date\", \"timestamp\"}:\n",
    "            result = result.append(describe_date_1d(df, column))\n",
    "        elif result[\"is_unique\"] == True:\n",
    "            result = result.append(describe_unique_1d(df, column))\n",
    "        else:\n",
    "            result = result.append(describe_categorical_1d(df, column))\n",
    "            # Fix to also count MISSING value in the distict_count field:\n",
    "            if result[\"n_missing\"] > 0:\n",
    "                result[\"distinct_count\"] = result[\"distinct_count\"] + 1\n",
    "\n",
    "        if (result[\"count\"] > result[\"distinct_count\"] > 1):\n",
    "            try:\n",
    "                result[\"mode\"] = result[\"top\"]\n",
    "            except KeyError:\n",
    "                result[\"mode\"] = 0\n",
    "        else:\n",
    "            try:\n",
    "                result[\"mode\"] = result[\"value_counts\"].index[0]\n",
    "            except KeyError:\n",
    "                result[\"mode\"] = 0\n",
    "            # If and IndexError happens,\n",
    "            # it is because all column are NULLs:\n",
    "            except IndexError:\n",
    "                result[\"mode\"] = \"MISSING\"\n",
    "\n",
    "        if lookup_config:\n",
    "            lookup_object = lookup_config['object']\n",
    "            col_name_in_db = lookup_config['col_name_in_db'] if 'col_name_in_db' in lookup_config else None\n",
    "            try:\n",
    "                matched, unmatched = lookup_object.lookup(df.select(column), col_name_in_db)\n",
    "                result['lookedup_values'] = str(matched.count()) + \"/\" + str(df.select(column).count())\n",
    "            except:\n",
    "                result['lookedup_values'] = 'FAILED'\n",
    "        else:\n",
    "            result['lookedup_values'] = ''\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    # Do the thing:\n",
    "    ldesc = {}\n",
    "    for colum in df.columns:\n",
    "        if colum in config:\n",
    "            if 'lookup' in config[colum]:\n",
    "                lookup_config = config[colum]['lookup']\n",
    "                desc = describe_1d(df, colum, table_stats[\"n\"], lookup_config=lookup_config)\n",
    "            else:\n",
    "                desc = describe_1d(df, colum, table_stats[\"n\"])\n",
    "        else:\n",
    "            desc = describe_1d(df, colum, table_stats[\"n\"])\n",
    "        ldesc.update({colum: desc})\n",
    "\n",
    "    # Compute correlation matrix\n",
    "    if corr_reject is not None:\n",
    "        computable_corrs = [colum for colum in ldesc if ldesc[colum][\"type\"] in {\"NUM\"}]\n",
    "\n",
    "        if len(computable_corrs) > 0:\n",
    "            corr = corr_matrix(df, columns=computable_corrs)\n",
    "            for x, corr_x in corr.iterrows():\n",
    "                for y, corr in corr_x.iteritems():\n",
    "                    if x == y:\n",
    "                        break\n",
    "\n",
    "    # Convert ldesc to a DataFrame\n",
    "    variable_stats = pd.DataFrame(ldesc)\n",
    "\n",
    "    # General statistics\n",
    "    table_stats[\"nvar\"] = len(df.columns)\n",
    "    table_stats[\"total_missing\"] = float(variable_stats.loc[\"n_missing\"].sum()) / (table_stats[\"n\"] * table_stats[\"nvar\"])\n",
    "    memsize = 0\n",
    "    table_stats['memsize'] = fmt_bytesize(memsize)\n",
    "    table_stats['recordsize'] = fmt_bytesize(memsize / table_stats['n'])\n",
    "    table_stats.update({k: 0 for k in (\"NUM\", \"DATE\", \"CONST\", \"CAT\", \"UNIQUE\", \"CORR\")})\n",
    "    table_stats.update(dict(variable_stats.loc['type'].value_counts()))\n",
    "    table_stats['REJECTED'] = table_stats['CONST'] + table_stats['CORR']\n",
    "\n",
    "    freq_dict = {}\n",
    "    for var in variable_stats:\n",
    "        if \"value_counts\" not in variable_stats[var]:\n",
    "            pass\n",
    "        elif not(variable_stats[var][\"value_counts\"] is np.nan):\n",
    "            freq_dict[var] = variable_stats[var][\"value_counts\"]\n",
    "        else:\n",
    "            pass\n",
    "    try:\n",
    "        variable_stats = variable_stats.drop(\"value_counts\")\n",
    "    except (ValueError, KeyError):\n",
    "        pass\n",
    "\n",
    "    return table_stats, variable_stats.T, freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import abs as absou\n",
    "\n",
    "SKEWNESS_CUTOFF = 20\n",
    "DEFAULT_FLOAT_FORMATTER = u'spark_df_profiling.__default_float_formatter'\n",
    "\n",
    "\n",
    "def gradient_format(value, limit1, limit2, c1, c2):\n",
    "    def LerpColour(c1,c2,t):\n",
    "        return (int(c1[0]+(c2[0]-c1[0])*t),int(c1[1]+(c2[1]-c1[1])*t),int(c1[2]+(c2[2]-c1[2])*t))\n",
    "    c = LerpColour(c1, c2, (value-limit1)/(limit2-limit1))\n",
    "    return fmt_color(value,\"rgb{}\".format(str(c)))\n",
    "\n",
    "\n",
    "def fmt_color(text, color):\n",
    "    return(u'<span style=\"color:{color}\">{text}</span>'.format(color=color,text=str(text)))\n",
    "\n",
    "\n",
    "def fmt_class(text, cls):\n",
    "    return(u'<span class=\"{cls}\">{text}</span>'.format(cls=cls,text=str(text)))\n",
    "\n",
    "\n",
    "def fmt_bytesize(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if num < 0:\n",
    "            num = num*-1\n",
    "            if num < 1024.0:\n",
    "                return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "            num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "\n",
    "def fmt_percent(v):\n",
    "    return  \"{:2.1f}%\".format(v*100)\n",
    "\n",
    "def fmt_varname(v):\n",
    "    return u'<code>{0}</code>'.format(v)\n",
    "\n",
    "\n",
    "value_formatters={\n",
    "        u'freq': (lambda v: gradient_format(v, 0, 62000, (30, 198, 244), (99, 200, 72))),\n",
    "        u'p_missing': fmt_percent,\n",
    "        u'p_infinite': fmt_percent,\n",
    "        u'p_unique': fmt_percent,\n",
    "        u'p_zeros': fmt_percent,\n",
    "        u'memorysize': fmt_bytesize,\n",
    "        u'total_missing': fmt_percent,\n",
    "        DEFAULT_FLOAT_FORMATTER: lambda v: str(float('{:.5g}'.format(v))).rstrip('0').rstrip('.'),\n",
    "        u'correlation_var': lambda v: fmt_varname(v),\n",
    "        u'unparsed_json_types': lambda v: ', '.join([s.__name__ for s in v])\n",
    "        }\n",
    "\n",
    "def fmt_row_severity(v):\n",
    "    if np.isnan(v) or v<= 0.01:\n",
    "        return \"ignore\"\n",
    "    else:\n",
    "        return \"alert\"\n",
    "\n",
    "def fmt_skewness(v):\n",
    "    if not np.isnan(v) and (v<-SKEWNESS_CUTOFF or v> SKEWNESS_CUTOFF):\n",
    "        return \"alert\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "row_formatters={\n",
    "    u'p_zeros': fmt_row_severity,\n",
    "    u'p_missing': fmt_row_severity,\n",
    "    u'p_infinite': fmt_row_severity,\n",
    "    u'n_duplicates': fmt_row_severity,\n",
    "    u'skewness': fmt_skewness,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Spark Describe Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run([\"/bin/bash\", \"/etc/config/v3io/v3io-spark-operator.sh\"])\n",
    "\n",
    "def describe_spark(context: MLClientCtx, \n",
    "                   dataset: DataItem,\n",
    "                   bins: int=30,\n",
    "                   describe_extended: bool=True)-> None:\n",
    "    \"\"\"\n",
    "    Generates profile reports from an Apache Spark DataFrame. \n",
    "    Based on pandas_profiling, but for Spark's DataFrames instead of pandas.\n",
    "    For each column the following statistics - if relevant for the column type - are presented:\n",
    "    \n",
    "    Essentials: type, unique values, missing values\n",
    "    Quantile statistics: minimum value, Q1, median, Q3, maximum, range, interquartile range\n",
    "    Descriptive statistics: mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness\n",
    "    Most frequent values: for categorical data \n",
    "    \n",
    "    :param context:               Function context.\n",
    "    :param dataset:               Raw data file (currently needs to be a local file located in v3io://User/bigdata)\n",
    "    :param bins:                  Number of bin in histograms\n",
    "    :param describe_extended:     (True) set to False if the aim is to get a simple .describe() infomration\n",
    "    \"\"\"\n",
    "    \n",
    "    # get file location\n",
    "    location = dataset.local()\n",
    "    \n",
    "    # build spark session\n",
    "    spark = SparkSession.builder.appName(\"Spark job\").getOrCreate()\n",
    "    \n",
    "    # read csv\n",
    "    df = spark.read.csv(location, header=True, inferSchema= True)\n",
    "\n",
    "    # No use for now\n",
    "    kwargs = []\n",
    "    \n",
    "    # take only numric column\n",
    "    float_cols = [item[0] for item in df.dtypes if item[1].startswith('float') or item[1].startswith('double')]\n",
    "    \n",
    "    if describe_extended == True:\n",
    "        \n",
    "        # run describe function\n",
    "        table, variables, freq = describe(df, bins, float_cols, kwargs)\n",
    "\n",
    "        # get summary table\n",
    "        tbl_1 = variables.reset_index()\n",
    "\n",
    "        # prep report \n",
    "        if len(freq) != 0:\n",
    "            tbl_2 = pd.DataFrame.from_dict(freq, orient = \"index\").sort_index().stack().reset_index()\n",
    "            tbl_2.columns = ['col', 'key', 'val']\n",
    "            tbl_2['Merged'] = [{key: val} for key, val in zip(tbl_2.key, tbl_2.val)]\n",
    "            tbl_2 = tbl_2.groupby('col', as_index=False).agg(lambda x: tuple(x))[['col','Merged']]\n",
    "\n",
    "            # get summary\n",
    "            summary = pd.merge(tbl_1, tbl_2, how='left', left_on='index', right_on='col')\n",
    "\n",
    "        else:\n",
    "            summary = tbl_1\n",
    "\n",
    "        # log final report\n",
    "        context.log_dataset(\"summary_stats\", \n",
    "                            df=summary,\n",
    "                            format=\"csv\", index=False,\n",
    "                            artifact_path=context.artifact_subpath('data'))\n",
    "\n",
    "        # log overview\n",
    "        context.log_results(table)\n",
    "    \n",
    "    else:\n",
    "        # run simple describe and save to pandas\n",
    "        tbl_1 = df.describe().toPandas()\n",
    "        \n",
    "        # save final report and transpose \n",
    "        summary = tbl_1.T\n",
    "        \n",
    "        # log final report\n",
    "        context.log_dataset(\"summary_stats\", \n",
    "                            df=summary,\n",
    "                            format=\"csv\", index=False,\n",
    "                            artifact_path=context.artifact_subpath('data'))\n",
    "    \n",
    "    # stop spark session\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = mlrun.code_to_function(handler=\"describe_spark\", code_output=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-10-28 13:54:47,724 [info] function spec saved to path: function.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7fb4beb8fa50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.apply(mount_v3io())\n",
    "fn.apply(mount_v3iod(namespace=\"default-tenant\", v3io_config_configmap=\"spark-operator-v3io-config\"))\n",
    "fn.spec.image_pull_policy = \"IfNotPresent\"\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-10-28 13:54:47,738 [warning] warning!, server (0.5.3-rc1) and client (0.5.2) ver dont match\n"
     ]
    }
   ],
   "source": [
    "artifact_path = mlrun.set_environment(api_path = 'http://mlrun-api:8080',\n",
    "                                      artifact_path = os.path.abspath('./'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run and Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-10-28 13:54:47,762 [warning] warning!, server (0.5.3-rc1) and client (0.5.2) ver dont match\n",
      "> 2020-10-28 13:54:47,763 [info] starting run describe-spark-describe_spark uid=a9cc8a2b48ce42d180e490043091da52  -> http://mlrun-api:8080\n",
      "> 2020-10-28 13:54:48,205 [info] Job is running in the background, pod: describe-spark-describe-spark-pxzxc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #b3edff;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #ffe6cc;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #ffe6cc;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>default</td>\n",
       "      <td><div title=\"a9cc8a2b48ce42d180e490043091da52\"><a href=\"https://mlrun-ui.default-tenant.app.dsteam.iguazio-cd1.com/projects/default/jobs/a9cc8a2b48ce42d180e490043091da52/info\" target=\"_blank\" >...3091da52</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Oct 28 13:54:47</td>\n",
       "      <td>running</td>\n",
       "      <td>describe-spark-describe_spark</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=admin</div><div class=\"dictlist\">kind=job</div><div class=\"dictlist\">owner=admin</div></td>\n",
       "      <td><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"resulta279d734\" title=\"/files/functions/describe_spark/iris_dataset.csv\">dataset</div></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"resulta279d734-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"resulta279d734-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"resulta279d734\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"resulta279d734-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to track results use .show() or .logs() or in CLI: \n",
      "!mlrun get run a9cc8a2b48ce42d180e490043091da52 --project default , !mlrun logs a9cc8a2b48ce42d180e490043091da52 --project default\n",
      "> 2020-10-28 13:54:48,285 [info] run executed, status=running\n"
     ]
    }
   ],
   "source": [
    "run_res = fn.run(inputs={\"dataset\": \"iris_dataset.csv\"},\n",
    "                 artifact_path=artifact_path, watch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-10-28 13:55:36,021 [warning] warning!, server (0.5.3-rc1) and client (0.5.2) ver dont match\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #b3edff;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #ffe6cc;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #ffe6cc;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>default</td>\n",
       "      <td><div title=\"a9cc8a2b48ce42d180e490043091da52\"><a href=\"https://mlrun-ui.default-tenant.app.dsteam.iguazio-cd1.com/projects/default/jobs/a9cc8a2b48ce42d180e490043091da52/info\" target=\"_blank\" >...3091da52</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Oct 28 13:54:55</td>\n",
       "      <td>completed</td>\n",
       "      <td>describe-spark-describe_spark</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=admin</div><div class=\"dictlist\">kind=job</div><div class=\"dictlist\">owner=admin</div><div class=\"dictlist\">host=describe-spark-describe-spark-pxzxc</div></td>\n",
       "      <td><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"result258198bb\" title=\"/files/functions/describe_spark/iris_dataset.csv\">dataset</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">n=150</div><div class=\"dictlist\">nvar=5</div><div class=\"dictlist\">total_missing=0.0</div><div class=\"dictlist\">memsize=0.0 YiB</div><div class=\"dictlist\">recordsize=0.0 YiB</div><div class=\"dictlist\">NUM=5</div><div class=\"dictlist\">DATE=0</div><div class=\"dictlist\">CONST=0</div><div class=\"dictlist\">CAT=0</div><div class=\"dictlist\">UNIQUE=0</div><div class=\"dictlist\">CORR=0</div><div class=\"dictlist\">REJECTED=0</div></td>\n",
       "      <td><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"result258198bb\" title=\"/files/functions/describe_spark/data/summary_stats.csv\">summary_stats</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result258198bb-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result258198bb-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result258198bb\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result258198bb-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
