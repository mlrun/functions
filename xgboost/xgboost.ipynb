{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from cloudpickle import dump, load\n",
    "\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from typing import List\n",
    "from mlrun.execution import MLClientCtx\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun.artifacts import PlotArtifact\n",
    "\n",
    "from utils import get_model_configs\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "def xgboost_train(\n",
    "    context: MLClientCtx,\n",
    "    data_key: str,\n",
    "    sample: int,\n",
    "    label_column: str,\n",
    "\n",
    "    # xgboost params:\n",
    "    num_classes: int = 2,\n",
    "    class_names: list[str] = [\"background\", \"signal\"]\n",
    "    max_depth: int = 50,\n",
    "    learning_rate: float = 0.1\n",
    "    # CHOOSE OBJ BASED ON num_classes objective: 'multi:softmax'\n",
    "    # some reg params\n",
    "    \n",
    "    other_xgb_params: dict = {},\n",
    "    test_size: float = 0.05,\n",
    "    train_val_split: float = 0.75,\n",
    "    rng: int = 1,\n",
    "    models_dir: str = \"models\",\n",
    "    plots_dir: str = \"plots\",\n",
    "    score_method: str = \"micro\",\n",
    "    \n",
    "\n",
    ") -> None:\n",
    "    \"\"\"train a classifier.\n",
    "\n",
    "    :param context:           the function context\n",
    "    :param model_pkg_class:   the model to train, e.g, \"sklearn.neural_networks.MLPClassifier\", \n",
    "                              or json model config\n",
    "    :param data_key:          (\"raw\") name of raw data file\n",
    "    :param sample:            Selects the first n rows, or select a sample\n",
    "                              starting from the first. If negative <-1, select\n",
    "                              a random sample\n",
    "    :param label_column:      ground-truth (y) labels\n",
    "    :param model_key:         (\"model\") name of model in artifact store,\n",
    "                              points to a directory\n",
    "    :param test_size:         (0.05) test set size\n",
    "    :param train_val_split:   (0.75) Once the test set has been removed the\n",
    "                              training set gets this proportion.\n",
    "    :param test_set_key:      store the test data set under this key in the\n",
    "                              artifact store\n",
    "    :param rng:               (1) sklearn rng seed\n",
    "    :param models_dir:        models subfolder on artifact path\n",
    "    :param plots_dir:         plot subfolder on artifact path\n",
    "    :param score_method:      for multiclass classification\n",
    "    :param class_updates:     update these scikit-learn classifier params,\n",
    "                              input as a dict\n",
    "    :param fit_updates:       update scikit-learn fit parameters, input as\n",
    "                              a dict.\n",
    "    \"\"\"\n",
    "    # extract file name from DataItem\n",
    "    srcfilepath = str(data_key)\n",
    "    \n",
    "    # TODO: this should be part of data\"s metadata dealt with in another step get a data set, sample, etc...\n",
    "    # get all data or a sample\n",
    "    if (sample == -1) or (sample >= 1):\n",
    "        # get all rows, or contiguous sample starting at row 1.\n",
    "        raw = pq.read_table(srcfilepath).to_pandas().dropna()\n",
    "        labels = raw.pop(label_column)\n",
    "        raw = raw.iloc[:sample, :]\n",
    "        labels = labels.iloc[:sample]\n",
    "    else:\n",
    "        # grab a random sample\n",
    "        raw = pq.read_table(srcfilepath).to_pandas().dropna().sample(sample * -1)\n",
    "        labels = raw.pop(label_column)\n",
    "\n",
    "    # TODO: this should be part of data\"s metadata dealt with in another step\n",
    "    context.header = raw.columns.values\n",
    "    \n",
    "    # TODO: all of this should be part of a spitter component that does cv too, dealt with in another step\n",
    "    # make a hot encode copy of labels before the split\n",
    "    yb = label_binarize(labels, classes=labels.unique()) # if binary 0/1 labels, will return labels as is\n",
    "    \n",
    "    # double split to generate 3 data sets: train, validation and test\n",
    "    # with xtest,ytest set aside\n",
    "    # here we hide the binary encoded labels inside the X matrix so that when splitting we preserve order in both the encoded\n",
    "    # and non-encoded labels:\n",
    "    x, xtest, y, ytest = train_test_split(np.concatenate([raw, yb], axis=1), labels, test_size=test_size, random_state=rng)\n",
    "    xtrain, xvalid, ytrain, yvalid = train_test_split(x, y, train_size=train_val_split, random_state=rng)\n",
    "    # now extract the hot_encoded labels\n",
    "    ytrainb = xtrain[:, -yb.shape[1]:].copy()\n",
    "    xtrain = xtrain[:, :-yb.shape[1]].copy()\n",
    "    # extract the hot_encoded labels\n",
    "    yvalidb = xvalid[:, -yb.shape[1]:].copy()\n",
    "    xvalid = xvalid[:, :-yb.shape[1]].copy()\n",
    "    # extract the hot_encoded labels\n",
    "    ytestb = xtest[:, -yb.shape[1]:].copy()\n",
    "    xtest = xtest[:, :-yb.shape[1]].copy()                                      \n",
    "    \n",
    "    # set-aside test_set\n",
    "    test_set = pd.concat(\n",
    "        [pd.DataFrame(data=xtest, columns=context.header),\n",
    "         pd.DataFrame(data=ytest.values, columns=[label_column])],\n",
    "        axis=1,)\n",
    "    context.log_dataset(test_set_key, df=test_set, format=file_ext, index=False)\n",
    "\n",
    "    if model_pkg_class.endswith(\".json\"):\n",
    "        model_config = json.load(open(model_pkg_class, \"r\"))\n",
    "    else:\n",
    "        # load the model config\n",
    "        model_config = get_model_configs(model_pkg_class)\n",
    "\n",
    "    # get update params if any\n",
    "    if isinstance(class_params_updates, DataItem):\n",
    "        class_params_updates = json.loads(class_params_updates.get())\n",
    "    if isinstance(fit_params_updates, DataItem):\n",
    "        fit_params_updates = json.loads(fit_params_updates.get())\n",
    "    # update the parameters            \n",
    "    # add data to fit params\n",
    "    fit_params_updates.update({\"X\": xtrain,\"y\": ytrain.values})\n",
    "    \n",
    "    model_config[\"CLASS\"].update(class_params_updates)\n",
    "    model_config[\"FIT\"].update(fit_params_updates)\n",
    "    \n",
    "    # create class and fit\n",
    "    ClassifierClass = _create_class(model_config[\"META\"][\"class\"])\n",
    "    model = ClassifierClass(**model_config[\"CLASS\"])\n",
    "    model.fit(**model_config[\"FIT\"])\n",
    "\n",
    "    # save model\n",
    "    filepath = os.path.join(base_path, f\"{models_dir}/{model_key}.pkl\")\n",
    "    try:\n",
    "        dump(model, open(filepath, \"wb\"))\n",
    "        context.log_artifact(model_key, local_path=models_dir)\n",
    "    except Exception as e:\n",
    "        print(\"SERIALIZE MODEL ERROR:\", str(e))\n",
    "\n",
    "    # compute validation metrics\n",
    "    ypred = model.predict(xvalid)\n",
    "    y_score = model.predict_proba(xvalid)\n",
    "    context.logger.info(f\"y_score.shape {y_score.shape}\")\n",
    "    context.logger.info(f\"yvalidb.shape {yvalidb.shape}\")\n",
    "    if yvalidb.shape[1] > 1:\n",
    "        # label encoding was applied:\n",
    "        average_precision = metrics.average_precision_score(yvalidb,\n",
    "                                                            y_score,\n",
    "                                                            average=score_method)\n",
    "        context.log_result(f\"rocauc\", metrics.roc_auc_score(yvalidb, y_score))\n",
    "    else:\n",
    "        average_precision = metrics.average_precision_score(yvalidb,\n",
    "                                                            y_score[:, 1],\n",
    "                                                            average=score_method)\n",
    "        context.log_result(f\"rocauc\", metrics.roc_auc_score(yvalidb, y_score[:, 1]))\n",
    "        \n",
    "    context.log_result(f\"avg_precscore\", average_precision)\n",
    "    context.log_result(f\"accuracy\", float(model.score(xvalid, yvalid)))\n",
    "    context.log_result(f\"f1_score\", metrics.f1_score(yvalid, ypred,\n",
    "                                             average=score_method))\n",
    "\n",
    "    # TODO: missing validation plots, callbacks need to reintroduced\n",
    "    \n",
    "    plot_roc(context, yvalidb, y_score)\n",
    "    plot_confusion_matrix(context, yvalid, ypred, key=\"confusion\", fmt=\"png\")\n",
    "\n",
    "def plot_roc(\n",
    "    context,\n",
    "    y_labels,\n",
    "    y_probs,\n",
    "    key=\"roc\",\n",
    "    plots_dir: str = \"plots\",\n",
    "    fmt=\"png\",\n",
    "    fpr_label: str = \"false positive rate\",\n",
    "    tpr_label: str =  \"true positive rate\",\n",
    "    title: str = \"roc curve\",\n",
    "    legend_loc: str = \"best\"\n",
    "):\n",
    "    \"\"\"plot roc curves\n",
    "    \n",
    "    TODO:  add averaging method (as string) that was used to create probs, \n",
    "    display in legend\n",
    "    \n",
    "    :param context:      the function context\n",
    "    :param y_labels:     ground truth labels, hot encoded for multiclass  \n",
    "    :param y_probs:      model prediction probabilities\n",
    "    :param key:          (\"roc\") key of plot in artifact store\n",
    "    :param plots_dir:    (\"plots\") destination folder relative path to artifact path\n",
    "    :param fmt:          (\"png\") plot format\n",
    "    :param fpr_label:    (\"false positive rate\") x-axis labels\n",
    "    :param tpr_label:    (\"true positive rate\") y-axis labels\n",
    "    :param title:        (\"roc curve\") title of plot\n",
    "    :param legend_loc:   (\"best\") location of plot legend\n",
    "    \"\"\"\n",
    "    # clear matplotlib current figure\n",
    "    _gcf_clear(plt)\n",
    "    \n",
    "    # draw 45 degree line\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    \n",
    "    # labelling\n",
    "    plt.xlabel(fpr_label)\n",
    "    plt.ylabel(tpr_label)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=legend_loc)\n",
    "    \n",
    "    # single ROC or mutliple\n",
    "    if y_labels.shape[1] > 1:\n",
    "        # data accummulators by class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(y_labels[:,:-1].shape[1]):\n",
    "            fpr[i], tpr[i], _ = metrics.roc_curve(y_labels[:, i], y_probs[:, i], pos_label=1)\n",
    "            roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "            plt.plot(fpr[i], tpr[i], label=f\"class {i}\")\n",
    "    else:\n",
    "        fpr, tpr, _ = metrics.roc_curve(y_labels, y_probs[:, 1], pos_label=1)\n",
    "        plt.plot(fpr, tpr, label=f\"positive class\")\n",
    "        \n",
    "    fname = f\"{plots_dir}/{key}.html\"\n",
    "    context.log_artifact(PlotArtifact(key, body=plt.gcf()), local_path=fname)\n",
    "    \n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    context: MLClientCtx,\n",
    "    labels,\n",
    "    predictions,\n",
    "    key: str = \"confusion_matrix\",\n",
    "    plots_dir: str = \"plots\",\n",
    "    colormap: str = \"Blues\",\n",
    "    fmt: str = \"png\",\n",
    "    sample_weight=None\n",
    "):\n",
    "    \"\"\"Create a confusion matrix.\n",
    "    Plot and save a confusion matrix using test data from a\n",
    "    modelline step.\n",
    "    \n",
    "    See https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "    \n",
    "    TODO: fix label alignment\n",
    "    TODO: consider using another packaged version\n",
    "    TODO: refactor to take params dict for plot options\n",
    "\n",
    "    :param context:         function context\n",
    "    :param labels:          validation data ground-truth labels\n",
    "    :param predictions:     validation data predictions\n",
    "    :param key:             str\n",
    "    :param plots_dir:       relative path of plots in artifact store\n",
    "    :param colormap:        colourmap for confusion matrix\n",
    "    :param fmt:             plot format\n",
    "    :param sample_weight:   sample weights\n",
    "    \"\"\"\n",
    "    _gcf_clear(plt)\n",
    "    \n",
    "    cm = metrics.confusion_matrix(labels, predictions, sample_weight=None)\n",
    "    sns.heatmap(cm, annot=True, cmap=colormap, square=True)\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fname = f\"{plots_dir}/{key}.html\"\n",
    "    context.log_artifact(PlotArtifact(key, body=fig), local_path=fname)\n",
    "\n",
    "    \n",
    "def _gcf_clear(plt):\n",
    "    \"\"\"Utility to clear matplotlib figure\n",
    "\n",
    "    Run this inside every plot method before calling any matplotlib\n",
    "    methods\n",
    "\n",
    "    :param plot:    matloblib figure object\n",
    "    \"\"\"\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function \n",
    "# create job function object from notebook code\n",
    "fn = code_to_function(\"xgboost\", kind=\"job\", with_doc=True,\n",
    "                      handler=train_model, image=\"mlrun/ml-models\")\n",
    "\n",
    "# add metadata (for templates and reuse)\n",
    "fn.spec.default_handler = \"xgboost\"\n",
    "fn.spec.description = \"train an xgboost modedl\"\n",
    "fn.metadata.categories = [\"models\", \"xgboost\"]\n",
    "fn.spec.image_pull_policy = \"Always\"\n",
    "fn.metadata.labels = {\"author\": \"yjb\"}\n",
    "\n",
    "fn.save()\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import import_function, mount_v3io\n",
    "\n",
    "func = import_function(\"hub://xgboost\").apply(mount_v3io())\n",
    "# func = import_function(\"function.yaml\").apply(mlrun.mount_v3io())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change any scikit-learn class params here (__init__ funciton params)\n",
    "class_params_updates = {\n",
    "    \"random_state\" : 1\n",
    "}\n",
    "\n",
    "# change any scikit-learn fit params here\n",
    "fit_params_updates = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_params = {\n",
    "    \"name\" : \"tasks train an xgboost model\",\n",
    "    \"params\" : {\n",
    "        \"model_key\"       : \"models\",\n",
    "        \n",
    "        # POINT THIS TO YOUR DATA\n",
    "        #\"data_key\"        : \"/User/artifacts/iris.parquet\",\n",
    "        #\"data_key\"        : \"/User/artifacts/wine.parquet\",\n",
    "        \"data_key\"        : \"/User/artifacts/breast_cancer.parquet\",\n",
    "        \"sample\"          : -1,\n",
    "        \"label_column\"    : \"labels\",\n",
    "        \"test_size\"       : 0.10,\n",
    "        \"train_val_split\" : 0.75,\n",
    "        \"rng\"             : 1,\n",
    "        \n",
    "        # xgboost parameters\n",
    "        \"max_depth\"\n",
    "        \n",
    "        # other xgboost paramters\n",
    "        \"xgboost_params\"   : class_params_updates}\n",
    "\n",
    "\n",
    "from mlrun import NewTask\n",
    "run = func.run(NewTask(**task_params), artifact_path=\"/User/artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-v0.4.6",
   "language": "python",
   "name": "mlrun-v0.4.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
