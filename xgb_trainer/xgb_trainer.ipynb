{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook function enables training and logging of xgboost models by exposing the higher level **[xgboost sklearn api](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)**.\n",
    "\n",
    "For the **[low-level xgboost api](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training)** please use **[xgb_custom.ipynb](../xgb_custom/xgb_custom.ipynb)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## steps\n",
    "1. generate an xgboost model configuration by selecting one of 5 available types\n",
    "2. get a sample of data from a data source (random rows, consecutive rows, or the entire dataset, custom sample)\n",
    "3. split the data into train, validation, and test sets (WIP, this will be parametrized cross-validator)  \n",
    "4. train the model using xgboost in one of its flavours (dask, gpu, mpi...)\n",
    "5. dump the model\n",
    "6. evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun.mlutils import (get_sample, get_splits, gen_sklearn_model,\n",
    "                           create_class, eval_model_v2, gcf_clear)\n",
    "\n",
    "from mlrun.execution import MLClientCtx\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun.artifacts import PlotArtifact, TableArtifact\n",
    "\n",
    "from cloudpickle import dumps\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate an xgb model\n",
    "\n",
    "generate a model config using the xgboost's sklearn api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gen_xgb_model(model_type: str, xgb_params: dict):\n",
    "    \"\"\"generate an xgboost model\n",
    "    \n",
    "    Multiple model types that can be estimated using\n",
    "    the XGBoost Scikit-Learn API.\n",
    "    \n",
    "    Input can either be a predefined json model configuration or one\n",
    "    of the five xgboost model types: \"classifier\", \"regressor\", \"ranker\",\n",
    "    \"rf_classifier\", or \"rf_regressor\".\n",
    "    \n",
    "    In either case one can pass in a params dict to modify defaults values.\n",
    "    \n",
    "    Based on `mlutils.models.gen_sklearn_model`, see the function\n",
    "    `sklearn_classifier` in this repository.\n",
    "    \n",
    "    :param model_type: one of \"classifier\", \"regressor\",\n",
    "                       \"ranker\", \"rf_classifier\", or\n",
    "                      \"rf_regressor\"\n",
    "    :param xgb_params: class init parameters\n",
    "    \"\"\"\n",
    "    # generate model and fit function\n",
    "    mtypes = {\n",
    "        \"classifier\"   : \"xgboost.XGBClassifier\",\n",
    "        \"regressor\"    : \"xgboost.XGBRegressor\",\n",
    "        \"ranker\"       : \"xgboost.XGBRanker\",\n",
    "        \"rf_classifier\": \"xgboost.XGBRFClassifier\",\n",
    "        \"rf_regressor\" : \"xgboost.XGBRFRegressor\"\n",
    "    }\n",
    "    if model_type.endswith(\"json\"):\n",
    "        model_config = model_type\n",
    "    elif model_type in mtypes.keys():\n",
    "        model_config = mtypes[model_type]\n",
    "    else:\n",
    "        raise Exception(\"unrecognized model type, see help documentation\")\n",
    "\n",
    "    return gen_sklearn_model(model_config, xgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    context: MLClientCtx,\n",
    "    model_type: str,\n",
    "    dataset: DataItem,\n",
    "    label_column: str = \"labels\",\n",
    "    encode_cols: dict = {},\n",
    "    sample: int = -1,\n",
    "    imbal_vec = [],\n",
    "    test_size: float = 0.25,\n",
    "    valid_size: float = 0.75,\n",
    "    random_state: int = 1,\n",
    "    models_dest: str = \"models\",\n",
    "    plots_dest: str = \"plots\",\n",
    "    eval_metrics: list= [\"error\", \"auc\"],\n",
    "    file_ext: str = \"parquet\",\n",
    "    model_pkg_file: str = \"\",    \n",
    ") -> None:\n",
    "    \"\"\"train an xgboost model.\n",
    "    \n",
    "    Note on imabalanced data:  the `imbal_vec` parameter represents the measured\n",
    "    class representations in the sample and can be used as a first step in tuning\n",
    "    an XGBoost model.  This isn't a hyperparamter, merely an estimate that should\n",
    "    be set as 'constant' throughout tuning process.\n",
    "    \n",
    "    :param context:           the function context\n",
    "    :param model_type:        the model type to train, \"classifier\", \"regressor\"...\n",
    "    :param dataset:           (\"data\") name of raw data file\n",
    "    :param label_column:      ground-truth (y) labels\n",
    "    :param encode_cols:       dictionary of names and prefixes for columns that are\n",
    "                              to hot be encoded.\n",
    "    :param sample:            Selects the first n rows, or select a sample\n",
    "                              starting from the first. If negative <-1, select\n",
    "                              a random sample\n",
    "    :param imbal_vec:         ([]) vector of class weights seen in sample\n",
    "    :param test_size:         (0.05) test set size\n",
    "    :param valid_size:        (0.75) Once the test set has been removed the\n",
    "                              training set gets this proportion.\n",
    "    :param random_state:      (1) sklearn rng seed\n",
    "    :param models_dest:       destination subfolder for model artifacts\n",
    "    :param plots_dest:        destination subfolder for plot artifacts\n",
    "    :param eval_metrics:      ([\"error\", \"auc\"]) learning curve metrics\n",
    "    :param file_ext:          format for test_set_key hold out data\n",
    "    \"\"\"\n",
    "    # deprecate:\n",
    "    models_dest = models_dest or \"models\"\n",
    "    plots_dest = plots_dest or f\"plots/{context.name}\"\n",
    "    \n",
    "    # get a sample from the raw data\n",
    "    raw, labels, header = get_sample(dataset, sample, label_column)\n",
    "    \n",
    "    # hot-encode\n",
    "    if encode_cols:\n",
    "        raw = pd.get_dummies(raw, \n",
    "                             columns=list(encode_cols.keys()), \n",
    "                             prefix=list(encode_cols.values()), \n",
    "                             drop_first=True)\n",
    "    \n",
    "    # split the sample into train validate, test and calibration sets:\n",
    "    (xtrain, ytrain), (xvalid, yvalid), (xtest, ytest) = \\\n",
    "        get_splits(raw, labels, 3, test_size, valid_size, random_state)\n",
    "    \n",
    "    # save test data\n",
    "    context.log_dataset(\"test-set\", df=pd.concat([xtest, ytest], axis=1), format=file_ext, index=False)\n",
    "\n",
    "    # get model config\n",
    "    model_config = _gen_xgb_model(model_type, context.parameters.items())\n",
    "\n",
    "    # create model instance\n",
    "    XGBBoostClass = create_class(model_config[\"META\"][\"class\"])\n",
    "    model = XGBBoostClass(**model_config[\"CLASS\"])\n",
    "\n",
    "    # update the model config with training data and callbacks\n",
    "    model_config[\"FIT\"].update({\"X\": xtrain, \n",
    "                                \"y\": ytrain.values,\n",
    "                                \"eval_set\":[(xtrain, ytrain), (xvalid, yvalid)],\n",
    "                                \"eval_metric\": eval_metrics})\n",
    "\n",
    "    # run the fit\n",
    "    model.fit(**model_config[\"FIT\"])\n",
    "\n",
    "    # evaluate model\n",
    "    eval_metrics = eval_model_v2(context, xvalid, yvalid, model)\n",
    "\n",
    "    # just do this inside log_model?\n",
    "    if hasattr(eval_metrics, \"plots\"):\n",
    "        model_plots = eval_metrics.pop(\"plots\")\n",
    "        for plot in model_plots:\n",
    "            context.log_artifact(plot, local_path=f\"{plots_dest}/{plot.key}.html\")\n",
    "    if hasattr(eval_metrics, \"tables\"):\n",
    "        model_tables = eval_metrics.pop(\"tables\")\n",
    "        for tbl in model_tables:\n",
    "            context.log_artifact(tbl, local_path=f\"{plots_dest}/{plot.key}.csv\")\n",
    "\n",
    "    model_bin = dumps(model) # .get_booster())\n",
    "#     context.log_model(\"model\", body=model_bin,\n",
    "#                       artifact_path=os.path.join(context.artifact_path, models_dest),\n",
    "#                       #model_dir=models_dest, \n",
    "#                       model_file=\"model.pkl\",\n",
    "#                       metrics=eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function\n",
    "from mlrun.platforms.other import auto_mount\n",
    "\n",
    "# create job function object from notebook code\n",
    "fn = code_to_function(\n",
    "    name=\"xgb_trainer\",\n",
    "    handler=\"train_model\",\n",
    "    kind=\"job\",\n",
    "    image=\"mlrun/ml-models\",\n",
    "    description=\"train multiple model types using xgboost\",\n",
    "    categories=[\"training\", \"ml\", \"experimental\"],\n",
    "    labels={\"author\": \"yjb\", \"framework\": \"xgboost\"}\n",
    ")\n",
    "\n",
    "fn.export(\"function.yaml\")\n",
    "fn.apply(auto_mount())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = False\n",
    "\n",
    "task_params = {\n",
    "    \"name\" : \"tasks xgb cpu trainer\",\n",
    "    \"params\" : {\n",
    "        \"model_type\"              : \"classifier\",\n",
    "        \"CLASS_tree_method\"       : \"gpu_hist\" if gpus else \"hist\",\n",
    "        \"CLASS_objective\"         : \"binary:logistic\",\n",
    "        \"CLASS_booster\"           : \"gbtree\",\n",
    "        \"FIT_verbose\"             : 0,\n",
    "        \"imbal_vec\"               : [],\n",
    "        \"label_column\"            : \"labels\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://raw.githubusercontent.com/yjb-ds/testdata/master/data/classifier-data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import run_local, NewTask\n",
    "\n",
    "run = run_local(\n",
    "    NewTask(**task_params),\n",
    "    handler=train_model,\n",
    "    inputs={\"dataset\"  : DATA_URL},\n",
    "    artifact_path=mlconf.artifact_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
