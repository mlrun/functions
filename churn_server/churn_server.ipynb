{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting kind to 'nuclio:serving'\n",
      "%nuclio: setting 'MODEL_CLASS' environment variable\n",
      "%nuclio: setting spec.build.baseImage to 'mlrun/ml-models'\n"
     ]
    }
   ],
   "source": [
    "%nuclio config kind=\"nuclio:serving\"\n",
    "%nuclio env MODEL_CLASS=ChurnModel\n",
    "\n",
    "%nuclio config spec.build.baseImage = \"mlrun/ml-models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nuclio cmd -c\n",
    "python -m pip install lifelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from cloudpickle import load\n",
    "from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "import xgboost as xgb\n",
    "\n",
    "### Model Serving Class\n",
    "\n",
    "import mlrun\n",
    "class ChurnModel(mlrun.runtimes.MLModelServer):\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        load multiple models in nested folders, churn model only\n",
    "        \"\"\"\n",
    "        clf_model_file, extra_data = self.get_model(\".pkl\")\n",
    "        self.model = load(open(str(clf_model_file), \"rb\"))\n",
    "        \n",
    "        if \"cox\" in extra_data.keys():\n",
    "            cox_model_file = extra_data[\"cox\"]\n",
    "            self.cox_model = load(open(str(cox_model_file), \"rb\"))\n",
    "            if \"cox/km\" in extra_data.keys():\n",
    "                km_model_file = extra_data[\"cox/km\"]\n",
    "                self.km_model = load(open(str(km_model_file), \"rb\"))\n",
    "        return\n",
    "\n",
    "    def predict(self, body):\n",
    "        try:\n",
    "            # we have potentially 3 models to work with:\n",
    "            #if hasattr(self, \"cox_model\") and hasattr(self, \"km_model\"):\n",
    "                # hack for now, just predict using one:\n",
    "            feats = np.asarray(body[\"instances\"], dtype=np.float32).reshape(-1, 23)\n",
    "            result = self.model.predict(feats, validate_features=False)\n",
    "            return result.tolist()\n",
    "            #else:\n",
    "            #    raise Exception(\"models not found\")\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Failed to predict %s\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following end-code annotation tells ```nuclio``` to stop parsing the notebook from this cell. _**Please do not remove this cell**_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import mlconf\n",
    "import os\n",
    "mlconf.dbpath = mlconf.dbpath or \"http://mlrun-api:8080\"\n",
    "mlconf.artifact_path = mlconf.artifact_path or f\"{os.environ['HOME']}/artifacts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"test-locally\"></a>\n",
    "## Test the function locally\n",
    "\n",
    "The class above can be tested locally. Just instantiate the class, `.load()` will load the model to a local dir.\n",
    "\n",
    "> **Verify there is a model file in the model_dir path (generated by the training notebook)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(mlconf.artifact_path, \"churn\", \"models\")\n",
    "\n",
    "my_server = ChurnModel(\"my-model\", model_dir=model_dir)\n",
    "my_server.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "xtest = pd.read_csv(os.path.join(mlconf.artifact_path, \"churn\", \"test-set.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `.predict(body)` method to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np\n",
    "\n",
    "# this should fail if the churn model hasn't been saved properly\n",
    "preds = my_server.predict({\"instances\":xtest.values[:10,:-1].tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class: [0, 1, 0, 1, 1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"predicted class:\", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deploy\"></a>\n",
    "### **deploy our serving class using as a serverless function**\n",
    "in the following section we create a new model serving function which wraps our class , and specify model and other resources.\n",
    "\n",
    "the `models` dict store model names and the assosiated model **dir** URL (the URL can start with `S3://` and other blob store options), the faster way is to use a shared file volume, we use `.apply(mount_v3io())` to attach a v3io (iguazio data fabric) volume to our function. By default v3io will mount the current user home into the `\\User` function path.\n",
    "\n",
    "**verify the model dir does contain a valid `model.bst` file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import new_model_server, mount_v3io\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-05-12 22:00:01,735 function spec saved to path: function.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7f02e5e61c18>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = new_model_server(\"churn-test\",\n",
    "                      model_class=\"ChurnModel\",\n",
    "                      models={\"churn_server_v1\": f\"{model_dir}\"})\n",
    "fn.spec.description = \"churn classification and predictor\"\n",
    "fn.metadata.categories = [\"serving\", \"ml\"]\n",
    "fn.metadata.labels = {\"author\": \"yashab\", \"framework\": \"churn\"}\n",
    "\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"V3IO_HOME\" in list(os.environ):\n",
    "    from mlrun import mount_v3io\n",
    "    fn.apply(mount_v3io())\n",
    "else:\n",
    "    # is you set up mlrun using the instructions at\n",
    "    # https://github.com/mlrun/mlrun/blob/master/hack/local/README.md\n",
    "    from mlrun.platforms import mount_pvc\n",
    "    fn.apply(mount_pvc(\"nfsvol\", \"nfsvol\", \"/home/joyan/data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-05-12 22:00:05,445 deploy started\n",
      "[nuclio] 2020-05-12 22:00:09,560 (info) Build complete\n",
      "[nuclio] 2020-05-12 22:00:23,745 (info) Function deploy complete\n",
      "[nuclio] 2020-05-12 22:00:23,751 done updating churn-project-churn-test, function address: 3.21.117.61:30144\n"
     ]
    }
   ],
   "source": [
    "addr = fn.deploy(project=\"churn-project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"test-model-server\"></a>\n",
    "### **test our model server using HTTP request**\n",
    "\n",
    "\n",
    "We invoke our model serving function using test data, the data vector is specified in the `instances` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFServing protocol event\n",
    "event_data = {\"instances\": xtest.values[:10,:-1].tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "resp = requests.put(addr + \"/churn_server_v1/predict\", json=json.dumps(event_data))\n",
    "\n",
    "# mlutils function for this?\n",
    "tl = resp.text.replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")\n",
    "assert preds == [int(i) for i in np.asarray(tl)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[back to top](#top)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
