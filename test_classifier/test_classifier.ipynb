{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio config kind = \"job\"\n",
    "%nuclio config spec.image = \"mlrun/ml-models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun.artifacts import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval_model(context, xtest, ytest, model, score_method=\"micro\", plots_dest=\"plots\"):\n",
    "    \"\"\"internal evaluate one model\n",
    "    \"\"\"\n",
    "    from sklearn import metrics\n",
    "    from cloudpickle import load\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    from sklearn.utils.multiclass import unique_labels\n",
    "    from mlrun.artifacts import PlotArtifact\n",
    "    from mlrun.mlutils import plot_roc, feature_importances, gcf_clear\n",
    "\n",
    "    # enclose all except model\n",
    "    ytestb = label_binarize(ytest, classes=ytest.unique())\n",
    "    \n",
    "    context.header = xtest.columns.names\n",
    "    \n",
    "    clf = load(open(model, \"rb\"))\n",
    "    if callable(getattr(clf, \"predict_proba\")):\n",
    "        y_score = clf.predict_proba(xtest.values, validate_features=False)\n",
    "        ypred = clf.predict(xtest.values, validate_features=False)\n",
    "        plot_roc(context, ytestb, y_score, key='roc', plots_dir=\"plots\")\n",
    "    else:\n",
    "        ypred = clf.predict(xtest.values, validate_features=False) # refactor\n",
    "        y_score = None\n",
    "\n",
    "    gcf_clear(plt)\n",
    "    metrics.plot_confusion_matrix(clf, xtest, ytest, \n",
    "                                  labels=ytest.unique(), normalize='true') \n",
    "    \n",
    "    context.log_artifact(PlotArtifact(\"confusion\", body=plt.gcf()), \n",
    "                         artifact_path_path=f\"{model.split('.')[-2]}-confusion.html\") \n",
    "    \n",
    "    if hasattr(clf, \"feature_importances_\"):\n",
    "        plot, tbl = feature_importances(clf, list(xtest.columns))\n",
    "        context.log_artifact(plot, local_path=\"plots/feature-importances.html\")\n",
    "        context.log_artifact(tbl, local_path=\"plots/feature-importances-table.csc\")\n",
    "\n",
    "    ytestb = label_binarize(ytest, classes=ytest.unique())\n",
    "\n",
    "    if ytestb.shape[1] > 1:\n",
    "        average_precision = metrics.average_precision_score(ytestb,\n",
    "                                                            y_score,\n",
    "                                                            average=score_method)\n",
    "        context.log_result(f\"rocauc\", metrics.roc_auc_score(ytestb, y_score))\n",
    "    else:\n",
    "        average_precision = metrics.average_precision_score(ytestb,\n",
    "                                                            y_score[:, 1],\n",
    "                                                            average=score_method)\n",
    "        context.log_result(f\"rocauc\", metrics.roc_auc_score(ytestb, y_score[:, 1]))\n",
    "\n",
    "    context.log_result(f\"avg_precscore\", average_precision)\n",
    "    context.log_result(f\"accuracy\", float(clf.score(xtest, ytest)))\n",
    "    context.log_result(f\"f1_score\", metrics.f1_score(ytest, ypred,\n",
    "                                                     average=score_method))\n",
    "    if y_score is None:\n",
    "        return y_score\n",
    "    else:\n",
    "        return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_classifier(\n",
    "    context,\n",
    "    models_path: DataItem, \n",
    "    test_set: DataItem,\n",
    "    label_column: str,\n",
    "    score_method: str = 'micro',\n",
    "    plots_dest: str = \"plots\",\n",
    "    model_evaluator = None\n",
    ") -> None:\n",
    "    \"\"\"Test one or more classifier models against held-out dataset\n",
    "    \n",
    "    Using held-out test features, evaluates the peformance of the estimated model\n",
    "    \n",
    "    Can be part of a kubeflow pipeline as a test step that is run post EDA and \n",
    "    training/validation cycles\n",
    "    \n",
    "    :param context:         the function context\n",
    "    :param models_path:     artifact models representing a file or a folder\n",
    "    :param test_set:        test features and labels\n",
    "    :param label_column:    column name for ground truth labels\n",
    "    :param score_method:    for multiclass classification\n",
    "    :param plots_dest:      dir for test plots\n",
    "    :param model_evaluator: NOT IMPLEMENTED: specific method to generate eval, passed in as string\n",
    "                            or available in this folder\n",
    "    \"\"\"\n",
    "    xtest = test_set.as_df()\n",
    "    ytest = xtest.pop(label_column)\n",
    "    \n",
    "    model_file, model_obj, _ = get_model(models_path.url, suffix='.pkl')\n",
    "    \n",
    "    # there could be different eval_models, type of model (xgboost, tfv1, tfv2...)\n",
    "    # or how probabilities are calculated, etc...\n",
    "    if not model_evaluator:\n",
    "        # binary and multiclass\n",
    "        y_hat = _eval_model(context, xtest, ytest, model_file, \n",
    "                            score_method,\n",
    "                            plots_dest or 'plots')\n",
    "\n",
    "    # give the prediction columns titles/headers\n",
    "    if y_hat.ndim == 1 or y_hat.shape[1] == 1:\n",
    "        score_names = [\"yscore\"]\n",
    "    else:\n",
    "        score_names = [\"yscore_\" + str(x) for x in range(y_hat.shape[1])]\n",
    "\n",
    "    # log the test set and its predictions (should also bind model and metadata)\n",
    "    df = pd.concat([xtest, ytest, pd.DataFrame(y_hat, columns=score_names)], axis=1)\n",
    "    context.log_dataset(\"test_set_preds\", df=df, format=\"parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import mlconf\n",
    "import os\n",
    "\n",
    "mlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'\n",
    "mlconf.artifact_path = mlconf.artifact_path or f'{os.environ[\"HOME\"]}/artifacts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function \n",
    "# create job function object from notebook code\n",
    "fn = code_to_function(\"test_classifier\")\n",
    "\n",
    "# add metadata (for templates and reuse)\n",
    "fn.spec.default_handler = \"test_classifier\"\n",
    "fn.spec.description = \"test a classifier using held-out or new data\"\n",
    "fn.metadata.categories = [\"ml\", \"test\"]\n",
    "fn.metadata.labels = {\"author\": \"yjb\", \"framework\": \"sklearn\"}\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"V3IO_HOME\" in list(os.environ):\n",
    "    from mlrun import mount_v3io\n",
    "    fn.apply(mount_v3io())\n",
    "else:\n",
    "    # is you set up mlrun using the instructions at https://github.com/mlrun/mlrun/blob/master/hack/local/README.md\n",
    "    from mlrun.platforms import mount_pvc\n",
    "    fn.apply(mount_pvc('nfsvol', 'nfsvol', '/home/jovyan/data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_params = {\n",
    "    \"name\" : \"tasks - test classifier\",\n",
    "    \"params\": {\n",
    "        # Ina pipeline setting, the models_path parameter would be the output of a training step\n",
    "        \"models_path\"   : mlconf.artifact_path + \"/models\",\n",
    "        \"label_column\"  : \"labels\",\n",
    "        \"plots_dest\"    : mlconf.artifact_path + \"/plots\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://raw.githubusercontent.com/yjb-ds/testdata/master/data/test_set.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import run_local, NewTask\n",
    "\n",
    "run = run_local(NewTask(**task_params),\n",
    "                handler=test_classifier,\n",
    "                inputs={\"test_set\": \"/User/artifacts/test_set.parquet\",\n",
    "                        \"models_path\": \"models\"},\n",
    "                workdir=mlconf.artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import NewTask\n",
    "run = fn.run(NewTask(**task_params), \n",
    "             inputs={\"test_set\": \"/User/artifacts/test_set.parquet\",\n",
    "                        \"models_path\": \"models\"},\n",
    "             workdir=\"/User/artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}