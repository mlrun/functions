{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Model - Serving Function\n",
    "\n",
    "This notebook demonstrates how to deploy a Tensorflow model using MLRun & Nuclio.\n",
    "\n",
    "**In this notebook you will:**\n",
    "* Write a Tensorflow-Model class to load and predict on the incoming data\n",
    "* Deploy the model as a serverless function\n",
    "* Invoke the serving endpoint with data as:\n",
    "  * URLs to images hosted on S3\n",
    "  * Direct image send\n",
    "  \n",
    "**Steps:**  \n",
    "* [Define Nuclio function](#Define-Nuclio-function)  \n",
    "  * [Install dependencies and set config](#Install-dependencies-and-set-config)  \n",
    "  * [Model serving class](#Model-Serving-Class)  \n",
    "* [Deploy the serving function to the cluster](#Deploy-the-serving-function-to-the-cluster)  \n",
    "* [Define test parameters](#Define-test-parameters)\n",
    "* [Test the deployed function on the cluster](#Test-the-deployed-function-on-the-cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Nuclio Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the magic commands for deploying this jupyter notebook as a nuclio function we must first import nuclio  \n",
    "Since we do not want to import nuclio in the actual function, the comment annotation `nuclio: ignore` is used. This marks the cell for nuclio, telling it to ignore the cell's values when building the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies and set config\n",
    "> Note: Since tensorflow 1.13.2 is being pulled from the baseimage it is not directly installed as a build command.\n",
    "If it is not installed on your system please uninstall and install using the line: `pip install tensorflow==1.13.2 keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio config spec.build.baseImage = \"mlrun/ml-serving:0.4.6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using packages which are not surely installed on our baseimage, or want to verify that a specific version of the package will be installed we use the `%nuclio cmd` annotation.  \n",
    ">`%nuclio cmd` works both locally and during deployment by default, but can be set with `-c` flag to only run the commands while deploying or `-l` to set the variable for the local environment only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nuclio cmd -c\n",
    "pip install tensorflow==1.13.2\n",
    "pip install keras requests pillow\n",
    "pip install numpy==1.16.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import load_img\n",
    "from os import environ, path\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "import mlrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Serving Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the `TFModel` class which we will use to define data handling and prediction of our model.  \n",
    "\n",
    "The class should consist of:\n",
    "* `__init__(name, model_dir)` - Setup the internal parameters\n",
    "* `load(self)` - How to load the model and broadcast it's ready for prediction\n",
    "* `preprocess(self, body)` - How to handle the incoming event, forming the request to an `{'instances': [<samples>]}` dictionary as requested by the protocol\n",
    "* `predict(self, data)` - Receives and `{'instances': [<samples>]}` and returns the model's prediction as a list\n",
    "* `postprocess(self, data)` - Does any additional processing needed on the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFModel():\n",
    "    def __init__(self, name: str, model_dir: str):\n",
    "        self.name = name\n",
    "        self.model_filepath = model_dir\n",
    "        self.model = None\n",
    "        self.ready = None\n",
    "\n",
    "        self.IMAGE_WIDTH = int(environ.get('IMAGE_WIDTH', '128'))\n",
    "        self.IMAGE_HEIGHT = int(environ.get('IMAGE_HEIGHT', '128'))\n",
    "        \n",
    "        try:\n",
    "            with open(environ['classes_map'], 'r') as f:\n",
    "                self.classes = json.load(f)\n",
    "        except:\n",
    "            self.classes = None\n",
    "        \n",
    "    def load(self):\n",
    "        self.model = load_model(self.model_filepath)\n",
    "\n",
    "        self.ready = True\n",
    "        \n",
    "    def preprocess(self, body):\n",
    "        try:\n",
    "            output = {'instances': []}\n",
    "            instances = body.get('instances', [])\n",
    "            for byte_image in instances:\n",
    "                img = Image.open(byte_image)\n",
    "                img = img.resize((self.IMAGE_WIDTH, self.IMAGE_HEIGHT))\n",
    "\n",
    "                # Load image\n",
    "                x = image.img_to_array(img)\n",
    "                x = np.expand_dims(x, axis=0)\n",
    "                output['instances'].append(x)\n",
    "            \n",
    "            # Format instances list\n",
    "            output['instances'] = [np.vstack(output['instances'])]\n",
    "            return output\n",
    "        except:\n",
    "            raise Exception(f'received: {body}')\n",
    "            \n",
    "\n",
    "    def predict(self, data):\n",
    "        images = data.get('instances', [])\n",
    "\n",
    "        # Predict\n",
    "        predicted_probability = self.model.predict(images)\n",
    "\n",
    "        # return prediction\n",
    "        return predicted_probability\n",
    "        \n",
    "    def postprocess(self, predicted_probability):\n",
    "        if self.classes:\n",
    "            predicted_classes = np.around(predicted_probability, 1).tolist()[0]\n",
    "            predicted_probabilities = predicted_probability.tolist()[0]\n",
    "            return {\n",
    "                'prediction': [self.classes[str(int(cls))] for cls in predicted_classes], \n",
    "                f'{self.classes[\"1\"]}-probability': predicted_probabilities\n",
    "            }\n",
    "        else:\n",
    "            return predicted_probability.tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To let our nuclio builder know that our function code ends at this point we will use the comment annotation `nuclio: end-code`.  \n",
    "\n",
    "Any new cell from now on will be treated as if a `nuclio: ignore` comment was set, and will not be added to the funcion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the function locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your local TF / Keras version is the same as pulled in the nuclio image for accurate testing\n",
    "\n",
    "Set the served models and their file paths using: `SERVING_MODEL_<name> = <model file path>`\n",
    "\n",
    "> Note: this notebook assumes the model and categories are under <b>/User/mlrun/examples/</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import os, requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define test parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing event\n",
    "cat_image_url = 'https://s3.amazonaws.com/iguazio-sample-data/images/catanddog/cat.102.jpg'\n",
    "response = requests.get(cat_image_url)\n",
    "cat_image = response.content\n",
    "img = Image.open(BytesIO(cat_image))\n",
    "\n",
    "print('Test image:')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Server variables\n",
    "model_class = 'TFModel'\n",
    "model_name = 'cat_vs_dog_v1' # Define for later use in tests\n",
    "models = {model_name: '/User/artifacts/images/models/cats_n_dogs.h5'}\n",
    "\n",
    "# Specific model variables\n",
    "function_envs = {\n",
    "    'IMAGE_HEIGHT': 128,\n",
    "    'IMAGE_WIDTH': 128,\n",
    "    'classes_map': os.path.join('/User/artifacts/images', 'categories_map.json'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run local test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add env variables to be available for the model\n",
    "for k, v in function_envs.items():\n",
    "    os.environ[k] = str(v)\n",
    "    \n",
    "# Instantiate the model class and load the model\n",
    "local_model = TFModel(model_name, models[model_name])\n",
    "local_model.load()\n",
    "\n",
    "# Process cat image byte array event\n",
    "event = {'instances': [BytesIO(cat_image)]}\n",
    "preprocessed_event = local_model.preprocess(event)\n",
    "prediction = local_model.predict(preprocessed_event)\n",
    "postprocessed_event = local_model.postprocess(prediction)\n",
    "\n",
    "# Display results\n",
    "display(postprocessed_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the serving function to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import new_model_server, mount_v3io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model server function\n",
    "fn = new_model_server('tf-images-server', \n",
    "                      model_class=model_class,\n",
    "                      models=models)\n",
    "fn.set_envs(function_envs)\n",
    "fn.apply(mount_v3io())\n",
    "\n",
    "# Deploy the model server\n",
    "addr = fn.deploy(project='nuclio-serving')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the deployed function on the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the deployed function (with URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL event\n",
    "event_body = json.dumps({\"data_url\": cat_image_url})\n",
    "print(f'Sending event: {event_body}')\n",
    "\n",
    "headers = {'Content-type': 'application/json'}\n",
    "response = requests.post(url=addr + f'/{model_name}/predict', data=event_body, headers=headers)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the deployed function (with Jpeg Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL event\n",
    "event_body = cat_image\n",
    "print(f'Sending image from {cat_image_url}')\n",
    "plt.imshow(img)\n",
    "\n",
    "headers = {'Content-type': 'image/jpeg'}\n",
    "response = requests.post(url=addr + f'/{model_name}/predict/', data=event_body, headers=headers)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_func = new_model_server('tf-images-server', model_class='TFModel')\n",
    "clean_func.with_http(workers=4)\n",
    "clean_func.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-4.6",
   "language": "python",
   "name": "mlrun-4.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
