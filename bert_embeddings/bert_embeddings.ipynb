{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embeddings Serverless Function\n",
    "This notebook presents deployment of pretrained BERT model that outputs embeddings for given textual sequences as a serverless function. Embeddings are meaningful, contextual representations of text in the form of ndarrays that are used frequently as input to various learning tasks in the field of NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nuclio cmd -c\n",
    "pip install torch\n",
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from typing import Union, List\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "def init_context(context):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    model.eval()\n",
    "    setattr(context.user_data, 'tokenizer', tokenizer)\n",
    "    setattr(context.user_data, 'model', model)\n",
    "\n",
    "def handler(context, event):\n",
    "    docs = json.loads(event.body)\n",
    "    docs = [doc.lower() for doc in docs]\n",
    "    docs = context.user_data.tokenizer.batch_encode_plus(docs, pad_to_max_length=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        embeddings = context.user_data.model(**docs)\n",
    "    embeddings = [embeddings[0].numpy(), embeddings[1].numpy()]\n",
    "    return pickle.dumps(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### local test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = nuclio.Event(body=json.dumps(['John loves Mary']))\n",
    "init_context(context)\n",
    "outputs = pickle.loads(handler(context, event))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good chance to view the outputs of this BERT model. It gives two different outputs. The first is a contextual embedding for each token in the input sequence and the second is a pooled embedding for the complete sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings per token shape: (1, 5, 768), pooled embeddings shape: (1, 768)\n"
     ]
    }
   ],
   "source": [
    "print(f'embeddings per token shape: {outputs[0].shape}, pooled embeddings shape: {outputs[1].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen both outputs share first dimension size of 1. This corresponds to the single sequence we passed as input, \"John loves Mary\". The last dimension for both is of size 768 which is the embedding dimension for this default configuration of bert. Note that the first input has an intermediate dimension of size 5 that corresponds to the number of tokens in the input sequence after addtion of two special tokens marking beginning and end of a sequence by the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy as serverless function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-06-11 13:16:26,161 function spec saved to path: function.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7f1649c00128>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun import code_to_function\n",
    "fn = code_to_function(\"bert-embeddings\", kind=\"nuclio\",\n",
    "                      description=\"Get BERT based embeddings for given text\",\n",
    "                      categories=[\"NLP\", \"BERT\", \"embeddings\"],\n",
    "                      labels = {\"author\": \"roye\", \"framework\": \"pytorch\"},\n",
    "                      code_output='.')\n",
    "\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-06-11 13:16:30,576 deploy started\n",
      "[nuclio] 2020-06-11 13:16:38,751 (info) Build complete\n",
      "[nuclio] 2020-06-11 13:16:58,965 (info) Function deploy complete\n",
      "[nuclio] 2020-06-11 13:16:58,972 done updating nlp-servers-bert-embeddings, function address: 192.168.224.208:31596\n",
      "[mlrun] 2020-06-11 13:16:58,982 warning!, server (0.4.7) and client (0.4.8) ver dont match\n"
     ]
    }
   ],
   "source": [
    "addr = fn.deploy(project='nlp-servers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the function via http request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "event_data = ['the quick brown fox jumps over the lazy dog', 'Hello I am Jacob']\n",
    "resp = requests.post(addr, json=json.dumps(event_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_embeddings = pickle.loads(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings per token shape: (2, 11, 768), pooled embeddings shape: (2, 768)\n"
     ]
    }
   ],
   "source": [
    "print(f'embeddings per token shape: {output_embeddings[0].shape}, pooled embeddings shape: {output_embeddings[1].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the size of the first dimension of the outputs is two since we passed in two sequences. Also the intermediate dimension of the first output is the maximal number of tokens across all input sequences. Sequences with less tokens are padded with zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
