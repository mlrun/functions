{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Operations\n",
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the MLRun environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import new_function, code_to_function, get_run_db, mount_v3io, NewTask, mlconf, new_model_server, run_local\n",
    "mlconf.dbpath = 'http://mlrun-api:8080'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting kind to 'job'\n",
      "%nuclio: setting spec.image to 'mlrun/ml-models:0.4.6'\n",
      "%nuclio: setting spec.description to 'Rolling aggregation over Metrics and Lables according to specifications'\n",
      "%nuclio: setting metadata.categories to ['preprocessing']\n"
     ]
    }
   ],
   "source": [
    "%nuclio config kind = \"job\"\n",
    "%nuclio config spec.image = \"mlrun/ml-models:0.4.6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(context,\n",
    "              df_artifact: Union[DataItem, str], \n",
    "              keys=None, \n",
    "              metrics=None, \n",
    "              labels=None, \n",
    "              metric_aggs=['mean'], \n",
    "              label_aggs=['max'], \n",
    "              suffix=None, \n",
    "              window=3, \n",
    "              center=False, \n",
    "              inplace=False,\n",
    "              save_to=None):\n",
    "    context.logger.info(f'Aggregating {df_artifact}')\n",
    "    input_df = pd.read_parquet(str(df_artifact))\n",
    "    \n",
    "    # Verify there is work to be done\n",
    "    if not (metrics or labels):\n",
    "        context.log_artifact('df', input_df)\n",
    "        return input_df\n",
    "    \n",
    "    # Select the correct indexes\n",
    "    if keys:\n",
    "        current_index = input_df.index.names\n",
    "        indexes_to_drop = [col for col in input_df.index.names if col not in keys]\n",
    "        df = input_df.reset_index(level=indexes_to_drop)\n",
    "    else:\n",
    "        df = input_df\n",
    "    \n",
    "    # For each metrics\n",
    "    if metrics:\n",
    "        metrics_df = df.loc[:, metrics].rolling(window=window,\n",
    "                                                center=center).aggregate(metric_aggs)\n",
    "        \n",
    "        # Flatten all the aggs\n",
    "        metrics_df.columns = ['_'.join(col).strip() for col in metrics_df.columns.values]\n",
    "        \n",
    "        # Add suffix\n",
    "        if suffix:\n",
    "            metrics_df.columns = [f'{metric}_{suffix}' for metric in metrics_df.columns]\n",
    "            \n",
    "        if not inplace:\n",
    "            final_df = pd.merge(input_df, metrics_df, suffixes=('', suffix), left_index=True, right_index=True)\n",
    "        else:\n",
    "            final_df = metrics_df\n",
    "\n",
    "    # For each label\n",
    "    if labels:\n",
    "        labels_df = df.loc[:, labels].rolling(window=window,\n",
    "                                              center=center).aggregate(label_aggs)\n",
    "        # Flatten all the aggs\n",
    "        labels_df.columns = ['_'.join(col).strip() for col in labels_df.columns.values]\n",
    "        \n",
    "        # Add suffix\n",
    "        if suffix:\n",
    "            labels_df.columns = [f'{label}_{suffix}' for label in labels_df.columns]\n",
    "            \n",
    "        if metrics:\n",
    "            final_df = pd.merge(final_df, labels_df, suffixes=('', suffix), left_index=True, right_index=True)   \n",
    "        else:\n",
    "            if not inplace:\n",
    "                final_df = pd.merge(input_df, labels_df, suffixes=('', suffix), left_index=True, right_index=True)      \n",
    "            else:\n",
    "                final_df = labels_df\n",
    "        \n",
    "    # Save the result dataframe\n",
    "    # TODO: Change to log_datset\n",
    "    context.log_dataset(key='aggregate', \n",
    "                        df=final_df, \n",
    "                        format='parquet',\n",
    "                        local_path=save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Do tests using data from the `Network Operations Demo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define V3IO Client\n",
    "import v3io_frames as v3f\n",
    "client = v3f.Client('framesd:8081', container='bigdata')\n",
    "\n",
    "# Define base dirs\n",
    "project_dir = os.path.join('/', 'User', 'demo-network-operations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = client.read('tsdb', 'netops_metrics', multi_index=True)\n",
    "metrics_pq = os.path.join(project_dir, 'data', 'metrics.pq')\n",
    "metrics.to_parquet(metrics_pq, engine='pyarrow', index=True)\n",
    "metrics.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = '/User/v3io/bigdata/netops_metrics_parquet/20200329T133835-20200329T143835.parquet'\n",
    "metrics = pd.read_parquet('/User/v3io/bigdata/netops_metrics_parquet/20200329T133835-20200329T143835.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Test\n",
    "Define the aggregate test task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_task = NewTask(name='aggregate',\n",
    "                         project='network-operations',\n",
    "                         params={'df_artifact': metrics_path,\n",
    "                                 'metrics': ['cpu_utilization'],\n",
    "                                 'labels': ['is_error'],\n",
    "                                 'metric_aggs': ['mean', 'sum'],\n",
    "                                 'label_aggs': ['max'],\n",
    "                                 'suffix': 'daily',\n",
    "                                 'inplace': False,\n",
    "                                 'window': 5,\n",
    "                                 'center': True},\n",
    "                         handler=aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_run = run_local(aggregate_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the code to an MLRun function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-04-02 16:40:16,301 function spec saved to path: function.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7fb42464fb00>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = code_to_function('aggregate',\n",
    "                      handler='aggregate')\n",
    "fn.spec.description = \"Rolling aggregation over Metrics and Lables according to specifications\"\n",
    "fn.metadata.categories =  [\"preprocessing\"]\n",
    "fn.export('function.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.apply(mount_v3io(remote='bigdata', mount_path='/User/v3io/bigdata')).run(aggregate_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(aggregate_run.outputs['aggregate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
