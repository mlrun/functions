{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio config kind = \"job\"\n",
    "%nuclio config spec.image = \"mlrun/ml-models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun.artifacts import get_model, update_model\n",
    "from mlrun.mlutils import eval_model_v2\n",
    "from cloudpickle import load\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def test_classifier(\n",
    "    context,\n",
    "    models_path: DataItem, \n",
    "    test_set: DataItem,\n",
    "    label_column: str,\n",
    "    score_method: str = 'micro',\n",
    "    plots_dest: str = \"\",\n",
    "    model_evaluator = None,\n",
    "    default_model: str = \"model.pkl\",\n",
    "    predictions_column: str = 'yscore',\n",
    "    model_update = True\n",
    ") -> None:\n",
    "    \"\"\"Test one or more classifier models against held-out dataset\n",
    "    \n",
    "    Using held-out test features, evaluates the peformance of the estimated model\n",
    "    \n",
    "    Can be part of a kubeflow pipeline as a test step that is run post EDA and \n",
    "    training/validation cycles\n",
    "    \n",
    "    :param context:            the function context\n",
    "    :param models_path:        artifact models representing a file or a folder\n",
    "    :param test_set:           test features and labels\n",
    "    :param label_column:       column name for ground truth labels\n",
    "    :param score_method:       for multiclass classification\n",
    "    :param plots_dest:         dir for test plots\n",
    "    :param model_evaluator:    NOT IMPLEMENTED: specific method to generate eval, passed in as string\n",
    "                               or available in this folder\n",
    "    :param predictions_column: column name for the predictions column on the resulted artifact\n",
    "    :param model_update:       (True) update model, when running as stand alone no need in update\n",
    "    \"\"\"\n",
    "    xtest = test_set.as_df()\n",
    "    ytest = xtest.pop(label_column)\n",
    "    \n",
    "    try:\n",
    "        model_file, model_obj, _ = get_model(models_path, suffix='.pkl')\n",
    "        model_obj = load(open(model_file, \"rb\"))\n",
    "    except Exception as a:\n",
    "        raise Exception(\"model location likely specified\")\n",
    "    \n",
    "    extra_data = eval_model_v2(context, xtest, ytest.values, model_obj)\n",
    "    if model_obj and model_update == True:\n",
    "        update_model(models_path, extra_data=extra_data, \n",
    "                     metrics=context.results, key_prefix='validation-')\n",
    "    \n",
    "    # get y_hat:\n",
    "    y_hat = model_obj.predict(xtest)\n",
    "    # give the prediction columns titles/headers\n",
    "    if y_hat.ndim == 1 or y_hat.shape[1] == 1:\n",
    "        score_names = [predictions_column]\n",
    "    else:\n",
    "        score_names = [f\"{predictions_column}_\" + str(x) for x in range(y_hat.shape[1])]\n",
    "\n",
    "    # log the test set and its predictions (should also bind model and metadata)\n",
    "    df = pd.concat([xtest, ytest, pd.DataFrame(y_hat, columns=score_names)], axis=1)\n",
    "    context.log_dataset(\"test_set_preds\", df=df, format=\"parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### mlconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlrun import mlconf\n",
    "import os\n",
    "\n",
    "mlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'\n",
    "artifact_path = mlconf.artifact_path or os.path.abspath('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlrun import code_to_function \n",
    "# create job function object from notebook code\n",
    "fn = code_to_function(\"test_classifier\", handler=\"test_classifier\",\n",
    "                      description=\"test a classifier using held-out or new data\",\n",
    "                      categories=[\"ml\", \"test\"],\n",
    "                      labels = {\"author\": \"yjb\", \"framework\": \"sklearn\"},\n",
    "                      code_output='.')\n",
    "fn.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlrun import mount_v3io\n",
    "fn.apply(mount_v3io())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_user = 'mlrun' \n",
    "DATA_PATH  = 'https://raw.githubusercontent.com/{}/functions/master/describe/iris_dataset.csv'.format(git_user)\n",
    "MODEL_PATH = 'https://raw.githubusercontent.com/{}/functions/master/test_classifier/model.pkl'.format(git_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlrun import run_local, NewTask\n",
    "\n",
    "run = run_local(NewTask(\n",
    "                    params= {'label_column':'label',\n",
    "                             'model_update': False}), #Change to True when you have a old model metadata to update\n",
    "                    handler=test_classifier,\n",
    "                    inputs={\"test_set\": DATA_PATH,\n",
    "                            \"models_path\": MODEL_PATH})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}