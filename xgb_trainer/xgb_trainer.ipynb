{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook function handles training and logging of xgboost models **only**, exposing both the sklearn and low level api's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## steps\n",
    "1. generate an xgboost model configuration by selecting one of 5 available types\n",
    "2. get a sample of data from a data source (random rows, consecutive rows, or the entire dataset)\n",
    "3. split the data into train, validation, and test sets.  \n",
    "\n",
    "> _PLEASE NOTE_:  there are many approaches to cross validation (cv) and as many ways to implement cv in scikit learn.  In this third stage, an alternative, two-way train and test split can be created.  The training set would then, for example, serve as input to a cross validation splitter.  The latter creates multiple training and validation subsets, called folds. These folds are then input, either in sequence or in parallel into the fit algorithm.\n",
    "\n",
    "4. train the model\n",
    "5. dump the model\n",
    "6. generate predictions and probabilities\n",
    "7. calibrate probabilities if needed\n",
    "8. calculate evaluation statistics and plots\n",
    "\n",
    "All these steps have been separated here into independent functions since many can be reused for other model types. Some of the following functions will be transferred in the `mlrun.mlutils` module. Additionally, each function contains its own imports in order to isolate and identify dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate an xgb model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_xgb_model(model_type: str, xgb_params: dict):\n",
    "    \"\"\"generate an xgboost model\n",
    "    \n",
    "    Multiple model types that can be estimated using\n",
    "    the XGBoost Scikit-Learn API\n",
    "    \n",
    "    :param model_type: one of \"classifier\", \"regressor\",\n",
    "                       \"ranker\", \"rf_classifier\", or\n",
    "                      \"rf_regressor\"\n",
    "    :param xgb_params: parameters passed through the \n",
    "                       function execution context\n",
    "    \"\"\"\n",
    "    from json import load\n",
    "    from mlrun.mlutils import get_class_fit, create_class\n",
    "\n",
    "    # generate model and fit function\n",
    "    mtypes = {\n",
    "        \"classifier\"   : \"xgboost.XGBClassifier\",\n",
    "        \"regressor\"    : \"xgboost.XGBRegressor\",\n",
    "        \"ranker\"       : \"xgboost.XGBRanker\",\n",
    "        \"rf_classifier\": \"xgboost.XGBRFClassifier\",\n",
    "        \"rf_regressor\" : \"xgboost.XGBRFRegressor\"\n",
    "    }\n",
    "    if model_type not in mtypes.keys():\n",
    "        raise Exception(\"unrecognized model types, see help documentation\")\n",
    "    \n",
    "    model_config = get_class_fit(mtypes[model_type])\n",
    "\n",
    "    for k, v in xgb_params:\n",
    "        if k.startswith(\"CLASS_\"):\n",
    "            model_config[\"CLASS\"][k[6:]] = v\n",
    "        if k.startswith(\"FIT_\"):\n",
    "            model_config[\"FIT\"][k[4:]] = v\n",
    "\n",
    "    ClassifierClass = create_class(model_config[\"META\"][\"class\"])\n",
    "    model = ClassifierClass(**model_config[\"CLASS\"])\n",
    "\n",
    "    return model, model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get a sample of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(src:str, sample: int, label: str, reader=None):\n",
    "    \"\"\"generate data sample to be split (candidate for mlrun)\n",
    "     \n",
    "    Returns features matrix and header (x), and labels (y)\n",
    "    :param src:    full path and filename of data artifact\n",
    "    :param sample: sample size from data source, use negative \n",
    "                   integers to sample randomly, positive to\n",
    "                   sample consecutively from the first row\n",
    "    :param label:  label column title\n",
    "    :param reader: pandas type reader (read_csv, read_parquet, ...) returning\n",
    "                   a pandas dataframe, and with a `dropna` attribute\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # read data function -- deprecate/add new mlrun dataset functionality\n",
    "    if not reader:\n",
    "        if src.endswith(\"csv\"):\n",
    "            reader = pd.read_csv\n",
    "        elif src.endswith(\"parquet\") or src.endswith(\"pq\"):\n",
    "            reader = pd.read_parquet\n",
    "        else:\n",
    "            raise Exception(f\"file type unhandled {src}\")\n",
    "\n",
    "    # get sample\n",
    "    if (sample == -1) or (sample >= 1):\n",
    "        # get all rows, or contiguous sample starting at row 1.\n",
    "        raw = reader(src).dropna()\n",
    "        labels = raw.pop(label)\n",
    "        raw = raw.iloc[:sample, :]\n",
    "        labels = labels.iloc[:sample]\n",
    "    else:\n",
    "        # grab a random sample\n",
    "        raw = reader(src).dropna().sample(sample * -1)\n",
    "        labels = raw.pop(label)\n",
    "\n",
    "    return raw, labels, raw.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data into train, validate and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(\n",
    "    raw, \n",
    "    labels, \n",
    "    n_ways: int = 3,\n",
    "    test_size: float = 0.15,\n",
    "    valid_size: float = 0.30,\n",
    "    label_names: list = [\"labels\"],\n",
    "    random_state: int = 1\n",
    "):\n",
    "    \"\"\"generate train and test sets (candidate for mlrun)\n",
    "\n",
    "    cross validation:\n",
    "    1. cut out a test set\n",
    "    2a. use the training set in a cross validation scheme, or\n",
    "    2b. make another split to generate a validation set\n",
    "    \n",
    "    2 parts (n_ways=2): train and test set only\n",
    "    3 parts (n_ways=3): train, validation and test set\n",
    "    4 parts (n_ways=4): n_ways=3 + a held-out probability calibration set\n",
    "    \n",
    "    :param raw:            dataframe or numpy array of raw features\n",
    "    :param labels:         dataframe or numpy array of raw labels\n",
    "    :param n_ways:         (3) split data into 2, 3, or 4 parts\n",
    "    :param test_size:      proportion of raw data to set asid as test data\n",
    "    :param valid_size:     proportion of remaining data to be set as validation\n",
    "    :param label_names:         label names\n",
    "    :param random_state:   (1) random number seed\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    if isinstance(raw, np.ndarray):\n",
    "        if labels.ndim==1:\n",
    "            labels=labels.reshape(-1,1)\n",
    "        xy = np.concatenate([raw, labels], axis=1)\n",
    "    else:\n",
    "        if isinstance(labels, pd.Series):\n",
    "            labels = pd.DataFrame(data=labels, columns=label_names)\n",
    "        xy = pd.concat([raw, labels], axis=1)\n",
    "        \n",
    "    x, xte, y, yte = train_test_split(xy, labels, test_size=test_size,\n",
    "                                      random_state=random_state)\n",
    "    if n_ways==2:\n",
    "        return (x, y), (xte, yte), None, None\n",
    "    elif n_ways==3:\n",
    "        xtr, xva, ytr, yva = train_test_split(x, y,train_size=valid_size,\n",
    "                                              random_state=random_state)\n",
    "        return (xtr, ytr), (xva, yva), (xte, yte), None\n",
    "    elif n_ways==4:\n",
    "        xt, xva, yt, yva = train_test_split(x, y,train_size=valid_size,\n",
    "                                              random_state=random_state)\n",
    "        xtr, xcal, ytr, ycal = train_test_split(xt, yt, train_size=0.8,\n",
    "                                              random_state=random_state)\n",
    "        return (xtr, ytr), (xva, yva), (xte, yte), (xcal, ycal)\n",
    "    else:\n",
    "        raise Exception(\"n_ways must be in the range [2,4]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the test data separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_set(\n",
    "    context, \n",
    "    xtest, \n",
    "    ytest, \n",
    "    header: list, \n",
    "    label: str = \"labels\", \n",
    "    file_ext: str = \"parquet\", \n",
    "    index: bool = False,\n",
    "    debug: bool = False\n",
    "):\n",
    "    \"\"\"log a held out test set\n",
    "\n",
    "    :param context:    the function execution context\n",
    "    :param xtest:      test features, as np.ndarray output from `get_splits`\n",
    "    :param ytest:      test labels, as np.ndarray output from `get_splits`\n",
    "    :param header:     ([])features header if required\n",
    "    :param label:      (\"labels\") name of label column\n",
    "    :param file_ext:   format of test set file\n",
    "    :param index:      preserve index column\n",
    "    :param debug:      (False)\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from mlrun import mlconf\n",
    "    \n",
    "    test_set = pd.concat(\n",
    "        [pd.DataFrame(data=xtest, columns=header),\n",
    "         pd.DataFrame(data=ytest.values, columns=[label])],\n",
    "        axis=1,)\n",
    "    \n",
    "    if debug:\n",
    "        test_set.to_parquet(mlconf.artifact_path+'/test_set-dev.parquet')\n",
    "        \n",
    "    context.log_dataset(\"test_set\", df=test_set, format=file_ext, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dump an xgb model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_xgb_model(\n",
    "    context, \n",
    "    model,\n",
    "    dump_type: str,\n",
    "    dest_folder: str,\n",
    "    dest_name: str\n",
    "):\n",
    "    \"\"\"serialize/log model\n",
    "    \n",
    "    XGBoost model can be save in 3 different ways:\n",
    "    1. pickle the internal _booster object, inside the model\n",
    "    2. using model.save_model(\"fn.bin\") using a legacy binary xgb format\n",
    "    2. using model.save_model(\"fn.json\") using a portable json format\n",
    "    \n",
    "    :param context:     the function\"s execution context\n",
    "    :param model:       the fitted xgboost model\n",
    "    :param dump_type:   \"pickle\" legacy\", or \"json\", \n",
    "    :param dest_folder: path for serialized model \n",
    "    :param dest_name:   name for serialized model file\n",
    "    \"\"\"\n",
    "    from cloudpickle import dumps, dump\n",
    "    try:\n",
    "        # if dump_type is \"pickle\":\n",
    "        # https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier.save_model\n",
    "        model.save_model(f\"{dest_folder}/{dest_name}-save_model.pkl\")\n",
    "        \n",
    "        # elif dump_type is \"json\":\n",
    "        # see https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
    "        # this save all contents as json\n",
    "        model.save_model(f\"{dest_folder}/{dest_name}-save_model.json\")\n",
    "        \n",
    "        # else:\n",
    "        # this saves all internal contents as pickle\n",
    "        _booster = model.get_booster()\n",
    "        dump(_booster, open(f\"{dest_folder}/{dest_name}-dump.pkl\", \"wb\"))\n",
    "        dump(model, open(f\"{dest_folder}/{dest_name}-dump-model.pkl\", \"wb\"))\n",
    "        \n",
    "        # log model needs to be spec\"ed:\n",
    "        data = dumps(_booster)\n",
    "        context.log_artifact(\"model\", body=data, local_path=f\"{dest_folder}/{dest_name}.pkl\")\n",
    "    except Exception as e:\n",
    "        print(\"xgboost model serialization error\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(\n",
    "    labels,\n",
    "    predictions,\n",
    "    classes,\n",
    "    normalize=\"all\",\n",
    "    title='Confusion matrix',\n",
    "    cmap=None\n",
    "):\n",
    "    \"\"\"prints and plots the confusion matrix.\n",
    "    \n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn import metrics\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "    \n",
    "    if not cmap:\n",
    "        cmap = plt.cm.Blues\n",
    "\n",
    "    cm = metrics.confusion_matrix(labels, predictions, normalize=normalize)\n",
    "    \n",
    "    # plt.gcf().set_size_inches(30, 10)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round(cm[i, j], 2),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(\n",
    "    context,\n",
    "    y_labels,\n",
    "    y_probs,\n",
    "    fpr_label: str = \"false positive rate\",\n",
    "    tpr_label: str = \"true positive rate\",\n",
    "    title: str = \"roc curve\",\n",
    "    legend_loc: str = \"best\",\n",
    "):\n",
    "    \"\"\"plot roc curves\n",
    "\n",
    "    TODO:  add averaging method (as string) that was used to create probs, \n",
    "    display in legend\n",
    "\n",
    "    :param context:      the function context\n",
    "    :param y_labels:     ground truth labels, hot encoded for multiclass  \n",
    "    :param y_probs:      model prediction probabilities\n",
    "    :param key:          (\"roc\") key of plot in artifact store\n",
    "    :param plots_dir:    (\"plots\") destination folder relative path to artifact path\n",
    "    :param fmt:          (\"png\") plot format\n",
    "    :param fpr_label:    (\"false positive rate\") x-axis labels\n",
    "    :param tpr_label:    (\"true positive rate\") y-axis labels\n",
    "    :param title:        (\"roc curve\") title of plot\n",
    "    :param legend_loc:   (\"best\") location of plot legend\n",
    "    \"\"\"\n",
    "    from sklearn import metrics\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mlrun.mlutils import gcf_clear\n",
    "    \n",
    "    # clear matplotlib current figure\n",
    "    gcf_clear(plt)\n",
    "\n",
    "    # draw 45 degree line\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "\n",
    "    # labelling\n",
    "    plt.xlabel(fpr_label)\n",
    "    plt.ylabel(tpr_label)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=legend_loc)\n",
    "\n",
    "    # single ROC or mutliple\n",
    "    if y_labels.ndim > 2:\n",
    "        # data accummulators by class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(y_labels[:, :-1].shape[1]):\n",
    "            fpr[i], tpr[i], _ = metrics.roc_curve(\n",
    "                y_labels[:, i], y_probs[:, i], pos_label=1\n",
    "            )\n",
    "            roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "            plt.plot(fpr[i], tpr[i], label=f\"class {i}\")\n",
    "    else:\n",
    "        fpr, tpr, _ = metrics.roc_curve(y_labels, y_probs[:,-1])\n",
    "        plt.plot(fpr, tpr, label=f\"positive class\")\n",
    "\n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_proba(\n",
    "    context,\n",
    "    feats,\n",
    "    labels,\n",
    "    model,\n",
    "    score_method,\n",
    "    plots_dest,\n",
    "    ntree_limit=None,\n",
    "    validate_features=True,\n",
    "    base_margin=None\n",
    "):\n",
    "    \"\"\" generate predictions and validation stats\n",
    "    \n",
    "    :param context:           the function execution context\n",
    "    :param feats:             validation features array \n",
    "    :param labels:            validation ground-truth labels\n",
    "    :param model:             estimated model\n",
    "    :param scrore_method:     (\"average\") multiclass scoring\n",
    "    :param plots_dest:        destination folder for plot artifacts\n",
    "    :param ntree_limit:       (None) limit no. trees used in prediction\n",
    "    :param validate_features: (True) ensure consistent feature names \n",
    "                              between model and input data\n",
    "    :param base_margin:       (None) undefined\n",
    "    \"\"\"\n",
    "    from sklearn import metrics\n",
    "    from mlrun.artifacts import PlotArtifact\n",
    "    from mlrun.mlutils import gcf_clear\n",
    "    from xgboost import XGBClassifier\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    ypred = model.predict(feats, False, ntree_limit, validate_features, base_margin)\n",
    "    \n",
    "    y_proba = []\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(feats, ntree_limit, validate_features, base_margin)\n",
    "    ypred_binary = [round(value) for value in y_proba[:,-1]]\n",
    "    \n",
    "    average_precision = metrics.average_precision_score(labels, y_proba[:,-1], average=score_method)\n",
    "    context.log_result(f\"avg_precision\", average_precision)\n",
    "    context.log_result(f\"rocauc\", metrics.roc_auc_score(labels, y_proba[:,-1]))\n",
    "    context.log_result(f\"accuracy_score\", float(metrics.accuracy_score(labels, ypred_binary)))\n",
    "    context.log_result(f\"f1_score\", metrics.f1_score(labels, ypred_binary, average=score_method))\n",
    "    \n",
    "    # ROC plot\n",
    "    context.log_artifact(PlotArtifact(\"roc\", body=plot_roc(context, labels, y_proba)),\n",
    "                         local_path=f\"{plots_dest}/roc.html\")\n",
    "    gcf_clear(plt)\n",
    "\n",
    "    body = plot_confusion_matrix(labels, ypred_binary, classes=labels.labels.unique()) \n",
    "    context.log_artifact(PlotArtifact(\"confusion\", body=body), local_path=f\"{plots_dest}/confusion.html\")\n",
    "    \n",
    "    return y_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calibrate probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proba_calibration(model, xcal, ycal):\n",
    "    \"\"\"calibrate output probabilities\n",
    "    \n",
    "    adapted from https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html#sphx-glr-auto-examples-calibration-plot-calibration-curve-py\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.calibration import (CalibratedClassifierCV,\n",
    "                                     calibration_curve)\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    def plot_calibration_curve(name, fig_index):\n",
    "        \"\"\"Plot calibration curve for est w/o and with calibration. \"\"\"\n",
    "        # Calibrated with isotonic calibration\n",
    "        isotonic = CalibratedClassifierCV(model, cv=\"prefit\", method='isotonic')\n",
    "\n",
    "        # Calibrated with sigmoid calibration\n",
    "        sigmoid = CalibratedClassifierCV(model, cv=\"prefit\", method='sigmoid')\n",
    "\n",
    "        # Logistic regression with no calibration as baseline\n",
    "        lr = LogisticRegression(C=1.)\n",
    "\n",
    "        fig = plt.figure(fig_index, figsize=(10, 10))\n",
    "        ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "        ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "        ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "        for clf, name in [(lr, 'Logistic'),\n",
    "                          (model, name),\n",
    "                          (isotonic, name + ' + Isotonic'),\n",
    "                          (sigmoid, name + ' + Sigmoid')]:\n",
    "            clf.fit(xcal, ycal)\n",
    "            y_pred = clf.predict(xcal)\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                prob_pos = clf.predict_proba(xcal)[:, 1]\n",
    "            else:  # use decision function\n",
    "                prob_pos = clf.decision_function(xcal)\n",
    "                prob_pos = \\\n",
    "                    (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "\n",
    "            clf_score = metrics.brier_score_loss(ycal, prob_pos, pos_label=1) # pos_label=ycal.max())\n",
    "            print(\"%s:\" % name)\n",
    "            print(\"\\tBrier: %1.3f\" % (clf_score))\n",
    "            print(\"\\tPrecision: %1.3f\" % metrics.precision_score(ycal, y_pred))\n",
    "            print(\"\\tRecall: %1.3f\" % metrics.recall_score(ycal, y_pred))\n",
    "            print(\"\\tF1: %1.3f\\n\" % metrics.f1_score(ycal, y_pred))\n",
    "\n",
    "            fraction_of_positives, mean_predicted_value = \\\n",
    "                calibration_curve(ycal, prob_pos, n_bins=10)\n",
    "\n",
    "            ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n",
    "                     label=\"%s (%1.3f)\" % (name, clf_score))\n",
    "\n",
    "            ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n",
    "                     histtype=\"step\", lw=2)\n",
    "\n",
    "        ax1.set_ylabel(\"Fraction of positives\")\n",
    "        ax1.set_ylim([-0.05, 1.05])\n",
    "        ax1.legend(loc=\"lower right\")\n",
    "        ax1.set_title('Calibration plots  (reliability curve)')\n",
    "\n",
    "        ax2.set_xlabel(\"Mean predicted value\")\n",
    "        ax2.set_ylabel(\"Count\")\n",
    "        ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "    # Plot calibration curve for XGBoost\n",
    "    plot_calibration_curve(\"XGBoost\", 1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    context,\n",
    "    model_type: str,\n",
    "    dataset,\n",
    "    label_column: str = \"labels\",\n",
    "    sample: int = -1,\n",
    "    test_size: float = 0.05,\n",
    "    valid_size: float = 0.75,\n",
    "    random_state: int = 1,\n",
    "    model_filename: str = \"model\",\n",
    "    models_dest: str = \"\",\n",
    "    plots_dest: str = \"\",\n",
    "    score_method: str = \"micro\",\n",
    "    file_ext: str = \"parquet\",\n",
    "    model_pkg_file: str = \"\",    \n",
    ") -> None:\n",
    "    \"\"\"train an xgboost model.\n",
    "\n",
    "    :param context:           the function context\n",
    "    :param model_type:        the model type to train, 'classifier', 'regressor'...\n",
    "    :param dataset:           (\"data\") name of raw data file\n",
    "    :param label_column:      ground-truth (y) labels\n",
    "    :param sample:            Selects the first n rows, or select a sample\n",
    "                              starting from the first. If negative <-1, select\n",
    "                              a random sample\n",
    "    :param model_filename:    model file filename,\n",
    "                              points to a directory\n",
    "    :param test_size:         (0.05) test set size\n",
    "    :param valid_size:          (0.75) Once the test set has been removed the\n",
    "                              training set gets this proportion.\n",
    "    :param random_state:      (1) sklearn rng seed\n",
    "    :param models_dest:       models subfolder on artifact path\n",
    "    :param plots_dest:        plot subfolder on artifact path\n",
    "    :param score_method:      for multiclass classification\n",
    "    \n",
    "    :param file_ext:          format for test_set_key hold out data\n",
    "    :param model_pkg_file:    json model config file                                  \n",
    "    \"\"\"\n",
    "    # deprecate:\n",
    "    models_dest = models_dest or \"models\"\n",
    "    plots_dest = plots_dest or f\"plots/{context.name}\"\n",
    "    \n",
    "    # get a sample from the raw data\n",
    "    raw, labels, header = get_sample(str(dataset), sample, label_column)\n",
    "    \n",
    "    # split the sample into train validate, test and calibration sets:\n",
    "    (xtr,ytr), (xva,yva), (xte,yte), (xcal, ycal) = get_splits(raw, labels, 4,\n",
    "                                                               test_size, \n",
    "                                                               valid_size, \n",
    "                                                               [\"labels\"],\n",
    "                                                               random_state)\n",
    "        \n",
    "    # get xgboost model and model config\n",
    "    model, model_config = gen_xgb_model(model_type, context.parameters.items())\n",
    "    \n",
    "    # update the model config with training data and callbacks\n",
    "    model_config[\"FIT\"].update({\"X\": xtr,\"y\": ytr.values})\n",
    "    \n",
    "    # run the fit\n",
    "    model.fit(**model_config[\"FIT\"])\n",
    "    \n",
    "    # serialize the model\n",
    "    dump_xgb_model(context, model, \"json\", models_dest, model_filename)\n",
    "\n",
    "    # generate predictions\n",
    "    y_proba = gen_proba(context, xva, yva, model, score_method, plots_dest)\n",
    "    \n",
    "    # calibrate probabilities\n",
    "    y_proba_cal = proba_calibration(model, xcal, ycal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import mlconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://mlrun-api:8080'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlconf.dbpath = mlconf.dbpath or \"./\"\n",
    "mlconf.dbpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/User/repos/functions/{name}/function.yaml'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vcs_branch = \"development\"\n",
    "base_vcs = f\"https://raw.githubusercontent.com/mlrun/functions/{vcs_branch}/\"\n",
    "\n",
    "mlconf.hub_url = mlconf.hub_url or base_vcs + f\"{name}/function.yaml\"\n",
    "mlconf.hub_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/User/artifacts'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "mlconf.artifact_path = mlconf.artifact_path or f\"{os.environ['V3IO_HOME']}/artifacts\"\n",
    "mlconf.artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TAG = os.environ[\"MLRUN_COMMIT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-04-28 00:07:25,218 saving function: xgb-trainer, tag: latest\n",
      "[mlrun] 2020-04-28 00:07:25,321 function spec saved to path: function.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7f83b6052748>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun import code_to_function \n",
    "# create job function object from notebook code\n",
    "fn = code_to_function(\"xgb_trainer\", kind=\"job\", with_doc=True,\n",
    "                      handler=train_model,\n",
    "                      image=f\"mlrun/ml-models:{TAG}\")\n",
    "\n",
    "# add metadata (for templates and reuse)\n",
    "fn.spec.default_handler = \"train_model\"\n",
    "fn.spec.description = \"train any classifier using scikit-learn's API\"\n",
    "fn.metadata.categories = [\"models\", \"classifier\"]\n",
    "fn.metadata.labels = {\"author\": \"yjb\"}\n",
    "\n",
    "fn.save()\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import import_function, mount_v3io, NewTask, run_local\n",
    "\n",
    "func = import_function(\"hub://xgb_trainer\")\n",
    "\n",
    "if \"V3IO_HOME\" in list(os.environ):\n",
    "    # mlrun on the iguazio platform\n",
    "    from mlrun import mount_v3io\n",
    "    fn.apply(mount_v3io())\n",
    "else:\n",
    "    # mlrun is setup using the instructions at \n",
    "    # https://github.com/mlrun/mlrun/blob/master/hack/local/README.md\n",
    "    from mlrun.platforms import mount_pvc\n",
    "    fn.apply(mount_pvc(\"nfsvol\", \"nfsvol\", \"/home/joyan/data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = False\n",
    "\n",
    "task_params = {\n",
    "    \"name\" : \"tasks xgb cpu trainer\",\n",
    "    \"params\" : {\n",
    "        \"model_type\"         : \"classifier\", # choose regressor, ranker, rfclassifier...\n",
    "        \"num_class\"          : 2,  # do not use this when binary\n",
    "        \"CLASS_tree_method\"  : \"gpu_hist\" if gpus else \"hist\",\n",
    "        \"CLASS_objective\"    : \"binary:logistic\",  # have this chosen by default\n",
    "        \"CLASS_random_state\" : 1,\n",
    "        \"sample\"             : -1,\n",
    "        \"label_column\"       : \"labels\",\n",
    "        \"test_size\"          : 0.10,\n",
    "        \"valid_size\"         : 0.75,\n",
    "        \"score_method\"       : \"weighted\",\n",
    "        \"models_dest\"        : os.path.join(mlconf.artifact_path, \"models\"),\n",
    "        \"plots_dest\"         : os.path.join(mlconf.artifact_path, \"plots\"),\n",
    "    }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-04-28 00:07:25,414 starting run tasks xgb cpu trainer uid=e10d57efe7504f1282214dd7621148c9  -> http://mlrun-api:8080\n",
      "[mlrun] 2020-04-28 00:07:25,688 Job is running in the background, pod: tasks-xgb-cpu-trainer-l4gpm\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "No handles with labels found to put in legend.\n",
      "[mlrun] 2020-04-28 00:07:33,283 log artifact model at /User/artifacts/e10d57efe7504f1282214dd7621148c9/User/artifacts/models/model.pkl, size: 24364, db: Y\n",
      "[mlrun] 2020-04-28 00:07:33,717 log artifact roc at /User/artifacts/e10d57efe7504f1282214dd7621148c9/User/artifacts/plots/roc.html, size: 30870, db: Y\n",
      "[mlrun] 2020-04-28 00:07:33,937 log artifact confusion at /User/artifacts/e10d57efe7504f1282214dd7621148c9/User/artifacts/plots/confusion.html, size: 22152, db: Y\n",
      "Logistic:\n",
      "\tBrier: 0.000\n",
      "\tPrecision: 1.000\n",
      "\tRecall: 1.000\n",
      "\tF1: 1.000\n",
      "\n",
      "XGBoost:\n",
      "\tBrier: 0.000\n",
      "\tPrecision: 1.000\n",
      "\tRecall: 1.000\n",
      "\tF1: 1.000\n",
      "\n",
      "[mlrun] 2020-04-28 00:07:34,663 Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/mlrun/runtimes/local.py\", line 184, in exec_from_params\n",
      "    val = handler(*args_list)\n",
      "  File \"main.py\", line 473, in train_model\n",
      "    y_proba_cal = proba_calibration(model, xcal, ycal)\n",
      "  File \"main.py\", line 410, in proba_calibration\n",
      "    plot_calibration_curve(\"XGBoost\", 1)\n",
      "  File \"main.py\", line 376, in plot_calibration_curve\n",
      "    clf.fit(xcal, ycal)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/sklearn/calibration.py\", line 165, in fit\n",
      "    calibrated_classifier.fit(X, y)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/sklearn/calibration.py\", line 347, in fit\n",
      "    df, idx_pos_class = self._preproc(X)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/sklearn/calibration.py\", line 306, in _preproc\n",
      "    df = self.base_estimator.predict_proba(X)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py\", line 937, in predict_proba\n",
      "    validate_features=validate_features)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/xgboost/core.py\", line 1443, in predict\n",
      "    self._validate_features(data)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/xgboost/core.py\", line 1862, in _validate_features\n",
      "    data.feature_names))\n",
      "ValueError: feature_names mismatch: ['feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6', 'feat_7', 'feat_8', 'feat_9', 'feat_10', 'feat_11', 'feat_12', 'feat_13', 'feat_14', 'feat_15', 'feat_16', 'feat_17', 'feat_18', 'feat_19', 'feat_20', 'feat_21', 'feat_22', 'feat_23', 'feat_24', 'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29', 'feat_30', 'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_35', 'feat_36', 'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41', 'feat_42', 'feat_43', 'feat_44', 'feat_45', 'feat_46', 'feat_47', 'feat_48', 'feat_49', 'feat_50', 'feat_51', 'feat_52', 'feat_53', 'feat_54', 'feat_55', 'feat_56', 'feat_57', 'feat_58', 'feat_59', 'feat_60', 'feat_61', 'feat_62', 'feat_63', 'labels'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64']\n",
      "expected feat_44, feat_52, feat_37, feat_55, feat_10, feat_3, feat_34, feat_23, feat_43, feat_49, feat_38, feat_36, feat_29, feat_42, feat_9, feat_47, feat_63, feat_51, feat_6, feat_45, feat_62, feat_2, feat_22, labels, feat_40, feat_8, feat_46, feat_12, feat_26, feat_5, feat_60, feat_39, feat_28, feat_59, feat_25, feat_18, feat_57, feat_1, feat_20, feat_0, feat_58, feat_16, feat_30, feat_35, feat_50, feat_56, feat_13, feat_4, feat_19, feat_33, feat_7, feat_61, feat_17, feat_15, feat_21, feat_41, feat_14, feat_31, feat_54, feat_27, feat_11, feat_53, feat_48, feat_24, feat_32 in input data\n",
      "training data did not have the following fields: f17, f52, f30, f7, f25, f13, f39, f53, f9, f44, f47, f23, f10, f14, f42, f31, f21, f5, f46, f3, f24, f64, f6, f15, f41, f54, f60, f16, f33, f18, f32, f38, f27, f22, f59, f26, f50, f1, f56, f2, f35, f36, f61, f20, f28, f37, f55, f63, f4, f43, f57, f62, f58, f29, f8, f48, f49, f12, f0, f40, f19, f34, f45, f11, f51\n",
      "\n",
      "\n",
      "[mlrun] 2020-04-28 00:07:34,706 exec error - feature_names mismatch: ['feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6', 'feat_7', 'feat_8', 'feat_9', 'feat_10', 'feat_11', 'feat_12', 'feat_13', 'feat_14', 'feat_15', 'feat_16', 'feat_17', 'feat_18', 'feat_19', 'feat_20', 'feat_21', 'feat_22', 'feat_23', 'feat_24', 'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29', 'feat_30', 'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_35', 'feat_36', 'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41', 'feat_42', 'feat_43', 'feat_44', 'feat_45', 'feat_46', 'feat_47', 'feat_48', 'feat_49', 'feat_50', 'feat_51', 'feat_52', 'feat_53', 'feat_54', 'feat_55', 'feat_56', 'feat_57', 'feat_58', 'feat_59', 'feat_60', 'feat_61', 'feat_62', 'feat_63', 'labels'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64']\n",
      "expected feat_44, feat_52, feat_37, feat_55, feat_10, feat_3, feat_34, feat_23, feat_43, feat_49, feat_38, feat_36, feat_29, feat_42, feat_9, feat_47, feat_63, feat_51, feat_6, feat_45, feat_62, feat_2, feat_22, labels, feat_40, feat_8, feat_46, feat_12, feat_26, feat_5, feat_60, feat_39, feat_28, feat_59, feat_25, feat_18, feat_57, feat_1, feat_20, feat_0, feat_58, feat_16, feat_30, feat_35, feat_50, feat_56, feat_13, feat_4, feat_19, feat_33, feat_7, feat_61, feat_17, feat_15, feat_21, feat_41, feat_14, feat_31, feat_54, feat_27, feat_11, feat_53, feat_48, feat_24, feat_32 in input data\n",
      "training data did not have the following fields: f17, f52, f30, f7, f25, f13, f39, f53, f9, f44, f47, f23, f10, f14, f42, f31, f21, f5, f46, f3, f24, f64, f6, f15, f41, f54, f60, f16, f33, f18, f32, f38, f27, f22, f59, f26, f50, f1, f56, f2, f35, f36, f61, f20, f28, f37, f55, f63, f4, f43, f57, f62, f58, f29, f8, f48, f49, f12, f0, f40, f19, f34, f45, f11, f51\n",
      "[mlrun] 2020-04-28 00:07:34,794 run executed, status=error\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "feature_names mismatch: ['feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6', 'feat_7', 'feat_8', 'feat_9', 'feat_10', 'feat_11', 'feat_12', 'feat_13', 'feat_14', 'feat_15', 'feat_16', 'feat_17', 'feat_18', 'feat_19', 'feat_20', 'feat_21', 'feat_22', 'feat_23', 'feat_24', 'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29', 'feat_30', 'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_35', 'feat_36', 'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41', 'feat_42', 'feat_43', 'feat_44', 'feat_45', 'feat_46', 'feat_47', 'feat_48', 'feat_49', 'feat_50', 'feat_51', 'feat_52', 'feat_53', 'feat_54', 'feat_55', 'feat_56', 'feat_57', 'feat_58', 'feat_59', 'feat_60', 'feat_61', 'feat_62', 'feat_63', 'labels'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64']\n",
      "expected feat_44, feat_52, feat_37, feat_55, feat_10, feat_3, feat_34, feat_23, feat_43, feat_49, feat_38, feat_36, feat_29, feat_42, feat_9, feat_47, feat_63, feat_51, feat_6, feat_45, feat_62, feat_2, feat_22, labels, feat_40, feat_8, feat_46, feat_12, feat_26, feat_5, feat_60, feat_39, feat_28, feat_59, feat_25, feat_18, feat_57, feat_1, feat_20, feat_0, feat_58, feat_16, feat_30, feat_35, feat_50, feat_56, feat_13, feat_4, feat_19, feat_33, feat_7, feat_61, feat_17, feat_15, feat_21, feat_41, feat_14, feat_31, feat_54, feat_27, feat_11, feat_53, feat_48, feat_24, feat_32 in input data\n",
      "training data did not have the following fields: f17, f52, f30, f7, f25, f13, f39, f53, f9, f44, f47, f23, f10, f14, f42, f31, f21, f5, f46, f3, f24, f64, f6, f15, f41, f54, f60, f16, f33, f18, f32, f38, f27, f22, f59, f26, f50, f1, f56, f2, f35, f36, f61, f20, f28, f37, f55, f63, f4, f43, f57, f62, f58, f29, f8, f48, f49, f12, f0, f40, f19, f34, f45, f11, f51\n",
      "runtime error: feature_names mismatch: ['feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6', 'feat_7', 'feat_8', 'feat_9', 'feat_10', 'feat_11', 'feat_12', 'feat_13', 'feat_14', 'feat_15', 'feat_16', 'feat_17', 'feat_18', 'feat_19', 'feat_20', 'feat_21', 'feat_22', 'feat_23', 'feat_24', 'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29', 'feat_30', 'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_35', 'feat_36', 'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41', 'feat_42', 'feat_43', 'feat_44', 'feat_45', 'feat_46', 'feat_47', 'feat_48', 'feat_49', 'feat_50', 'feat_51', 'feat_52', 'feat_53', 'feat_54', 'feat_55', 'feat_56', 'feat_57', 'feat_58', 'feat_59', 'feat_60', 'feat_61', 'feat_62', 'feat_63', 'labels'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64']\n",
      "expected feat_44, feat_52, feat_37, feat_55, feat_10, feat_3, feat_34, feat_23, feat_43, feat_49, feat_38, feat_36, feat_29, feat_42, feat_9, feat_47, feat_63, feat_51, feat_6, feat_45, feat_62, feat_2, feat_22, labels, feat_40, feat_8, feat_46, feat_12, feat_26, feat_5, feat_60, feat_39, feat_28, feat_59, feat_25, feat_18, feat_57, feat_1, feat_20, feat_0, feat_58, feat_16, feat_30, feat_35, feat_50, feat_56, feat_13, feat_4, feat_19, feat_33, feat_7, feat_61, feat_17, feat_15, feat_21, feat_41, feat_14, feat_31, feat_54, feat_27, feat_11, feat_53, feat_48, feat_24, feat_32 in input data\n",
      "training data did not have the following fields: f17, f52, f30, f7, f25, f13, f39, f53, f9, f44, f47, f23, f10, f14, f42, f31, f21, f5, f46, f3, f24, f64, f6, f15, f41, f54, f60, f16, f33, f18, f32, f38, f27, f22, f59, f26, f50, f1, f56, f2, f35, f36, f61, f20, f28, f37, f55, f63, f4, f43, f57, f62, f58, f29, f8, f48, f49, f12, f0, f40, f19, f34, f45, f11, f51\n",
      "final state: failed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style> \n",
       ".dictlist {\n",
       "  background-color: #b3edff; \n",
       "  text-align: center; \n",
       "  margin: 4px; \n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer; \n",
       "  background-color: #ffe6cc; \n",
       "  text-align: left; \n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #ffe6cc;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "  \n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "  \n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }  \n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "  \n",
       "  \n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>default</td>\n",
       "      <td><div title=\"e10d57efe7504f1282214dd7621148c9\"><a href=\"https://mlrun-ui.default-tenant.app.yjb-mlrun-dav.iguazio-cd1.com/projects/default/jobs/e10d57efe7504f1282214dd7621148c9/info\" target=\"_blank\" >...621148c9</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Apr 28 00:07:29</td>\n",
       "      <td><div style=\"color: red;\" title=\"feature_names mismatch: ['feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6', 'feat_7', 'feat_8', 'feat_9', 'feat_10', 'feat_11', 'feat_12', 'feat_13', 'feat_14', 'feat_15', 'feat_16', 'feat_17', 'feat_18', 'feat_19', 'feat_20', 'feat_21', 'feat_22', 'feat_23', 'feat_24', 'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29', 'feat_30', 'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_35', 'feat_36', 'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41', 'feat_42', 'feat_43', 'feat_44', 'feat_45', 'feat_46', 'feat_47', 'feat_48', 'feat_49', 'feat_50', 'feat_51', 'feat_52', 'feat_53', 'feat_54', 'feat_55', 'feat_56', 'feat_57', 'feat_58', 'feat_59', 'feat_60', 'feat_61', 'feat_62', 'feat_63', 'labels'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64']\\nexpected feat_44, feat_52, feat_37, feat_55, feat_10, feat_3, feat_34, feat_23, feat_43, feat_49, feat_38, feat_36, feat_29, feat_42, feat_9, feat_47, feat_63, feat_51, feat_6, feat_45, feat_62, feat_2, feat_22, labels, feat_40, feat_8, feat_46, feat_12, feat_26, feat_5, feat_60, feat_39, feat_28, feat_59, feat_25, feat_18, feat_57, feat_1, feat_20, feat_0, feat_58, feat_16, feat_30, feat_35, feat_50, feat_56, feat_13, feat_4, feat_19, feat_33, feat_7, feat_61, feat_17, feat_15, feat_21, feat_41, feat_14, feat_31, feat_54, feat_27, feat_11, feat_53, feat_48, feat_24, feat_32 in input data\\ntraining data did not have the following fields: f17, f52, f30, f7, f25, f13, f39, f53, f9, f44, f47, f23, f10, f14, f42, f31, f21, f5, f46, f3, f24, f64, f6, f15, f41, f54, f60, f16, f33, f18, f32, f38, f27, f22, f59, f26, f50, f1, f56, f2, f35, f36, f61, f20, f28, f37, f55, f63, f4, f43, f57, f62, f58, f29, f8, f48, f49, f12, f0, f40, f19, f34, f45, f11, f51\">error</div></td>\n",
       "      <td>tasks xgb cpu trainer</td>\n",
       "      <td><div class=\"dictlist\">host=tasks-xgb-cpu-trainer-l4gpm</div><div class=\"dictlist\">kind=job</div><div class=\"dictlist\">owner=admin</div><div class=\"dictlist\">v3io_user=admin</div></td>\n",
       "      <td><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"result4c2d1b81\" title=\"/files/artifacts/classifier-data.csv\">dataset</div></td>\n",
       "      <td><div class=\"dictlist\">CLASS_objective=binary:logistic</div><div class=\"dictlist\">CLASS_random_state=1</div><div class=\"dictlist\">CLASS_tree_method=hist</div><div class=\"dictlist\">label_column=labels</div><div class=\"dictlist\">model_type=classifier</div><div class=\"dictlist\">models_dest=/User/artifacts/models</div><div class=\"dictlist\">num_class=2</div><div class=\"dictlist\">plots_dest=/User/artifacts/plots</div><div class=\"dictlist\">sample=-1</div><div class=\"dictlist\">score_method=weighted</div><div class=\"dictlist\">test_size=0.1</div><div class=\"dictlist\">valid_size=0.75</div></td>\n",
       "      <td><div class=\"dictlist\">accuracy_score=1.0</div><div class=\"dictlist\">avg_precision=1.0</div><div class=\"dictlist\">f1_score=1.0</div><div class=\"dictlist\">rocauc=1.0</div></td>\n",
       "      <td><div title=\"/User/artifacts/e10d57efe7504f1282214dd7621148c9/User/artifacts/models/model.pkl\">model</div><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"result4c2d1b81\" title=\"/files/artifacts/e10d57efe7504f1282214dd7621148c9/files/artifacts/plots/roc.html\">roc</div><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"result4c2d1b81\" title=\"/files/artifacts/e10d57efe7504f1282214dd7621148c9/files/artifacts/plots/confusion.html\">confusion</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result4c2d1b81-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result4c2d1b81-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result4c2d1b81\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result4c2d1b81-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to track results use .show() or .logs() or in CLI: \n",
      "!mlrun get run e10d57efe7504f1282214dd7621148c9  , !mlrun logs e10d57efe7504f1282214dd7621148c9 \n",
      "[mlrun] 2020-04-28 00:07:37,949 run executed, status=error\n",
      "runtime error: feature_names mismatch: ['feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6', 'feat_7', 'feat_8', 'feat_9', 'feat_10', 'feat_11', 'feat_12', 'feat_13', 'feat_14', 'feat_15', 'feat_16', 'feat_17', 'feat_18', 'feat_19', 'feat_20', 'feat_21', 'feat_22', 'feat_23', 'feat_24', 'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29', 'feat_30', 'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_35', 'feat_36', 'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41', 'feat_42', 'feat_43', 'feat_44', 'feat_45', 'feat_46', 'feat_47', 'feat_48', 'feat_49', 'feat_50', 'feat_51', 'feat_52', 'feat_53', 'feat_54', 'feat_55', 'feat_56', 'feat_57', 'feat_58', 'feat_59', 'feat_60', 'feat_61', 'feat_62', 'feat_63', 'labels'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64']\n",
      "expected feat_44, feat_52, feat_37, feat_55, feat_10, feat_3, feat_34, feat_23, feat_43, feat_49, feat_38, feat_36, feat_29, feat_42, feat_9, feat_47, feat_63, feat_51, feat_6, feat_45, feat_62, feat_2, feat_22, labels, feat_40, feat_8, feat_46, feat_12, feat_26, feat_5, feat_60, feat_39, feat_28, feat_59, feat_25, feat_18, feat_57, feat_1, feat_20, feat_0, feat_58, feat_16, feat_30, feat_35, feat_50, feat_56, feat_13, feat_4, feat_19, feat_33, feat_7, feat_61, feat_17, feat_15, feat_21, feat_41, feat_14, feat_31, feat_54, feat_27, feat_11, feat_53, feat_48, feat_24, feat_32 in input data\n",
      "training data did not have the following fields: f17, f52, f30, f7, f25, f13, f39, f53, f9, f44, f47, f23, f10, f14, f42, f31, f21, f5, f46, f3, f24, f64, f6, f15, f41, f54, f60, f16, f33, f18, f32, f38, f27, f22, f59, f26, f50, f1, f56, f2, f35, f36, f61, f20, f28, f37, f55, f63, f4, f43, f57, f62, f58, f29, f8, f48, f49, f12, f0, f40, f19, f34, f45, f11, f51\n"
     ]
    },
    {
     "ename": "RunError",
     "evalue": "feature_names mismatch: ['feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6', 'feat_7', 'feat_8', 'feat_9', 'feat_10', 'feat_11', 'feat_12', 'feat_13', 'feat_14', 'feat_15', 'feat_16', 'feat_17', 'feat_18', 'feat_19', 'feat_20', 'feat_21', 'feat_22', 'feat_23', 'feat_24', 'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29', 'feat_30', 'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_35', 'feat_36', 'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41', 'feat_42', 'feat_43', 'feat_44', 'feat_45', 'feat_46', 'feat_47', 'feat_48', 'feat_49', 'feat_50', 'feat_51', 'feat_52', 'feat_53', 'feat_54', 'feat_55', 'feat_56', 'feat_57', 'feat_58', 'feat_59', 'feat_60', 'feat_61', 'feat_62', 'feat_63', 'labels'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64']\nexpected feat_44, feat_52, feat_37, feat_55, feat_10, feat_3, feat_34, feat_23, feat_43, feat_49, feat_38, feat_36, feat_29, feat_42, feat_9, feat_47, feat_63, feat_51, feat_6, feat_45, feat_62, feat_2, feat_22, labels, feat_40, feat_8, feat_46, feat_12, feat_26, feat_5, feat_60, feat_39, feat_28, feat_59, feat_25, feat_18, feat_57, feat_1, feat_20, feat_0, feat_58, feat_16, feat_30, feat_35, feat_50, feat_56, feat_13, feat_4, feat_19, feat_33, feat_7, feat_61, feat_17, feat_15, feat_21, feat_41, feat_14, feat_31, feat_54, feat_27, feat_11, feat_53, feat_48, feat_24, feat_32 in input data\ntraining data did not have the following fields: f17, f52, f30, f7, f25, f13, f39, f53, f9, f44, f47, f23, f10, f14, f42, f31, f21, f5, f46, f3, f24, f64, f6, f15, f41, f54, f60, f16, f33, f18, f32, f38, f27, f22, f59, f26, f50, f1, f56, f2, f35, f36, f61, f20, f28, f37, f55, f63, f4, f43, f57, f62, f58, f29, f8, f48, f49, f12, f0, f40, f19, f34, f45, f11, f51",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRunError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2aed0ac6e3e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m run = fn.run(\n\u001b[1;32m      2\u001b[0m     \u001b[0mNewTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtask_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     inputs={\"dataset\"  : os.path.join(mlconf.artifact_path, \"classifier-data.csv\")})\n\u001b[0m",
      "\u001b[0;32m~/.pythonlibs/jupyter/lib/python3.6/site-packages/mlrun/runtimes/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, runspec, handler, name, project, params, inputs, out_path, workdir, artifact_path, watch, schedule)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrunspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_remote\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_api_server\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pythonlibs/jupyter/lib/python3.6/site-packages/mlrun/runtimes/base.py\u001b[0m in \u001b[0;36m_wrap_result\u001b[0;34m(self, result, runspec, err)\u001b[0m\n\u001b[1;32m    363\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_remote\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'runtime error: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRunError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRunError\u001b[0m: feature_names mismatch: ['feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6', 'feat_7', 'feat_8', 'feat_9', 'feat_10', 'feat_11', 'feat_12', 'feat_13', 'feat_14', 'feat_15', 'feat_16', 'feat_17', 'feat_18', 'feat_19', 'feat_20', 'feat_21', 'feat_22', 'feat_23', 'feat_24', 'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29', 'feat_30', 'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_35', 'feat_36', 'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41', 'feat_42', 'feat_43', 'feat_44', 'feat_45', 'feat_46', 'feat_47', 'feat_48', 'feat_49', 'feat_50', 'feat_51', 'feat_52', 'feat_53', 'feat_54', 'feat_55', 'feat_56', 'feat_57', 'feat_58', 'feat_59', 'feat_60', 'feat_61', 'feat_62', 'feat_63', 'labels'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64']\nexpected feat_44, feat_52, feat_37, feat_55, feat_10, feat_3, feat_34, feat_23, feat_43, feat_49, feat_38, feat_36, feat_29, feat_42, feat_9, feat_47, feat_63, feat_51, feat_6, feat_45, feat_62, feat_2, feat_22, labels, feat_40, feat_8, feat_46, feat_12, feat_26, feat_5, feat_60, feat_39, feat_28, feat_59, feat_25, feat_18, feat_57, feat_1, feat_20, feat_0, feat_58, feat_16, feat_30, feat_35, feat_50, feat_56, feat_13, feat_4, feat_19, feat_33, feat_7, feat_61, feat_17, feat_15, feat_21, feat_41, feat_14, feat_31, feat_54, feat_27, feat_11, feat_53, feat_48, feat_24, feat_32 in input data\ntraining data did not have the following fields: f17, f52, f30, f7, f25, f13, f39, f53, f9, f44, f47, f23, f10, f14, f42, f31, f21, f5, f46, f3, f24, f64, f6, f15, f41, f54, f60, f16, f33, f18, f32, f38, f27, f22, f59, f26, f50, f1, f56, f2, f35, f36, f61, f20, f28, f37, f55, f63, f4, f43, f57, f62, f58, f29, f8, f48, f49, f12, f0, f40, f19, f34, f45, f11, f51"
     ]
    }
   ],
   "source": [
    "run = fn.run(\n",
    "    NewTask(**task_params),\n",
    "    inputs={\"dataset\"  : os.path.join(mlconf.artifact_path, \"classifier-data.csv\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tests\n",
    "\n",
    "WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gen_xgb_model():\n",
    "    import xgboost\n",
    "    c, j = gen_xgb_model(\"rf_classifier\", {})\n",
    "    assert isinstance(c, xgboost.XGBRFClassifier)\n",
    "test_gen_xgb_model()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_sample():\n",
    "    from mlrun import mlconf\n",
    "    r, l, h = get_sample(mlconf.artifact_path+\"/breast_cancer.parquet\", -1, \"labels\")\n",
    "    assert r.shape[0]==l.shape[0]\n",
    "test_get_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_splits():\n",
    "    from mlrun import mlconf\n",
    "    r, l, h = get_sample(mlconf.artifact_path+\"/classifier-data.csv\", -1, \"labels\")\n",
    "    (xtr, ytr), (xva, yva), (xte, yte), (xcal, ycal) = get_splits(r, l, 4)\n",
    "\n",
    "    assert xtr.shape[0]+xva.shape[0]+xte.shape[0]+xcal.shape[0] == r.shape[0]\n",
    "test_get_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_save_test_set():\n",
    "    r, l, h = get_sample(mlconf.artifact_path+\"/classifier-data.csv\", -1, \"labels\")\n",
    "    (xtr, ytr), (xva, yva), (xte, yte) = get_splits(r,l)\n",
    "    from mlrun import get_or_create_ctx\n",
    "    save_test_set(get_or_create_ctx(\"test\"), xte, yte, h, debug=True)\n",
    "    import pandas as pd\n",
    "    # pd.read_parquet()\n",
    "    # assert\n",
    "test_save_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dump_xgb_model_pickle():\n",
    "    \"\"\"\n",
    "    note that this approach doesn't require information\n",
    "    about the underlying package, xgboost\n",
    "    \"\"\"\n",
    "    from cloudpickle import load\n",
    "    from mlrun import mlconf\n",
    "    \n",
    "    booster = load(open(mlconf.artifact_path + \"/models/xgb-dump.pkl\", \"rb\"))\n",
    "    assert \"booster\" in booster.__dict__.keys()\n",
    "\n",
    "test_dump_xgb_model_pickle()\n",
    "\n",
    "def test_dump_xgb_save_model():\n",
    "    import xgboost as xgb\n",
    "    from mlrun import mlconf\n",
    "\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(fname=mlconf.artifact_path + \"/models/xgb-save_model.pkl\")\n",
    "    assert \"booster\" in booster.__dict__.keys()\n",
    "\n",
    "test_dump_xgb_save_model()\n",
    "\n",
    "def test_dump_xgb_json_save_model():\n",
    "    import xgboost as xgb\n",
    "    from mlrun import mlconf\n",
    "\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(fname=mlconf.artifact_path + \"/models/xgb-save_model.json\")\n",
    "    assert \"booster\" in booster.__dict__.keys()\n",
    "\n",
    "test_dump_xgb_json_save_model()\n",
    "\n",
    "def test_gen_proba():\n",
    "    from cloudpickle import load\n",
    "    from mlrun import mlconf, get_or_create_ctx\n",
    "    \n",
    "    model = load(open(mlconf.artifact_path+'/models/xgb-dump.pkl', 'rb'))\n",
    "    r, l, h = get_sample(mlconf.artifact_path+\"/breast_cancer.parquet\", -1, \"labels\")\n",
    "    (xtr, ytr), (xva, yva), (xte, yte) = get_splits(r,l)\n",
    "    y_proba = gen_proba(get_or_create_ctx('test'),\n",
    "                        xtr, ytr, model, \"macro\",\n",
    "                        mlconf.artifact_path+'/plots')\n",
    "    return y_proba, model\n",
    "\n",
    "y_proba, model = test_gen_proba()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
