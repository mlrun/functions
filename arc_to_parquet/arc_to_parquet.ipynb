{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 Iguazio\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn\"t verify HTTPS certificates by default\n",
    "        pass\n",
    "else:\n",
    "    # Handle target environment that doesn\"t support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from cloudpickle import dump, load\n",
    "\n",
    "from mlrun.execution import MLClientCtx\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun.artifacts import PlotArtifact, TableArtifact\n",
    "\n",
    "from typing import IO, AnyStr, Union, List, Optional\n",
    "\n",
    "def arc_to_parquet(\n",
    "    context: MLClientCtx,\n",
    "    archive_url: Union[str, DataItem],\n",
    "    header: Optional[List[str]] = None,\n",
    "    chunksize: int = 10_000,\n",
    "    dtype=None,\n",
    "    encoding: str = \"latin-1\",\n",
    "    key: str = \"data\",\n",
    "    dataset: Optional[str] = None,\n",
    "    part_cols = [],\n",
    "    file_ext: str = 'parquet'\n",
    ") -> None:\n",
    "    \"\"\"Open a file/object archive and save as a parquet file.\n",
    "\n",
    "    Partitioning requires precise specification of column types.\n",
    "\n",
    "    :param context:      function context\n",
    "    :param archive_url:  any valid string path consistent with the path variable\n",
    "                         of pandas.read_csv, including strings as file paths, as urls, \n",
    "                         pathlib.Path objects, etc...\n",
    "    :param header:       column names\n",
    "    :param chunksize:    (0) row size retrieved per iteration\n",
    "    :param dtype         destination data type of specified columns\n",
    "    :param encoding      (\"latin-8\") file encoding\n",
    "    :param key:          key in artifact store (when log_data=True)\n",
    "    :param dataset:      (None) if not None then \"target_path/dataset\"\n",
    "                         is folder for partitioned files\n",
    "    :param file_ext:     (parquet) csv/parquet file extension\n",
    "    :param part_cols:    ([]) list of partitioning columns\n",
    "\n",
    "    \"\"\"\n",
    "    base_path = context.artifact_path\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    if dataset is not None:\n",
    "        dest_path = os.path.join(base_path, dataset)\n",
    "        exists = os.path.isdir(dest_path)\n",
    "    else:\n",
    "        dest_path = os.path.join(base_path, key+f\".{file_ext}\")\n",
    "        exists = os.path.isfile(dest_path)\n",
    "\n",
    "    # todo: more logic for header\n",
    "    if not exists:\n",
    "        context.logger.info(\"destination file does not exist, downloading\")\n",
    "        pqwriter = None\n",
    "        for i, df in enumerate(pd.read_csv(archive_url, \n",
    "                                           chunksize=chunksize, \n",
    "                                           names=header,\n",
    "                                           encoding=encoding, \n",
    "                                           dtype=dtype)):\n",
    "            table = pa.Table.from_pandas(df)\n",
    "            if i == 0:\n",
    "                if dataset:\n",
    "                    # just write header here\n",
    "                    pq.ParquetWriter(os.path.join(base_path,f\"header-only.{file_ext}\"), table.schema)\n",
    "                else:\n",
    "                    # start writing file\n",
    "                    pqwriter = pq.ParquetWriter(dest_path, table.schema)\n",
    "                context.log_artifact(\"header\", local_path=f\"header-only.{file_ext}\")\n",
    "            if dataset:\n",
    "                pq.write_to_dataset(table, root_path=dest_path, partition_cols=partition_cols)\n",
    "            else:\n",
    "                pqwriter.write_table(table)\n",
    "        if pqwriter:\n",
    "            pqwriter.close()\n",
    "\n",
    "        context.logger.info(f\"saved table to {dest_path}\")\n",
    "    else:\n",
    "        context.logger.info(\"destination file already exists\")\n",
    "    context.log_artifact(key, local_path=key+f\".{file_ext}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function \n",
    "# create job function object from notebook code\n",
    "fn = code_to_function('arc_to_parquet', kind='job', with_doc=True,\n",
    "                      handler=arc_to_parquet, image='mlrun/ml-base')\n",
    "\n",
    "# add metadata (for templates and reuse)\n",
    "fn.spec.default_handler = 'arc_to_parquet'\n",
    "fn.spec.description = \"retrieve remote archive, open and save as parquet\"\n",
    "fn.metadata.categories = ['fileutils', 'retrieve']\n",
    "fn.spec.image_pull_policy = \"Always\"\n",
    "fn.metadata.labels = {\"author\": \"yjb\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-03-26 15:30:16,215 saving function: arc-to-parquet, tag: latest\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a9a44ffbdf1b6a089495a7671d92bbe28f3ec81d'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-03-26 15:30:16,250 function spec saved to path: function.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7f79d6cb1a90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.export('function.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test db, yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import import_function, mount_v3io\n",
    "func = import_function(\"hub://arc_to_parquet\").apply(mount_v3io())\n",
    "# func = import_function(\"function.yaml\").apply(mlrun.mount_v3io())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_params = {\n",
    "    \"name\" : \"tasks archive to parquet\",\n",
    "    \"params\" :  {\n",
    "        \"archive_url\" : \"https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\",\n",
    "        \n",
    "        \"header\"      : ['labels', 'lepton_pT', 'lepton_eta', 'lepton_phi', 'missing_energy_magnitude', \n",
    "                         'missing_energy_phi', 'jet_1_pt', 'jet_1_eta', 'jet_1_phi', 'jet_1_b-tag', \n",
    "                         'jet_2_pt', 'jet_2_eta', 'jet_2_phi', 'jet_2_b-tag', 'jet_3_pt', 'jet_3_eta',\n",
    "                         'jet_3_phi', 'jet_3_b-tag', 'jet_4_pt', 'jet_4_eta', 'jet_4_phi', 'jet_4_b-tag',\n",
    "                         'm_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb'],\n",
    "        \n",
    "        \"key\"         : \"higgs\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import NewTask\n",
    "run = func.run(NewTask(**task_params), artifact_path=\"/User/artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
