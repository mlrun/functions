{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import importlib\n",
    "from cloudpickle import load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "import seaborn as sns\n",
    "\n",
    "from typing import Optional, Union, List, Tuple\n",
    "\n",
    "from mlrun.execution import MLClientCtx\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun.artifacts import TableArtifact, PlotArtifact\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "def _gcf_clear(plt):\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()        \n",
    "\n",
    "def test_classifier(\n",
    "    context: MLClientCtx,\n",
    "    models_dir: Union[DataItem, str], \n",
    "    test_set: DataItem,\n",
    "    label_column: str,\n",
    "    score_method: str = 'micro',\n",
    "    key: str = \"\",\n",
    "    plots_dir: str = \"plots\"\n",
    ") -> None:\n",
    "    \"\"\"Test one or more classifier models against held-out dataset\n",
    "    \n",
    "    Using held-out test features, evaluates the peformance of the estimated model\n",
    "    \n",
    "    Can be part of a kubeflow pipeline as a test step that is run post EDA and \n",
    "    training/validation cycles\n",
    "    \n",
    "    :param context:         the function context\n",
    "    :param models_dir:      artifact models representing a folder or a folder\n",
    "    :param test_set:        test features and labels\n",
    "    :param label_column:    column name for ground truth labels\n",
    "    :param score_method:    for multiclass classification\n",
    "    :param key:             key for results artifact (maybe just a dir of artifacts for test like plots_dir)\n",
    "    :param plots_dir:       dir for test plots\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.join(context.artifact_path, plots_dir), exist_ok=True)\n",
    "    \n",
    "    xtest = pd.read_parquet(str(test_set))\n",
    "    ytest = xtest.pop(label_column)\n",
    "    \n",
    "    context.header = list(xtest.columns.values)\n",
    "    \n",
    "    def _eval_model(model):\n",
    "        # enclose all except model\n",
    "        ytestb = label_binarize(ytest, classes=list(range(xtest.shape[1])))\n",
    "        clf = load(open(os.path.join(str(models_dir), model), \"rb\"))\n",
    "        if callable(getattr(clf, \"predict_proba\")):\n",
    "            y_score = clf.predict_proba(xtest.values)\n",
    "            ypred = clf.predict(xtest.values)\n",
    "            context.logger.info(f\"y_score.shape {y_score.shape}\")\n",
    "            context.logger.info(f\"ytestb.shape {ytestb.shape}\")\n",
    "            plot_roc(context, ytestb, y_score, key=f\"roc\", plots_dir=plots_dir)\n",
    "        else:\n",
    "            ypred = clf.predict(xtest.values) # refactor\n",
    "            y_score = None\n",
    "        plot_confusion_matrix(context, ytest, ypred, key=\"confusion\", fmt=\"png\")\n",
    "        if hasattr(clf, \"feature_importances_\"):\n",
    "            plot_importance(context, clf, key=f\"featimp\")\n",
    "        average_precision = metrics.average_precision_score(ytestb[:,:-1], y_score, average=score_method)\n",
    "        context.log_result(f\"accuracy\", float(clf.score(xtest.values, ytest.values)))\n",
    "        context.log_result(f\"rocauc\", metrics.roc_auc_score(ytestb, y_score))\n",
    "        context.log_result(f\"f1_score\", metrics.f1_score(ytest.values, ypred, average=score_method))\n",
    "        context.log_result(f\"avg_precscore\", average_precision)\n",
    "\n",
    "    \n",
    "    best_model = None\n",
    "    for model in os.listdir(str(models_dir)):\n",
    "        if model.endswith('.pkl'):\n",
    "            _eval_model(model)\n",
    "            # HACK: there is only one model here\n",
    "            best_model = model\n",
    "\n",
    "    # log 'best model' as artifact\n",
    "    context.log_artifact('TODAYS-MODELS-TEST-REPORT', local_path=best_model)\n",
    "    context.log_artifact('DEPLOY', body=b'true', local_path='DEPLOY')\n",
    "    \n",
    "def plot_roc(\n",
    "    context,\n",
    "    y_labels,\n",
    "    y_probs,\n",
    "    key=\"roc\",\n",
    "    plots_dir: str = \"plots\",\n",
    "    fmt=\"png\",\n",
    "    fpr_label: str = \"false positive rate\",\n",
    "    tpr_label: str =  \"true positive rate\",\n",
    "    title: str = \"roc curve\",\n",
    "    legend_loc: str = \"best\"\n",
    "):\n",
    "    \"\"\"plot roc curves\n",
    "    \n",
    "    TODO:  add averaging method (as string) that was used to create probs, \n",
    "    display in legend\n",
    "    \n",
    "    :param context:      the function context\n",
    "    :param y_labels:     ground truth labels, hot encoded for multiclass  \n",
    "    :param y_probs:      model prediction probabilities\n",
    "    :param key:          (\"roc\") key of plot in artifact store\n",
    "    :param plots_dir:    (\"plots\") destination folder relative path to artifact path\n",
    "    :param fmt:          (\"png\") plot format\n",
    "    :param fpr_label:    (\"false positive rate\") x-axis labels\n",
    "    :param tpr_label:    (\"true positive rate\") y-axis labels\n",
    "    :param title:        (\"roc curve\") title of plot\n",
    "    :param legend_loc:   (\"best\") location of plot legend\n",
    "    \"\"\"\n",
    "       # clear matplotlib current figure\n",
    "    _gcf_clear(plt)\n",
    "    \n",
    "    # draw 45 degree line\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    \n",
    "    # labelling\n",
    "    plt.xlabel(fpr_label)\n",
    "    plt.ylabel(tpr_label)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=legend_loc)\n",
    "    \n",
    "    # single ROC or mutliple\n",
    "    if y_labels.shape[1] > 1:\n",
    "        # data accummulators by class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(y_labels[:,:-1].shape[1]):\n",
    "            fpr[i], tpr[i], _ = metrics.roc_curve(y_labels[:, i], y_probs[:, i], pos_label=1)\n",
    "            roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "            plt.plot(fpr[i], tpr[i], label=f\"class {i}\")\n",
    "    else:\n",
    "        fpr, tpr, _ = metrics.roc_curve(y_labels, y_probs[:, 1], pos_label=1)\n",
    "        plt.plot(fpr, tpr, label=f\"positive class\")\n",
    "        \n",
    "    fname = f\"{plots_dir}/{key}.{fmt}\"\n",
    "    plt.savefig(os.path.join(context.artifact_path, fname))\n",
    "    context.log_artifact(PlotArtifact(key, body=plt.gcf()), local_path=fname)\n",
    "    \n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    context: MLClientCtx,\n",
    "    labels,\n",
    "    predictions,\n",
    "    key: str = \"confusion_matrix\",\n",
    "    plots_dir: str = \"plots\",\n",
    "    colormap: str = \"Blues\",\n",
    "    fmt: str = \"png\",\n",
    "    sample_weight=None\n",
    "):\n",
    "    \"\"\"Create a confusion matrix.\n",
    "    Plot and save a confusion matrix using test data from a\n",
    "    modelline step.\n",
    "    \n",
    "    See https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "    \n",
    "    TODO: fix label alignment\n",
    "    TODO: consider using another packaged version\n",
    "    TODO: refactor to take params dict for plot options\n",
    "\n",
    "    :param context:         function context\n",
    "    :param labels:          validation data ground-truth labels\n",
    "    :param predictions:     validation data predictions\n",
    "    :param key:             str\n",
    "    :param plots_dir:       relative path of plots in artifact store\n",
    "    :param colormap:        colourmap for confusion matrix\n",
    "    :param fmt:             plot format\n",
    "    :param sample_weight:   sample weights\n",
    "    \"\"\"\n",
    "    _gcf_clear(plt)\n",
    "    \n",
    "    cm = metrics.confusion_matrix(labels, predictions, sample_weight=None)\n",
    "    sns.heatmap(cm, annot=True, cmap=colormap, square=True)\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fname = f\"{plots_dir}/{key}.{fmt}\"\n",
    "    fig.savefig(os.path.join(context.artifact_path, fname))\n",
    "    context.log_artifact(PlotArtifact(key, body=fig), local_path=fname)\n",
    "\n",
    "def plot_importance(\n",
    "    context,\n",
    "    model,\n",
    "    key: str = \"feature-importances\",\n",
    "    fmt = \"png\"\n",
    "):\n",
    "    \"\"\"Display estimated feature importances.\n",
    "    :param context:     function context\n",
    "    :param model:       fitted lightgbm model\n",
    "    \"\"\"\n",
    "    _gcf_clear(plt)\n",
    "    \n",
    "    # create a feature importance table with desired labels\n",
    "    zipped = zip(model.feature_importances_, context.header)\n",
    "\n",
    "    feature_imp = pd.DataFrame(sorted(zipped), columns=[\"freq\",\"feature\"]\n",
    "                                ).sort_values(by=\"freq\", ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.barplot(x=\"freq\", y=\"feature\", data=feature_imp)\n",
    "    plt.title(\"features\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    os.makedirs(os.path.join(context.artifact_path, \"plots\"), exist_ok=True)\n",
    "    fig.savefig(os.path.join(context.artifact_path, f\"plots/{key}.{fmt}\"))\n",
    "    context.log_artifact(PlotArtifact(f\"{key}.{fmt}\", body=fig), local_path=f\"plots/{key}.{fmt}\")\n",
    "\n",
    "    # feature importances are also saved as a table:\n",
    "    feature_imp.to_csv(os.path.join(context.artifact_path, key+\".csv\"))\n",
    "    context.log_artifact(key+\".csv\", local_path=key+\".csv\")\n",
    "\n",
    "    _gcf_clear(plt)\n",
    "\n",
    "def _plot_confusion_matrix(y_true, y_pred, classes=[\"neg\", \"pos\"], \n",
    "                          normalize=True,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"This can be deprecated once intel python upgrades scikit-learn to >0.22\n",
    "    \n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \n",
    "    https://scikit-learn.org/0.21/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = \"Normalized confusion matrix\"\n",
    "        else:\n",
    "            title = \"Confusion matrix, without normalization\"\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel=\"True label\",\n",
    "           xlabel=\"Predicted label\")\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
