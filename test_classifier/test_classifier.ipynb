{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio config kind = \"job\"\n",
    "%nuclio config spec.image = \"mlrun/ml-models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun.artifacts import get_model\n",
    "\n",
    "def test_classifier(\n",
    "    context,\n",
    "    models_path: DataItem, \n",
    "    test_set: DataItem,\n",
    "    label_column: str,\n",
    "    plots_dest: str = \"\",\n",
    ") -> None:\n",
    "    \"\"\"Test one or more classifier models against held-out dataset\n",
    "    \n",
    "    Using held-out test features, evaluates the peformance of the estimated model\n",
    "    \n",
    "    Can be part of a kubeflow pipeline as a test step that is run post EDA and \n",
    "    training/validation cycles\n",
    "    \n",
    "    :param context:         the function context\n",
    "    :param models_path:     artifact models representing a file or a folder\n",
    "    :param test_set:        test features and labels\n",
    "    :param label_column:    column name for ground truth labels\n",
    "    :param score_method:    for multiclass classification\n",
    "    :param plots_dest:      dir for test plots\n",
    "    :param model_evaluator: NOT IMPLEMENTED: specific method to generate eval, passed in as string\n",
    "                            or available in this folder\n",
    "    \"\"\"\n",
    "    xtest = test_set.as_df()\n",
    "    ytest = xtest.pop(label_column)\n",
    "\n",
    "    model_file, model_obj, _ = get_model(models_path.url, suffix='.pkl')\n",
    "    print(model_obj)\n",
    "\n",
    "    # there could be different eval_models, type of model (xgboost, tfv1, tfv2...)\n",
    "    # or how probabilities are calculated, etc...\n",
    "#     if not model_evaluator:\n",
    "#         # binary and multiclass\n",
    "#         y_hat = eval_class_model(xtest, ytest, model_file)\n",
    "    \n",
    "#     elif model_evaluator is \"mutliclass\":\n",
    "#         pass\n",
    "#     elif model_evaluator is \"regression\":\n",
    "#         pass\n",
    "\n",
    "#     # give the prediction columns titles/headers\n",
    "#     if y_hat.ndim == 1 or y_hat.shape[1] == 1:\n",
    "#         score_names = [\"yscore\"]\n",
    "#     else:\n",
    "#         score_names = [\"yscore_\" + str(x) for x in range(y_hat.shape[1])]\n",
    "\n",
    "#     # log the test set and its predictions (should also bind model and metadata)\n",
    "#     df = pd.concat([xtest, ytest, pd.DataFrame(y_hat, columns=score_names)], axis=1)\n",
    "#     context.log_dataset(\"test_set_preds\", df=df, format=\"parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import mlconf\n",
    "import os\n",
    "mlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'\n",
    "mlconf.artifact_path = mlconf.artifact_path or f'{os.environ[\"HOME\"]}/artifacts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-05-03 19:26:43,028 function spec saved to path: function.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7fa0fac3d5c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun import code_to_function \n",
    "# create job function object from notebook code\n",
    "fn = code_to_function(\"test_classifier\")\n",
    "\n",
    "# add metadata (for templates and reuse)\n",
    "fn.spec.default_handler = \"test_classifier\"\n",
    "fn.spec.description = \"test a classifier using held-out or new data\"\n",
    "fn.metadata.categories = [\"ml\", \"test\"]\n",
    "fn.metadata.labels = {\"author\": \"yjb\", \"framework\": \"sklearn\"}\n",
    "\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-05-03 19:26:43,052 warning!, server (0.4.6) and client (0.4.7) ver dont match\n"
     ]
    }
   ],
   "source": [
    "if \"V3IO_HOME\" in list(os.environ):\n",
    "    from mlrun import mount_v3io\n",
    "    fn.apply(mount_v3io())\n",
    "else:\n",
    "    # is you set up mlrun using the instructions at https://github.com/mlrun/mlrun/blob/master/hack/local/README.md\n",
    "    from mlrun.platforms import mount_pvc\n",
    "    fn.apply(mount_pvc('nfsvol', 'nfsvol', '/home/joyan/data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_params = {\n",
    "    \"name\" : \"tasks - test classifier\",\n",
    "    \"params\": {\n",
    "        # Ina pipeline setting, the models_path parameter would be the output of a training step\n",
    "        \"models_path\"   : mlconf.artifact_path + \"/models/sklearn_classifier\",\n",
    "        \"label_column\"  : \"labels\",\n",
    "        \"plots_dest\"    : mlconf.artifact_path + \"/models/sklearn_classifier/plots\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-05-03 19:26:43,086 warning!, server (0.4.6) and client (0.4.7) ver dont match\n",
      "[mlrun] 2020-05-03 19:26:43,087 starting run tasks - test classifier uid=b05d3905d305462e91e618089842eb4f  -> http://mlrun-api:8080\n",
      "[mlrun] 2020-05-03 19:26:43,104 warning!, server (0.4.6) and client (0.4.7) ver dont match\n",
      "[mlrun] 2020-05-03 19:26:43,175 Traceback (most recent call last):\n",
      "  File \"/User/repos/mlrun/mlrun/runtimes/local.py\", line 184, in exec_from_params\n",
      "    val = handler(*args_list)\n",
      "  File \"<ipython-input-4-48e61531480f>\", line 29, in test_classifier\n",
      "    xtest = test_set.as_df()\n",
      "  File \"/User/repos/mlrun/mlrun/datastore/base.py\", line 198, in as_df\n",
      "    df_module=df_module, format=format, **kwargs)\n",
      "  File \"/User/repos/mlrun/mlrun/datastore/base.py\", line 117, in as_df\n",
      "    return reader(self._join(key), **kwargs)\n",
      "  File \"/User/.pythonlibs/jupyter/lib/python3.6/site-packages/pandas/io/parquet.py\", line 310, in read_parquet\n",
      "    return impl.read(path, columns=columns, **kwargs)\n",
      "  File \"/User/.pythonlibs/jupyter/lib/python3.6/site-packages/pandas/io/parquet.py\", line 125, in read\n",
      "    path, columns=columns, **kwargs\n",
      "  File \"/conda/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1274, in read_table\n",
      "    filesystem=filesystem, filters=filters)\n",
      "  File \"/conda/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1030, in __init__\n",
      "    open_file_func=partial(_open_dataset_file, self._metadata)\n",
      "  File \"/conda/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1229, in _make_manifest\n",
      "    .format(path))\n",
      "OSError: Passed non-file path: /User/artifacts/test_set.parquet\n",
      "\n",
      "\n",
      "[mlrun] 2020-05-03 19:26:43,192 exec error - Passed non-file path: /User/artifacts/test_set.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passed non-file path: /User/artifacts/test_set.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style> \n",
       ".dictlist {\n",
       "  background-color: #b3edff; \n",
       "  text-align: center; \n",
       "  margin: 4px; \n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer; \n",
       "  background-color: #ffe6cc; \n",
       "  text-align: left; \n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #ffe6cc;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "  \n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "  \n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }  \n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "  \n",
       "  \n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>default</td>\n",
       "      <td><div title=\"b05d3905d305462e91e618089842eb4f\"><a href=\"https://mlrun-ui.default-tenant.app.yjb-mlrun-dav.iguazio-cd1.com/projects/default/jobs/b05d3905d305462e91e618089842eb4f/info\" target=\"_blank\" >...9842eb4f</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>May 03 19:26:43</td>\n",
       "      <td><div style=\"color: red;\" title=\"Passed non-file path: /User/artifacts/test_set.parquet\">error</div></td>\n",
       "      <td>tasks - test classifier</td>\n",
       "      <td><div class=\"dictlist\">host=jupyter-6c5fccf844-gxlrw</div><div class=\"dictlist\">kind=handler</div><div class=\"dictlist\">owner=admin</div><div class=\"dictlist\">v3io_user=admin</div></td>\n",
       "      <td><div title=\"/User/artifacts/test_set.parquet\">test_set</div></td>\n",
       "      <td><div class=\"dictlist\">label_column=labels</div><div class=\"dictlist\">models_path=/User/artifacts/models/sklearn_classifier</div><div class=\"dictlist\">plots_dest=/User/artifacts/models/sklearn_classifier/plots</div></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result9f6f4c76-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result9f6f4c76-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result9f6f4c76\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result9f6f4c76-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to track results use .show() or .logs() or in CLI: \n",
      "!mlrun get run b05d3905d305462e91e618089842eb4f --project default , !mlrun logs b05d3905d305462e91e618089842eb4f --project default\n",
      "[mlrun] 2020-05-03 19:26:43,260 run executed, status=error\n"
     ]
    },
    {
     "ename": "RunError",
     "evalue": "Passed non-file path: /User/artifacts/test_set.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRunError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f84984260b55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"test_set\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"test_set.parquet\"\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mworkdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/User/artifacts/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 artifact_path=mlconf.artifact_path + \"/test-classifier\")\n\u001b[0m",
      "\u001b[0;32m~/repos/mlrun/mlrun/run.py\u001b[0m in \u001b[0;36mrun_local\u001b[0;34m(task, command, name, args, workdir, project, tag, secrets, handler, params, inputs, artifact_path)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     return fn.run(task, name=name, handler=handler, params=params, inputs=inputs,\n\u001b[0;32m--> 144\u001b[0;31m                   artifact_path=artifact_path)\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/mlrun/mlrun/runtimes/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, runspec, handler, name, project, params, inputs, out_path, workdir, artifact_path, watch, schedule)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrunspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlast_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunspec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRunObject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/mlrun/mlrun/runtimes/base.py\u001b[0m in \u001b[0;36m_wrap_result\u001b[0;34m(self, result, runspec, err)\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_remote\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'runtime error: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRunError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRunError\u001b[0m: Passed non-file path: /User/artifacts/test_set.parquet"
     ]
    }
   ],
   "source": [
    "from mlrun import run_local, NewTask\n",
    "\n",
    "run = run_local(NewTask(**task_params),\n",
    "                handler=test_classifier,\n",
    "                inputs={\"test_set\":\"test_set.parquet\" })\n",
    "                #,\n",
    "                #workdir=\"/User/artifacts/\",\n",
    "                #artifact_path=mlconf.artifact_path + \"/test-classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import NewTask\n",
    "run = func.run(NewTask(**task_params), \n",
    "               inputs={\"test_set\":\"test_set.parquet\" },\n",
    "               workdir=\"/User/artifacts\",\n",
    "               artifact_path=\"/User/artifacts/test-classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
