{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dask cluster with data\n",
    "load a parquet dataset into a dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio config kind = \"dask\"\n",
    "%nuclio config spec.image = \"mlrun/ml-models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "from mlrun.execution import MLClientCtx\n",
    "from mlrun.datastore import DataItem\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "def load_dask(\n",
    "    context: MLClientCtx,\n",
    "    src_data: DataItem,\n",
    "    dask_key: str = \"dask_key\",\n",
    "    inc_cols: List[str] = [],\n",
    "    index_cols: List[str] = [],\n",
    "    dask_persist: bool = True,\n",
    "    refresh_data: bool = True,\n",
    "    scheduler_key: str = \"scheduler\"\n",
    ") -> None:\n",
    "    \"\"\"Load dataset into an existing dask cluster\n",
    "    \n",
    "    dask jobs define the dask client parameters at the job level, this method will raise an error if no client is detected.\n",
    "    \n",
    "    :param context:         the function context\n",
    "    :param src_data:        url of the data file or partitioned dataset as either\n",
    "                            artifact DataItem, string, or path object (similar to \n",
    "                            pandas read_csv)\n",
    "    :param dask_key:        destination key of data on dask cluster and artifact store\n",
    "    :param inc_cols:        include only these columns (very fast)\n",
    "    :param index_cols:      list of index column names (can be a long-running process)\n",
    "    :param dask_persist:    (True) should the data be persisted (through the `client.persist` op)\n",
    "    :param refresh_data:    (False) if the dask_key already exists in the dask cluster, this will \n",
    "                            raise an Exception.  Set to True to replace the existing cluster data.\n",
    "    :param scheduler_key:   (scheduler) the dask scheduler configuration, json also logged as an artifact\n",
    "    \"\"\"\n",
    "    if hasattr(context, \"dask_client\"):\n",
    "        dask_client = context.dask_client\n",
    "    else:\n",
    "        raise Exception(\"a dask client was not found in the execution context\")\n",
    "    \n",
    "    df = src_data.as_df(df_module=dd)\n",
    "\n",
    "    if dask_persist:\n",
    "        df = dask_client.persist(df)\n",
    "        if dask_client.datasets and dask_key in dask_client.datasets:\n",
    "            dask_client.unpublish_dataset(dask_key)\n",
    "        dask_client.publish_dataset(df, name=dask_key)\n",
    "    \n",
    "    if context:\n",
    "        context.dask_client = dask_client\n",
    "        \n",
    "    # share the scheduler, whether data is persisted or not\n",
    "    dask_client.write_scheduler_file(scheduler_key+\".json\")\n",
    "    \n",
    "    # we don't use log_dataset here until it can take into account\n",
    "    # dask origin and apply dask describe.\n",
    "    context.log_artifact(scheduler_key, local_path=scheduler_key+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import mlconf\n",
    "import os\n",
    "mlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'\n",
    "mlconf.artifact_path = mlconf.artifact_path or f'{os.environ[\"V3IO_HOME\"]}/artifacts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function \n",
    "# create job function object from notebook code\n",
    "fn = code_to_function(\"load_dask\", handler=\"load_dask\")\n",
    "\n",
    "# add metadata (for templates and reuse)\n",
    "fn.spec.description = \"load dask cluster with data\"\n",
    "fn.metadata.categories = [\"data-movement\", \"utils\"]\n",
    "fn.metadata.labels = {\"author\": \"yjb\"}\n",
    "fn.spec.remote = True\n",
    "fn.spec.replicas = 4\n",
    "fn.spec.max_replicas = 8\n",
    "fn.spec.service_type = \"NodePort\"\n",
    "\n",
    "fn.save()\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"V3IO_HOME\" in list(os.environ):\n",
    "    from mlrun import mount_v3io\n",
    "    fn.apply(mount_v3io())\n",
    "else:\n",
    "    # is you set up mlrun using the instructions at https://github.com/mlrun/mlrun/blob/master/hack/local/README.md\n",
    "    from mlrun.platforms import mount_pvc\n",
    "    fn.apply(mount_pvc('nfsvol', 'nfsvol', '/home/joyan/data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import NewTask, run_local\n",
    "\n",
    "task_params = {\n",
    "    \"name\":        \"tasks load dask cluster with data\",\n",
    "    \"params\" : {\n",
    "        \"persist\"      : True,\n",
    "        \"refresh_data\" : True,\n",
    "        \"dask_key\"     : \"dask_key\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = fn.run(NewTask(**task_params), \n",
    "               handler=load_dask, \n",
    "               inputs={\"src_data\" : os.path.join(mlconf.artifact_path, 'iris.parquet') },\n",
    "               artifact_path=mlconf.artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.status.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: this client dash board wont work -- wrong port!\n",
    "\n",
    "...even though its the correct client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(func.status.to_dict()['scheduler_address'])\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(client.list_datasets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.datasets['dask_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
