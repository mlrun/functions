{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7955da79-02cc-42fe-aee0-5456d3e386fd",
   "metadata": {},
   "source": [
    "# LangChain ✕ MLRun Integration\n",
    "\n",
    "`langchain_mlrun` is a hub module that implements LangChain integration with MLRun. Using the module allows MLRun to orchestrate LangChain and LangGraph code, enabling tracing and monitoring batch workflows and realtime deployments.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8392a3e1-d0a1-409a-ae68-fcc36858d30a",
   "metadata": {},
   "source": [
    "## Main Components\n",
    "\n",
    "This is a short brief of the components available to import from the `langchain_mlrun` module. For full docs, see the documentation page.\n",
    "\n",
    "### Settings\n",
    "\n",
    "The module uses Pydantic settings classes that can be configured programmatically or via environment variables. The main class is `MLRunTracerSettings`. It contains two sub-settings:\n",
    "* `MLRunTracerClientSettings` - Connection settings (stream path, container, endpoint info). Env prefix: `\"MLRUN_TRACER_CLIENT_\"`\n",
    "* `MLRunTracerMonitorSettings` - Controls what/how runs are captured (filters, labels, debug mode). Env prefix: `\"MLRUN_TRACER_MONITOR_\"`\n",
    "\n",
    "For more information about each setting, see the class docstrings.\n",
    "\n",
    "#### Example - via code configuration\n",
    "\n",
    "```python\n",
    "from langchain_mlrun import MLRunTracerSettings, MLRunTracerClientSettings, MLRunTracerMonitorSettings\n",
    "\n",
    "settings = MLRunTracerSettings(\n",
    "    client=MLRunTracerClientSettings(\n",
    "        stream_path=\"my-project/model-endpoints/stream-v1\",\n",
    "        container=\"projects\",\n",
    "        model_endpoint_name=\"my_endpoint\",\n",
    "        model_endpoint_uid=\"abc123\",\n",
    "        serving_function=\"my_function\",\n",
    "    ),\n",
    "    monitor=MLRunTracerMonitorSettings(\n",
    "        label=\"production\",\n",
    "        root_run_only=True,  # Only monitor root runs, not child runs\n",
    "        tags_filter=[\"important\"],  # Only monitor runs with this tag\n",
    "    ),\n",
    ")\n",
    "```\n",
    "\n",
    "#### Example - environment variable configuration\n",
    "\n",
    "```bash\n",
    "export MLRUN_TRACER_CLIENT_STREAM_PATH=\"my-project/model-endpoints/stream-v1\"\n",
    "export MLRUN_TRACER_CLIENT_CONTAINER=\"projects\"\n",
    "export MLRUN_TRACER_MONITOR_LABEL=\"production\"\n",
    "export MLRUN_TRACER_MONITOR_ROOT_RUN_ONLY=\"true\"\n",
    "```\n",
    "\n",
    "### MLRun Tracer\n",
    "\n",
    "`MLRunTracer` is a LangChain-compatible tracer that converts LangChain `Run` objects into MLRun monitoring events and publishes them to a V3IO stream. \n",
    "\n",
    "Key points:\n",
    "* **No inheritance required** - use it directly without subclassing.\n",
    "* **Fully customizable via settings** - control filtering, summarization, and output format.\n",
    "* **Custom summarizer support** - pass your own `run_summarizer_function` via settings to customize how runs are converted to events.\n",
    "\n",
    "### Monitoring Setup Utility Function\n",
    "\n",
    "`setup_langchain_monitoring()` is a utility function that creates the necessary MLRun infrastructure for LangChain monitoring. This is a **temporary workaround** until custom endpoint creation support is added to MLRun.\n",
    "\n",
    "The function returns a dictionary of environment variables to configure auto-tracing. See how to use it in the tutorial section below.\n",
    "\n",
    "### LangChain Monitoring Application\n",
    "\n",
    "`LangChainMonitoringApp` is a base class (inheriting from MLRun's `ModelMonitoringApplicationBase`) for building monitoring applications that process events from the MLRun Tracer.\n",
    "\n",
    "It offers several built-in helper methods and metrics for analyzing LangChain runs:\n",
    "\n",
    "* Helper methods:\n",
    "  * `get_structured_runs()` - Parse raw monitoring samples into structured run dictionaries with filtering options\n",
    "  * `iterate_structured_runs()` - Iterate over all runs including nested child runs\n",
    "* Metric methods:\n",
    "  * `calculate_average_latency()` - Average latency across root runs\n",
    "  * `calculate_success_rate()` - Percentage of runs without errors\n",
    "  * `count_token_usage()` - Total input/output tokens from LLM runs\n",
    "  * `count_run_names()` - Count occurrences of each run name\n",
    "\n",
    "The base app can be used as-is, but it is recommended to extend it with your own custom monitoring logic.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e24e1a5-d80a-4b7e-9b94-57b24e8b39d7",
   "metadata": {},
   "source": [
    "## How to Apply MLRun?\n",
    "\n",
    "### Auto Tracing\n",
    "\n",
    "Auto tracing automatically instruments all LangChain code by setting the `MLRUN_MONITORING_ENABLED` environment variable and importing the module:\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"MLRUN_MONITORING_ENABLED\"] = \"1\"\n",
    "# Set other MLRUN_TRACER_* environment variables as needed...\n",
    "\n",
    "# Import the module BEFORE any LangChain code\n",
    "langchain_mlrun = mlrun.import_module(\"hub://langchain_mlrun\")\n",
    "\n",
    "# All LangChain/LangGraph code below will be automatically traced\n",
    "chain.invoke(...)\n",
    "```\n",
    "\n",
    "### Manual Tracing\n",
    "\n",
    "For more control, use the `mlrun_monitoring()` context manager to trace specific code blocks:\n",
    "\n",
    "```python\n",
    "from langchain_mlrun import mlrun_monitoring, MLRunTracerSettings\n",
    "\n",
    "# Optional: customize settings\n",
    "settings = MLRunTracerSettings(...)\n",
    "\n",
    "with mlrun_monitoring(settings=settings) as tracer:\n",
    "    # Only LangChain code within this block will be traced\n",
    "    result = chain.invoke({\"topic\": \"MLRun\"})\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b52d3d-a431-44fb-acd6-ea33fec37a49",
   "metadata": {},
   "source": [
    "## Tutorial\n",
    "\n",
    "In this tutorial we'll show how to orchestrate LangChain based code with MLRun using the `langchain_mlrun` hub module.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Install MLRun and the `langchain_mlrun` requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf72aa6-06e8-4a04-bfc4-409b39d255fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlrun langchain pydantic-settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa18266-d3b5-40bd-a8b9-65345e419d8c",
   "metadata": {},
   "source": [
    "### Create Project\n",
    "\n",
    "We'll first create an MLRun project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2664df3e-d9c6-40dd-a215-29d60e4b4208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2026-01-08 14:48:52,259 [info] Project loaded successfully: {\"project_name\":\"langchain-mlrun-7\"}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import mlrun\n",
    "\n",
    "project = mlrun.get_or_create_project(\"langchain-mlrun-tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f28986-c158-47fd-97a6-74f69892b4eb",
   "metadata": {},
   "source": "### Enable Monitoring\n\nTo use MLRun's monitoring feature in our project we first need to set up the monitoring infrastructure. If you use MLRun CE, you'll need to create a Kafka stream, if you use MLRun enterprise, you can use V3IO."
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d2fa66-0498-445d-ab4a-8370f46aec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add here MLRun CE handler with Kafka, currently the tutorial is only with V3IO.\n",
    "from mlrun.datastore import DatastoreProfileV3io\n",
    "\n",
    "# Create a V3IO data store:\n",
    "v3io_ds = DatastoreProfileV3io(name=\"v3io-ds\",v3io_access_key=os.environ[\"V3IO_ACCESS_KEY\"])\n",
    "project.register_datastore_profile(profile=v3io_ds)\n",
    "\n",
    "# Set the monitoring credentials:\n",
    "project.set_model_monitoring_credentials(\n",
    "    stream_profile_name=v3io_ds.name,\n",
    "    tsdb_profile_name=v3io_ds.name\n",
    ")\n",
    "\n",
    "# Enable monitoring for our project:\n",
    "project.enable_model_monitoring(\n",
    "    base_period=1,\n",
    "    wait_for_deployment=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23117fa-7b67-470c-80ca-976d14c2120e",
   "metadata": {},
   "source": [
    "### Import `langchain_mlrun`\n",
    "\n",
    "Now we'll import `langchain_mlrun` from the hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2360cd49-b260-4140-bd16-138349e000b3",
   "metadata": {},
   "outputs": [],
   "source": "# Import the module from the hub:\nlangchain_mlrun = mlrun.import_module(\"hub://langchain_mlrun\")\n\n# Import the utility function and monitoring application from the module:\nsetup_langchain_monitoring = langchain_mlrun.setup_langchain_monitoring\nLangChainMonitoringApp = langchain_mlrun.LangChainMonitoringApp"
  },
  {
   "cell_type": "markdown",
   "id": "de030131-ebaf-48f8-96ed-3c1013b5e260",
   "metadata": {},
   "source": "### Create Monitorable Endpoint\n\nEndpoints are the entities being monitored by MLRun. As such we'll use the `setup_langchain_monitoring` utility function to create the model monitoring endpoint. By default, our endpoint name will be `\"langchain_mlrun_endpoint\"` but feel free to change it by using the required arguments."
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e9baf78-3d38-46bd-89dd-6f83760eaeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LangChain model endpoint\n",
      "\n",
      "  [✓] Loading Project......................... Done (0.00s)\u001B[K\n",
      "  [✓] Creating Model.......................... Done (0.02s)               \u001B[K\n",
      "  [✓] Creating Function....................... Done (0.02s)                  \u001B[K\n",
      "  [✓] Creating Model Endpoint................. Done (0.02s)                  \u001B[K\n",
      "\n",
      "✨ Done! LangChain monitoring model endpoint created successfully.\n",
      "You can now set the following environment variables to enable MLRun tracing in your LangChain code:\n",
      "\n",
      "{\n",
      "    \"MLRUN_MONITORING_ENABLED\": \"1\",\n",
      "    \"MLRUN_TRACER_CLIENT_PROJECT\": \"langchain-mlrun-7\",\n",
      "    \"MLRUN_TRACER_CLIENT_STREAM_PATH\": \"langchain-mlrun-7/model-endpoints/stream-v1\",\n",
      "    \"MLRUN_TRACER_CLIENT_CONTAINER\": \"projects\",\n",
      "    \"MLRUN_TRACER_CLIENT_MODEL_ENDPOINT_NAME\": \"langchain_mlrun_endpoint\",\n",
      "    \"MLRUN_TRACER_CLIENT_MODEL_ENDPOINT_UID\": \"bb81af2058c14e7cbf58455aed3d69fc\",\n",
      "    \"MLRUN_TRACER_CLIENT_SERVING_FUNCTION\": \"langchain_mlrun_function\"\n",
      "}\n",
      "\n",
      "To customize the monitoring behavior, you can also set additional environment variables prefixed with 'MLRUN_TRACER_MONITOR_'. Refer to the MLRun tracer documentation for more details.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_vars = setup_langchain_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd45c94b-ee05-449c-9336-0aa659e66bda",
   "metadata": {},
   "source": [
    "### Setup Environment Variables for Auto Tracing\n",
    "\n",
    "We'll use the environment variables returned from `setup_langchain_monitoring` to setup the environment for auto-tracing. Read the printed outputs for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1988f8-c80a-4bf2-bfb1-d43523fc161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.update(env_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3b8e5-3538-4153-95da-e6d8776be3ac",
   "metadata": {},
   "source": "### Run `langchain` or `langgraph` Code\n\nHere we have 3 functions, each using different method utilizing LLMs with `langchain` and `langgraph`:\n* `run_simple_chain` - Using `langchain`'s chains.\n* `run_simple_agent` - Using `langchain`'s `create_agent` function and `tool`s.\n* `run_langgraph_graph` - Using pure `langgraph`.\n\n> **Notice**: You don't need to set OpenAI API credentials, there is a mock `ChatModel` that will replace it if the credentials are not set in the environment. If you wish to use OpenAI models, make sure you `pip install langchain_openai` and set the `OPENAI_API_KEY` environment variable before continue to the next cell.\n\nBecause the auto-tracing environment is set, any run will be automatically traced and monitored!\n\nFeel free to adjust the code as you like.\n\n> **Remember**: To enable auto-tracing you do need to set the environment variables and import the `langchain_mlrun` module before any LangChain code. For batch jobs and realtime functions, make sure you set env vars in the MLRun function and add the import line `langchain_mlrun = mlrun.import_module(\"hub://langchain_mlrun\")` at the top of your code."
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b4d4b0-8d10-4ad3-8f16-7b1b7daeac11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal, TypedDict, Annotated, Sequence, Any, Callable\n",
    "from operator import add\n",
    "\n",
    "from langchain_core.language_models import LanguageModelInput\n",
    "from langchain_core.runnables import Runnable, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.language_models.fake_chat_models import FakeListChatModel, GenericFakeChatModel\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.tools import tool, BaseTool\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "def _check_openai_credentials() -> bool:\n",
    "    \"\"\"\n",
    "    Check if OpenAI API key is set in environment variables.\n",
    "\n",
    "    :return: True if OPENAI_API_KEY is set, False otherwise.\n",
    "    \"\"\"\n",
    "    return \"OPENAI_API_KEY\" in os.environ\n",
    "\n",
    "\n",
    "# Import ChatOpenAI only if OpenAI credentials are available (meaning `langchain-openai` must be installed).\n",
    "if _check_openai_credentials():\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    \n",
    "class _ToolEnabledFakeModel(GenericFakeChatModel):\n",
    "    \"\"\"\n",
    "    A fake chat model that supports tool binding for running agent tracing tests.\n",
    "    \"\"\"\n",
    "\n",
    "    def bind_tools(\n",
    "        self,\n",
    "        tools: Sequence[\n",
    "            dict[str, Any] | type | Callable | BaseTool  # noqa: UP006\n",
    "        ],\n",
    "        *,\n",
    "        tool_choice: str | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable[LanguageModelInput, AIMessage]:\n",
    "        return self\n",
    "\n",
    "\n",
    "#: Tag value for testing tag filtering.\n",
    "_dummy_tag = \"dummy_tag\"\n",
    "\n",
    "\n",
    "def run_simple_chain() -> str:\n",
    "    \"\"\"\n",
    "    Run a simple LangChain chain that gets a fact about a topic.\n",
    "    \"\"\"\n",
    "    # Build a simple chain: prompt -> llm -> str output parser\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        tags=[_dummy_tag]\n",
    "    ) if _check_openai_credentials() else (\n",
    "        FakeListChatModel(\n",
    "            responses=[\n",
    "                \"MLRun is an open-source orchestrator for machine learning pipelines.\"\n",
    "            ],\n",
    "            tags=[_dummy_tag]\n",
    "        )\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_template(\"Tell me a short fact about {topic}\")\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run the chain:\n",
    "    response = chain.invoke({\"topic\": \"MLRun\"})\n",
    "    return response\n",
    "\n",
    "\n",
    "def run_simple_agent():\n",
    "    \"\"\"\n",
    "    Run a simple LangChain agent that uses two tools to get weather and stock price.\n",
    "    \"\"\"\n",
    "    # Define the tools:\n",
    "    @tool\n",
    "    def get_weather(city: str) -> str:\n",
    "        \"\"\"Get the current weather for a specific city.\"\"\"\n",
    "        return f\"The weather in {city} is 22°C and sunny.\"\n",
    "\n",
    "    @tool\n",
    "    def get_stock_price(symbol: str) -> str:\n",
    "        \"\"\"Get the current stock price for a symbol.\"\"\"\n",
    "        return f\"The stock price for {symbol} is $150.25.\"\n",
    "\n",
    "    # Define the model:\n",
    "    model = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        tags=[_dummy_tag]\n",
    "    ) if _check_openai_credentials() else (\n",
    "        _ToolEnabledFakeModel(\n",
    "            messages=iter(\n",
    "                [\n",
    "                    AIMessage(\n",
    "                        content=\"\",\n",
    "                        tool_calls=[\n",
    "                            {\"name\": \"get_weather\", \"args\": {\"city\": \"London\"}, \"id\": \"call_abc123\"},\n",
    "                            {\"name\": \"get_stock_price\", \"args\": {\"symbol\": \"AAPL\"}, \"id\": \"call_def456\"}\n",
    "                        ]\n",
    "                    ),\n",
    "                    AIMessage(content=\"The weather in London is 22°C and AAPL is trading at $150.25.\")\n",
    "                ]\n",
    "            ),\n",
    "            tags=[_dummy_tag]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the agent:\n",
    "    agent = create_agent(\n",
    "        model=model,\n",
    "        tools=[get_weather, get_stock_price],\n",
    "        system_prompt=\"You are a helpful assistant with access to tools.\"\n",
    "    )\n",
    "\n",
    "    # Run the agent:\n",
    "    return agent.invoke({\"messages\": [\"What is the weather in London and the stock price of AAPL?\"]})\n",
    "\n",
    "\n",
    "def run_langgraph_graph():\n",
    "    \"\"\"\n",
    "    Run a LangGraph agent that uses reflection to correct its answer.\n",
    "    \"\"\"\n",
    "    # Define the graph state:\n",
    "    class AgentState(TypedDict):\n",
    "        messages: Annotated[list[BaseMessage], add]\n",
    "        attempts: int\n",
    "\n",
    "    # Define the model:\n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\") if _check_openai_credentials() else (\n",
    "        _ToolEnabledFakeModel(\n",
    "            messages=iter(\n",
    "                [\n",
    "                    AIMessage(content=\"There are 2 'r's in Strawberry.\"),  # Mocking the failure\n",
    "                    AIMessage(content=\"I stand corrected. S-t-r-a-w-b-e-r-r-y. There are 3 'r's.\"),  # Mocking the fix\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Define the graph nodes and router:\n",
    "    def call_model(state: AgentState):\n",
    "        response = model.invoke(state[\"messages\"])\n",
    "        return {\"messages\": [response], \"attempts\": state[\"attempts\"] + 1}\n",
    "\n",
    "    def reflect_node(state: AgentState):\n",
    "        prompt = \"Wait, count the 'r's again slowly, letter by letter. Are you sure?\"\n",
    "        return {\"messages\": [HumanMessage(content=prompt)]}\n",
    "\n",
    "    def router(state: AgentState) -> Literal[\"reflect\", END]:\n",
    "        # Make sure there are 2 attempts at least for an answer:\n",
    "        if state[\"attempts\"] == 1:\n",
    "            return \"reflect\"\n",
    "        return END\n",
    "\n",
    "    # Build the graph:\n",
    "    builder = StateGraph(AgentState)\n",
    "    builder.add_node(\"model\", call_model)\n",
    "    tagged_reflect_node = RunnableLambda(reflect_node).with_config(tags=[_dummy_tag])\n",
    "    builder.add_node(\"reflect\", tagged_reflect_node)\n",
    "    builder.add_edge(START, \"model\")\n",
    "    builder.add_conditional_edges(\"model\", router)\n",
    "    builder.add_edge(\"reflect\", \"model\")\n",
    "    graph = builder.compile()\n",
    "\n",
    "    # Run the graph:\n",
    "    return graph.invoke({\"messages\": [HumanMessage(content=\"How many 'r's in Strawberry?\")], \"attempts\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49964f96-89ba-4f61-8788-38290a877aa2",
   "metadata": {},
   "source": [
    "Let's create some traffic, we'll run whatever function you want in a loop to get some events. We take timestamps in order to use them later to run the monitoring application on the data we'll send."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e6418d-76f4-4b18-9ef9-c5bb40b20545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LangChain code and now it should be tracked and monitored in MLRun:\n",
    "start_timestamp = datetime.datetime.now() - datetime.timedelta(minutes=1)\n",
    "for i in range(20):\n",
    "    run_simple_agent()\n",
    "end_timestamp = datetime.datetime.now() + datetime.timedelta(minutes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9085765-91fd-4d31-84b4-927ecf9cc455",
   "metadata": {},
   "source": "> **Note**: Please wait a minute or two until the events are processed."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fae3e4-5f1b-4f0c-ba71-81060f10804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2475ebec-fc32-4884-9723-3ca9cfde577f",
   "metadata": {},
   "source": [
    "### Test the LangChain Monitoring Application\n",
    "\n",
    "To test a monitoring application, we use the `evaluate` class method. We'll run an evaluation on the data we just sent. It is a small local job and should run fast.\n",
    "\n",
    "Keep an eye for the returned metrics from the monitoring application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d046755-9153-497a-a024-5d63316e1f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2026-01-08 14:49:22,970 [info] Changing function name - adding `\"-batch\"` suffix: {\"func_name\":\"testi-batch\"}\n",
      "> 2026-01-08 14:49:23,143 [info] Storing function: {\"db\":\"http://mlrun-api:8080\",\"name\":\"testi-batch--handler\",\"uid\":\"43b34f848b6049c0949f04adc1090f10\"}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "\n",
       "  // Get the base URL of the current notebook\n",
       "  var baseUrl = window.location.origin;\n",
       "\n",
       "  // Construct the full URL\n",
       "  var fullUrl = new URL(el.title, baseUrl).href;\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = fullUrl\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (fullUrl.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", fullUrl);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = fullUrl;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>state</th>\n",
       "      <th>kind</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>langchain-mlrun-7</td>\n",
       "      <td><div title=\"43b34f848b6049c0949f04adc1090f10\"><a href=\"https://mlrun-ui.default-tenant.app.innovation-dev.iguazio-cd2.com/mlprojects/langchain-mlrun-7/jobs/monitor-jobs/testi-batch--handler/43b34f848b6049c0949f04adc1090f10/overview\" target=\"_blank\" >...c1090f10</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Jan 08 14:49:23</td>\n",
       "      <td>NaT</td>\n",
       "      <td>completed</td>\n",
       "      <td>run</td>\n",
       "      <td>testi-batch--handler</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=guyl</div><div class=\"dictlist\">kind=local</div><div class=\"dictlist\">owner=guyl</div><div class=\"dictlist\">host=jupyter-guyl-66647f988c-4kjd9</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">endpoints=['langchain_mlrun_endpoint']</div><div class=\"dictlist\">start=2026-01-08T10:19:47.452879</div><div class=\"dictlist\">end=2026-01-08T10:26:28.861851</div><div class=\"dictlist\">base_period=None</div><div class=\"dictlist\">write_output=False</div><div class=\"dictlist\">existing_data_handling=fail_on_overlap</div><div class=\"dictlist\">stream_profile=None</div></td>\n",
       "      <td><div class=\"dictlist\">langchain_mlrun_endpoint-bb81af2058c14e7cbf58455aed3d69fc_2026-01-08T10:19:47.452879+00:00_2026-01-08T10:26:28.861851+00:00=[{metric_name: 'average_latency', metric_value: 1949.3444}, {metric_name: 'success_rate', metric_value: 1.0}, {metric_name: 'total_input_tokens', metric_value: 5480.0}, {metric_name: 'total_output_tokens', metric_value: 1404.0}, {metric_name: 'combined_total_tokens', metric_value: 6884.0}, {metric_name: 'run_name_counts_ChatOpenAI', metric_value: 40.0}, {metric_name: 'run_name_counts_model', metric_value: 40.0}, {metric_name: 'run_name_counts_get_weather', metric_value: 20.0}, {metric_name: 'run_name_counts_tools', metric_value: 40.0}, {metric_name: 'run_name_counts_get_stock_price', metric_value: 20.0}, {metric_name: 'run_name_counts_LangGraph', metric_value: 20.0}]</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result28f83d43-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result28f83d43-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result28f83d43\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result28f83d43-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods  or <a href=\"https://mlrun-ui.default-tenant.app.innovation-dev.iguazio-cd2.com/mlprojects/langchain-mlrun-7/jobs/monitor-jobs/testi-batch--handler/43b34f848b6049c0949f04adc1090f10/overview\" target=\"_blank\">click here</a> to open in UI</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2026-01-08 14:49:23,944 [info] Run execution finished: {\"name\":\"testi-batch--handler\",\"status\":\"completed\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.model.RunObject at 0x7f029fea1d50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LangChainMonitoringApp.evaluate(\n",
    "    func_name=\"langchain_monitoring_app_test\",\n",
    "    func_path=\"langchain_mlrun.py\",\n",
    "    run_local=True,\n",
    "    endpoints=[env_vars[\"MLRUN_TRACER_CLIENT_MODEL_ENDPOINT_NAME\"]],\n",
    "    start=start_timestamp.isoformat(),\n",
    "    end=end_timestamp.isoformat(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda724c3-27f3-4d28-a7ba-1e59b9be2a37",
   "metadata": {},
   "source": "### Deploy the Monitoring Application\n\nAll that's left to do now is to deploy our monitoring application!"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "652b00d4-070d-4849-9784-4d461cb83eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2026-01-08 17:06:50,801 [info] Starting remote function deploy\n",
      "2026-01-08 17:06:51  (info) Deploying function\n",
      "2026-01-08 17:06:51  (info) Building\n",
      "2026-01-08 17:06:52  (info) Staging files and preparing base images\n",
      "2026-01-08 17:06:52  (warn) Using user provided base image, runtime interpreter version is provided by the base image\n",
      "2026-01-08 17:06:52  (info) Building processor image\n",
      "2026-01-08 17:08:52  (info) Build complete\n",
      "2026-01-08 17:09:06  (info) Function deploy complete\n",
      "> 2026-01-08 17:09:13,972 [info] Model endpoint creation task completed with state succeeded\n",
      "> 2026-01-08 17:09:13,973 [info] Successfully deployed function: {\"external_invocation_urls\":[],\"internal_invocation_urls\":[\"nuclio-langchain-mlrun-7-langchain-monitoring-app.default-tenant.svc.cluster.local:8080\"]}\n"
     ]
    }
   ],
   "source": [
    "# Deploy the monitoring app:\n",
    "LangChainMonitoringApp.deploy(\n",
    "    func_name=\"langchain_monitoring_app\",\n",
    "    func_path=\"langchain_mlrun.py\",\n",
    "    image=\"mlrun/mlrun\",\n",
    "    requirements=[\n",
    "        \"langchain\",\n",
    "        \"pydantic-settings\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23bef7a-cbdb-4b22-a2d9-2edbfde5eb04",
   "metadata": {},
   "source": [
    "Once it is deployed, you can run events again and see the monitoring application in MLRun UI in action:\n",
    "\n",
    "![mlrun ui example](./notebook_images/mlrun_ui.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f799f06-2e62-4e2f-a42f-c94b5fc18623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-py311",
   "language": "python",
   "name": "conda-env-.conda-mlrun-py311-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
