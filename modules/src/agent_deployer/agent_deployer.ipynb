{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aecdfc37-4b22-4bb1-b052-930bcf7ec109",
   "metadata": {},
   "source": [
    "#### Configure mlrun project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be42e7c5-b2af-476f-8041-c17be56edb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-12-03 07:17:36,530 [info] Project loaded successfully: {\"project_name\":\"langchain-example-10\"}\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import mlrun\n",
    "from mlrun import get_or_create_project\n",
    "\n",
    "image = \"mlrun/mlrun\"\n",
    "project_name = \"langchain-example\"\n",
    "project = get_or_create_project(project_name, context=\"./\", allow_cross_project=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0e0513-223e-424d-8ae3-72ef78a74b85",
   "metadata": {},
   "source": [
    "#### Temp private marketplace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c1e2867-8594-4409-8c12-5b133bdec441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IndexedHubSource(index=1, source=HubSource(kind=HubSource, metadata=HubObjectMetadata(name='hub15', description='a private hub', labels={}, updated=datetime.datetime(2025, 12, 3, 7, 17, 54, 932778, tzinfo=datetime.timezone.utc), created=datetime.datetime(2025, 12, 3, 7, 17, 54, 932778, tzinfo=datetime.timezone.utc)), spec=HubSourceSpec(path='https://raw.githubusercontent.com/royischoss/marketplace/refs/heads/master', channel='master', credentials={}), status=ObjectStatus(state='created')))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlrun.common.schemas\n",
    "\n",
    "# Add a custom hub to the top of the list\n",
    "private_source = mlrun.common.schemas.IndexedHubSource(\n",
    "    index=1,\n",
    "    source=mlrun.common.schemas.HubSource(\n",
    "        metadata=mlrun.common.schemas.HubObjectMetadata(\n",
    "            name=\"hub15\", description=\"a private hub\"\n",
    "        ),\n",
    "        spec=mlrun.common.schemas.HubSourceSpec(\n",
    "            path=\"https://raw.githubusercontent.com/royischoss/marketplace/refs/heads/master\", \n",
    "            channel=\"master\",\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "db = mlrun.get_run_db()\n",
    "db.create_hub_source(private_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c136d-7b21-4191-81a9-7bd353cd2c9f",
   "metadata": {},
   "source": [
    "#### Create openai secret:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a47d7789-2ea2-493e-8905-f53b978e2abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create project secrets for project\n",
    "secrets = {\"OPENAI_API_KEY\": \"eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJhZXNKN2kxNGNidnVuTU40MTJrOU5yZ2ROeENhTlJudTNPbC1TU08ycFlJIn0.eyJleHAiOjE3NjQ3NDc3MDIsImlhdCI6MTc2NDc0NTkwMywiYXV0aF90aW1lIjoxNzY0NzQ1ODgzLCJqdGkiOiI5MjZiZWQ1ZS0wYWI1LTQyZmQtYjVlNi0wM2M0M2Q1ZTNkNWEiLCJpc3MiOiJodHRwczovL2F1dGgubWNraW5zZXkuaWQvYXV0aC9yZWFsbXMvciIsImF1ZCI6ImJjZDIzNzI4LTNkMjctNDQ3Yy1hMGE5LWVhY2FmMzkzYTZmNSIsInN1YiI6IjFiNGVlOThiLWRmYjUtNDBmZS05ODcwLWRjNWVlNTk2ZDM1MSIsInR5cCI6IklEIiwiYXpwIjoiYmNkMjM3MjgtM2QyNy00NDdjLWEwYTktZWFjYWYzOTNhNmY1Iiwic2Vzc2lvbl9zdGF0ZSI6IjI1ZmI5YTk3LTAzOWYtNDRhMi1hMDM0LWNlYzcwOTlkMWZhMCIsImF0X2hhc2giOiIwb1plcldMTXBPMXdpM1NNS2Y1M0tBIiwibmFtZSI6IlJveSBTY2hvc3NiZXJnZXIiLCJnaXZlbl9uYW1lIjoiUm95IiwiZmFtaWx5X25hbWUiOiJTY2hvc3NiZXJnZXIiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiIxZjgwM2Y4N2YxNDkxZjQxIiwiZW1haWwiOiJSb3lfU2Nob3NzYmVyZ2VyQG1ja2luc2V5LmNvbSIsImFjciI6IjEiLCJzaWQiOiIyNWZiOWE5Ny0wMzlmLTQ0YTItYTAzNC1jZWM3MDk5ZDFmYTAiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiZm1ubyI6IjMzODc0MCIsImdyb3VwcyI6WyI2NGE4NjBmOC03YjJiLTRmOWMtYmYzOS1iZjZjODJmZWM2NTciLCJBbGwgRmlybSBVc2VycyJdfQ.W_tq6nAMi_HWPp0v7G5Is52RguG6uq2XvlEMae2MoogQAY_6rnDcD3suPnszEKSOSTf7Lns3XJepcaWRFfimjoN1vexkJtL8nXfsIgZAUqicqrbIUpbTSU1lycGkuX4UcLphFPcSkrpiEqX1-7RsAZN5cmQPyYmW11zJeJjd6ZWq934HvZE6431j_d3md9v2K1OQu8homFEOkepu7BfLxXylqcs3k_1fehGiSCgcn5IXpL6TdK75Orkahadc1CaoW1jfzyDe6E2aTWMJ56KWuw0LDKLWsLLnBA5j7nZSXRKixMGCXz-NdXpEbz_0wxS1Me4U9JGQt22yzoCPqQmiuw\"}\n",
    "project.set_secrets(secrets=secrets, provider=\"kubernetes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0eaf4a-fb29-4a50-97de-8b60b26ae34b",
   "metadata": {},
   "source": [
    "#### Write your python file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cbd982-86de-43b5-91ef-24fc60b2d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wximq.py\n",
    "\n",
    "# Langchain impoets for Agent initialization and use:\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# mlrun imports:\n",
    "import mlrun\n",
    "from mlrun.serving import Model\n",
    "\n",
    "# General imports:\n",
    "from typing import Any\n",
    "import os\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Use this tool to evaluate a mathematical expression.\n",
    "    It can handle addition, mutliplication, subtraction, division and exponents.\n",
    "    Example: `calculator(\"2 + 2\")` or `calculator('3**4')`\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except Exception as e:\n",
    "        return f\"Error evaluating expression: {e}\"\n",
    "\n",
    "\n",
    "\n",
    "class ModuleModelWrapper(Model):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            *args,\n",
    "            prompt_template:str,\n",
    "            **kwargs\n",
    "    ) -> None:\n",
    "        self.prompt_template = prompt_template\n",
    "        print(f\"[Roy] {self.prompt_template}, {ModuleModelWrapper._dict_fields}\")\n",
    "        super().__init__(**kwargs)\n",
    "        self.executor = None\n",
    "        \n",
    "\n",
    "    def predict(self, body: Any, **kwargs) -> Any:\n",
    "        if not self.executor:\n",
    "            raise RuntimeError(\"Model not loaded. Call load() before predict().\")\n",
    "        answer = self.executor.invoke(\n",
    "            {\n",
    "            \"input\": body.pop(\"question\", \"\"),\n",
    "            # Custom fields if your agent template uses them\n",
    "            },\n",
    "            config={\n",
    "                \"max_iterations\": 8,\n",
    "                \"callbacks\": [],\n",
    "                \"run_name\": \"my_run\",\n",
    "                \"metadata\": {\"request_id\": \"123\"},\n",
    "                }\n",
    "            )\n",
    "        body = answer\n",
    "        return answer\n",
    "        \n",
    "\n",
    "    def load(self):\n",
    "        if not self.executor:\n",
    "            os.environ[\"OPENAI_API_KEY\"] = mlrun.get_secret_or_env(\"OPENAI_API_KEY\")\n",
    "            model = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "                               openai_api_base=\"https://openai.prod.ai-gateway.quantumblack.com/64a860f8-7b2b-4f9c-bf39-bf6c82fec657/v1\"\n",
    "                              )\n",
    "            agent = create_react_agent(llm=model, tools=[calculator], prompt=PromptTemplate.from_template(self.prompt_template))\n",
    "            self.executor = AgentExecutor(\n",
    "                agent=agent,\n",
    "                tools=[calculator],\n",
    "                \n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e6b930-4692-4ba3-b9fd-3ba3a4aa064b",
   "metadata": {},
   "source": [
    "#### Import the module from the hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "691e9068-ec9c-40d6-9ac8-e6c3e605b44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-12-03 10:55:46,194 [info] Project loaded successfully: {\"project_name\":\"langchain-example-10\"}\n",
      "> 2025-12-03 10:55:46,463 [info] Model monitoring credentials were set successfully. Please keep in mind that if you already had model monitoring functions / model monitoring infra / tracked model server deployed on your project, you will need to redeploy them. For redeploying the model monitoring infra, first disable it using `project.disable_model_monitoring()` and then enable it using `project.enable_model_monitoring()`.\n",
      "details: MLRunConflictError(\"The following model-montioring infrastructure functions are already deployed, aborting: ['model-monitoring-controller', 'model-monitoring-writer']\\nIf you want to redeploy the model-monitoring controller (maybe with different base-period), use update_model_monitoring_controller.If you want to redeploy all of model-monitoring infrastructure, call disable_model_monitoringbefore calling enable_model_monitoring again.\")\n"
     ]
    }
   ],
   "source": [
    "module = mlrun.import_module(\"hub://hub15/agent_wrapper\")\n",
    "\n",
    "agent = module.AgentDeployer(\n",
    "            project_name=project_name,\n",
    "            agent_name=\"langchain_agent\",\n",
    "            model_class_name=\"ModuleModelWrapper\",\n",
    "            function=\"langchain_model.py\",\n",
    "            model_params={\"prompt_template\": \"\"\"\n",
    "            Answer the following questions as best you can.\n",
    "            You have access to the following tools:\n",
    "            {tools}\n",
    "            Use the following format:\n",
    "            Question: the input question you must answer\n",
    "            Thought: you should always think about what to do\n",
    "            Action: the action to take, should be one of [{tool_names}]\n",
    "            Action Input: the input to the action\n",
    "            Observation: the result of the action\n",
    "            ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "            Thought: I now know the final answer\n",
    "            Final Answer: the final answer to the original input question\n",
    "            \n",
    "            Begin!\n",
    "            Question: {input}\n",
    "            Thought:{agent_scratchpad}\n",
    "            \"\"\"},\n",
    "            result_path=\"output\",\n",
    "            inputs_path=\"input\",\n",
    "            requirements=[\"langchain==0.3.7\", \"langchain-openai==0.2.5\", \"langchain-community==0.3.3\"],\n",
    "            image = image,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0bb1c4d1-5d7c-4d1c-bf51-8f53b319e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = agent.get_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8973e2a7-007e-45cc-9683-251489411d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-12-03 10:55:51,204 [info] Starting remote function deploy\n",
      "2025-12-03 10:55:51  (info) Deploying function\n",
      "2025-12-03 10:55:51  (info) Building\n",
      "2025-12-03 10:55:51  (info) Staging files and preparing base images\n",
      "2025-12-03 10:55:51  (warn) Using user provided base image, runtime interpreter version is provided by the base image\n",
      "2025-12-03 10:55:51  (info) Building processor image\n",
      "2025-12-03 10:57:17  (info) Build complete\n",
      "2025-12-03 10:57:25  (info) Function deploy complete\n",
      "> 2025-12-03 10:57:32,590 [info] Model endpoint creation task completed with state succeeded\n",
      "> 2025-12-03 10:57:32,590 [info] Successfully deployed function: {\"external_invocation_urls\":[\"langchain-example-10-langchain-agent-serving-function.default-tenant.app.vmdev75.lab.iguazeng.com/\"],\"internal_invocation_urls\":[\"nuclio-langchain-example-10-langchain-agent-serving-function.default-tenant.svc.cluster.local:8080\"]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://langchain-example-10-langchain-agent-serving-function.default-tenant.app.vmdev75.lab.iguazeng.com/'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "222dba03-9066-427f-8670-3b516da91ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'If a pizza costs $18.75 and I want to buy 3, plus a 15% tip, what is the total cost?',\n",
       " 'output': '$64.69'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.invoke(\"./\", {\"question\" : \"If a pizza costs $18.75 and I want to buy 3, plus a 15% tip, what is the total cost?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-base-py311",
   "language": "python",
   "name": "conda-env-mlrun-base-py311-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
