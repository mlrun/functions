{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7412335f",
   "metadata": {},
   "source": [
    "# LLM as a Judge\n",
    "\n",
    "Based on llm as a judge paper https://arxiv.org/abs/2306.05685, We have implemented this function.\n",
    "\n",
    "In this notebook we will go over the function's docs and outputs and see an end-to-end example of running it.\n",
    "\n",
    "1. [Single grading metrics](#chapter1)\n",
    "2. [Pairwise grading metrics](#chapter2)\n",
    "3. [Pairwise_with_reference_grading_metrics](#chapter3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb6c621",
   "metadata": {},
   "source": [
    "<a id=\"chapter1\"></a>\n",
    "## 1. Single grading metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8230bec7-797d-4eac-b195-043cb445aa8e",
   "metadata": {},
   "source": [
    "Single grading metrics will use a self-defined metrics and return a scroe based on the examples and rubric of the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba19c7c-3571-4151-8746-57b8eb167518",
   "metadata": {},
   "source": [
    "### define the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fae4e28-fe47-434d-9df6-1438ed900820",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_config = {\n",
    "        \"name\": \"accuracy\",\n",
    "        \"definition\": \"The accuracy of the provided answer.\",\n",
    "        \"rubric\": \"\"\"Accuracy: This rubric assesses the accuracy of the provided answer. The details for different scores are as follows:\n",
    "            - Score 1: The answer is completely incorrect or irrelevant to the question. It demonstrates a fundamental\n",
    "              misunderstanding of the topic or question.\n",
    "            - Score 2: The answer contains significant inaccuracies, though it shows some understanding of the topic. Key\n",
    "              elements of the question are addressed incorrectly.\n",
    "            - Score 3: The answer is partially correct but has noticeable inaccuracies or omissions. It addresses the\n",
    "              question but lacks depth or precision.\n",
    "            - Score 4: The answer is mostly correct, with only minor inaccuracies or omissions. It provides a generally\n",
    "              accurate response to the question.\n",
    "            - Score 5: The answer is completely correct and thorough. It demonstrates a deep and accurate understanding of\n",
    "              the topic, addressing all elements of the question effectively.\"\"\",\n",
    "        \"examples\": \"\"\"\n",
    "            Question: What is the capital of France?\n",
    "            Score 1: Completely Incorrect\n",
    "            Answer: \"The capital of France is Berlin.\"\n",
    "            Explanation: This answer is entirely incorrect and irrelevant, as Berlin is the capital of Germany, not France.\n",
    "            Score 2: Significantly Inaccurate\n",
    "            Answer: \"The capital of France is Lyon.\"\n",
    "            Explanation: This answer demonstrates some understanding that the question is about a city in France, but it incorrectly identifies Lyon as the capital instead of Paris.\n",
    "            Score 3: Partially Correct\n",
    "            Answer: \"I think the capital of France is either Paris or Marseille.\"\n",
    "            Explanation: This answer shows partial knowledge but includes a significant inaccuracy by suggesting Marseille might be the capital. Paris is correct, but the inclusion of Marseille indicates a lack of certainty or complete understanding.\n",
    "            Score 4: Mostly Correct\n",
    "            Answer: \"The capital of France is Paris, the largest city in the country.\"\n",
    "            Explanation: This answer is mostly correct and identifies Paris as the capital. The addition of \"the largest city in the country\" is accurate but not directly relevant to the capital status, introducing a slight deviation from the question's focus.\n",
    "            Score 5: Completely Correct and Thorough\n",
    "            Answer: \"The capital of France is Paris, which is not only the country's largest city but also its cultural and political center, hosting major institutions like the President's residence, the ElysÃ©e Palace.\"\n",
    "            Explanation: This answer is completely correct, providing a thorough explanation that adds relevant context about Paris's role as the cultural and political center of France, directly addressing the question with depth and precision.\n",
    "                     \"\"\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dccc112-0dc6-44dc-953a-6f8cda43e0ed",
   "metadata": {},
   "source": [
    "### HF model as Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abcfcb98-b5f4-4816-8d0b-7ecb8b675865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:07:20,105 [info] Project loaded successfully: {'project_name': 'llm-judge'}\n"
     ]
    }
   ],
   "source": [
    "import mlrun\n",
    "project = mlrun.get_or_create_project(\n",
    "    name=\"llm-judge\",\n",
    "    context = \"./\",\n",
    "    user_project=True\n",
    ")\n",
    "llm_judge_fn = project.set_function(name=\"llm-judge\", func=\"function.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dda6871-d132-443f-980d-ad796c5e5d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config of the hugggingface model as judge\n",
    "JUDGE_MODEL = \"TheBloke/Mistral-7B-OpenOrca-GPTQ\"\n",
    "JUDGE_CONFIG = {\n",
    "    \"device_map\": \"auto\",\n",
    "    \"revision\": \"gptq-8bit-128g-actorder_True\",\n",
    "    \"trust_remote_code\": False,\n",
    "}\n",
    "JUDGE_INFER_CONFIG = {\n",
    "    \"max_length\": 1500,\n",
    "}\n",
    "TOKENIZER_JUDGE_CONFIG = {\"use_fast\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b25bd59-e13c-4f15-956e-52dba14be620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:07:20,180 [warning] artifact/output path is not defined or is local and relative, artifacts will not be visible in the UI: {'output_path': './'}\n",
      "> 2024-02-21 00:07:20,180 [info] Storing function: {'name': 'llm-judge-llm-judge', 'uid': 'caa85cbd0c884d23a97ff39d13aae9f4', 'db': None}\n",
      "> 2024-02-21 00:07:28,093 [info] logging run results to: http://mlrun-api:8080\n",
      "> 2024-02-21 00:07:28,522 [info] Preparing the judge model TheBloke/Mistral-7B-OpenOrca-GPTQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:08:29,870 [info] Computing the metrics over all data\n",
      "> 2024-02-21 00:08:29,877 [info] Computing the metrics over one data point with What is the capital of China? and The capital of China is Kongfu\n",
      "> 2024-02-21 00:08:29,877 [info] Filling the prompt template with the prompt config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:09:08,888 [info] Extracting the score and explanation from \n",
      "Task:\n",
      "Please act as an impartial judge and evaluate the quality of the response provided by an\n",
      "AI assistant to the user question displayed below. You will be given the definition of accuracy, grading rubric, context information.\n",
      "Your task is to determine a numerical score of accuracy for the response. You must use the grading rubric to determine your score. You must also give a explanation about how did you determine the score step-by-step. Please use chain of thinking.\n",
      "Examples could be included beblow for your reference. Make sure you understand the grading rubric and use the examples before completing the task.\n",
      "[Examples]:\n",
      "\n",
      "            Question: What is the capital of France?\n",
      "            Score 1: Completely Incorrect\n",
      "            Answer: \"The capital of France is Berlin.\"\n",
      "            Explanation: This answer is entirely incorrect and irrelevant, as Berlin is the capital of Germany, not France.\n",
      "            Score 2: Significantly Inaccurate\n",
      "            Answer: \"The capital of France is Lyon.\"\n",
      "            Explanation: This answer demonstrates some understanding that the question is about a city in France, but it incorrectly identifies Lyon as the capital instead of Paris.\n",
      "            Score 3: Partially Correct\n",
      "            Answer: \"I think the capital of France is either Paris or Marseille.\"\n",
      "            Explanation: This answer shows partial knowledge but includes a significant inaccuracy by suggesting Marseille might be the capital. Paris is correct, but the inclusion of Marseille indicates a lack of certainty or complete understanding.\n",
      "            Score 4: Mostly Correct\n",
      "            Answer: \"The capital of France is Paris, the largest city in the country.\"\n",
      "            Explanation: This answer is mostly correct and identifies Paris as the capital. The addition of \"the largest city in the country\" is accurate but not directly relevant to the capital status, introducing a slight deviation from the question's focus.\n",
      "            Score 5: Completely Correct and Thorough\n",
      "            Answer: \"The capital of France is Paris, which is not only the country's largest city but also its cultural and political center, hosting major institutions like the President's residence, the ElysÃ©e Palace.\"\n",
      "            Explanation: This answer is completely correct, providing a thorough explanation that adds relevant context about Paris's role as the cultural and political center of France, directly addressing the question with depth and precision.\n",
      "                     \n",
      "[User Question]:\n",
      "What is the capital of China?\n",
      "[Response]:\n",
      "The capital of China is Kongfu\n",
      "[Definition of accuracy]:\n",
      "The accuracy of the provided answer.\n",
      "[Grading Rubric]:\n",
      "Accuracy: This rubric assesses the accuracy of the provided answer. The details for different scores are as follows:\n",
      "            - Score 1: The answer is completely incorrect or irrelevant to the question. It demonstrates a fundamental\n",
      "              misunderstanding of the topic or question.\n",
      "            - Score 2: The answer contains significant inaccuracies, though it shows some understanding of the topic. Key\n",
      "              elements of the question are addressed incorrectly.\n",
      "            - Score 3: The answer is partially correct but has noticeable inaccuracies or omissions. It addresses the\n",
      "              question but lacks depth or precision.\n",
      "            - Score 4: The answer is mostly correct, with only minor inaccuracies or omissions. It provides a generally\n",
      "              accurate response to the question.\n",
      "            - Score 5: The answer is completely correct and thorough. It demonstrates a deep and accurate understanding of\n",
      "              the topic, addressing all elements of the question effectively.\n",
      "You must return the following fields in your output:\n",
      "- score: a numerical score of accuracy for the response\n",
      "- explanation: a explanation about how did you determine the score step-by-step\n",
      "[Output]:\n",
      "score: 1\n",
      "explanation: The answer is completely incorrect and irrelevant to the question. The capital of China is Beijing, not Kongfu.\n",
      "> 2024-02-21 00:09:08,897 [info] Computing the metrics over one data point with What is the capital of France? and The capital of France is Paris\n",
      "> 2024-02-21 00:09:08,898 [info] Filling the prompt template with the prompt config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:10:15,169 [info] Extracting the score and explanation from \n",
      "Task:\n",
      "Please act as an impartial judge and evaluate the quality of the response provided by an\n",
      "AI assistant to the user question displayed below. You will be given the definition of accuracy, grading rubric, context information.\n",
      "Your task is to determine a numerical score of accuracy for the response. You must use the grading rubric to determine your score. You must also give a explanation about how did you determine the score step-by-step. Please use chain of thinking.\n",
      "Examples could be included beblow for your reference. Make sure you understand the grading rubric and use the examples before completing the task.\n",
      "[Examples]:\n",
      "\n",
      "            Question: What is the capital of France?\n",
      "            Score 1: Completely Incorrect\n",
      "            Answer: \"The capital of France is Berlin.\"\n",
      "            Explanation: This answer is entirely incorrect and irrelevant, as Berlin is the capital of Germany, not France.\n",
      "            Score 2: Significantly Inaccurate\n",
      "            Answer: \"The capital of France is Lyon.\"\n",
      "            Explanation: This answer demonstrates some understanding that the question is about a city in France, but it incorrectly identifies Lyon as the capital instead of Paris.\n",
      "            Score 3: Partially Correct\n",
      "            Answer: \"I think the capital of France is either Paris or Marseille.\"\n",
      "            Explanation: This answer shows partial knowledge but includes a significant inaccuracy by suggesting Marseille might be the capital. Paris is correct, but the inclusion of Marseille indicates a lack of certainty or complete understanding.\n",
      "            Score 4: Mostly Correct\n",
      "            Answer: \"The capital of France is Paris, the largest city in the country.\"\n",
      "            Explanation: This answer is mostly correct and identifies Paris as the capital. The addition of \"the largest city in the country\" is accurate but not directly relevant to the capital status, introducing a slight deviation from the question's focus.\n",
      "            Score 5: Completely Correct and Thorough\n",
      "            Answer: \"The capital of France is Paris, which is not only the country's largest city but also its cultural and political center, hosting major institutions like the President's residence, the ElysÃ©e Palace.\"\n",
      "            Explanation: This answer is completely correct, providing a thorough explanation that adds relevant context about Paris's role as the cultural and political center of France, directly addressing the question with depth and precision.\n",
      "                     \n",
      "[User Question]:\n",
      "What is the capital of France?\n",
      "[Response]:\n",
      "The capital of France is Paris\n",
      "[Definition of accuracy]:\n",
      "The accuracy of the provided answer.\n",
      "[Grading Rubric]:\n",
      "Accuracy: This rubric assesses the accuracy of the provided answer. The details for different scores are as follows:\n",
      "            - Score 1: The answer is completely incorrect or irrelevant to the question. It demonstrates a fundamental\n",
      "              misunderstanding of the topic or question.\n",
      "            - Score 2: The answer contains significant inaccuracies, though it shows some understanding of the topic. Key\n",
      "              elements of the question are addressed incorrectly.\n",
      "            - Score 3: The answer is partially correct but has noticeable inaccuracies or omissions. It addresses the\n",
      "              question but lacks depth or precision.\n",
      "            - Score 4: The answer is mostly correct, with only minor inaccuracies or omissions. It provides a generally\n",
      "              accurate response to the question.\n",
      "            - Score 5: The answer is completely correct and thorough. It demonstrates a deep and accurate understanding of\n",
      "              the topic, addressing all elements of the question effectively.\n",
      "You must return the following fields in your output:\n",
      "- score: a numerical score of accuracy for the response\n",
      "- explanation: a explanation about how did you determine the score step-by-step\n",
      "[Output]:\n",
      "score: 4\n",
      "explanation: The response is mostly correct, as it identifies Paris as the capital of France. However, it lacks depth or precision by not providing additional context or information about Paris's role as the political and cultural center of France.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>llm-judge-pengwei</td>\n",
       "      <td><div title=\"caa85cbd0c884d23a97ff39d13aae9f4\"><a href=\"https://dashboard.default-tenant.app.llm5.iguazio-cd1.com/mlprojects/llm-judge-pengwei/jobs/monitor/caa85cbd0c884d23a97ff39d13aae9f4/overview\" target=\"_blank\" >...13aae9f4</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Feb 21 00:07:20</td>\n",
       "      <td>completed</td>\n",
       "      <td>llm-judge-llm-judge</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=pengwei</div><div class=\"dictlist\">kind=local</div><div class=\"dictlist\">owner=pengwei</div><div class=\"dictlist\">host=jupyter-pengwei-gpu-7777658756-jcs45</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">input_path=data/qa.csv</div><div class=\"dictlist\">metric_type=LLMJudgeSingleGrading</div><div class=\"dictlist\">name=accuracy_metrics</div><div class=\"dictlist\">model_judge=TheBloke/Mistral-7B-OpenOrca-GPTQ</div><div class=\"dictlist\">model_judge_config={'device_map': 'auto', 'revision': 'gptq-8bit-128g-actorder_True', 'trust_remote_code': False}</div><div class=\"dictlist\">model_judge_infer_config={'max_length': 1500}</div><div class=\"dictlist\">prompt_config={'name': 'accuracy', 'definition': 'The accuracy of the provided answer.', 'rubric': 'Accuracy: This rubric assesses the accuracy of the provided answer. The details for different scores are as follows:\\n            - Score 1: The answer is completely incorrect or irrelevant to the question. It demonstrates a fundamental\\n              misunderstanding of the topic or question.\\n            - Score 2: The answer contains significant inaccuracies, though it shows some understanding of the topic. Key\\n              elements of the question are addressed incorrectly.\\n            - Score 3: The answer is partially correct but has noticeable inaccuracies or omissions. It addresses the\\n              question but lacks depth or precision.\\n            - Score 4: The answer is mostly correct, with only minor inaccuracies or omissions. It provides a generally\\n              accurate response to the question.\\n            - Score 5: The answer is completely correct and thorough. It demonstrates a deep and accurate understanding of\\n              the topic, addressing all elements of the question effectively.', 'examples': '\\n            Question: What is the capital of France?\\n            Score 1: Completely Incorrect\\n            Answer: \"The capital of France is Berlin.\"\\n            Explanation: This answer is entirely incorrect and irrelevant, as Berlin is the capital of Germany, not France.\\n            Score 2: Significantly Inaccurate\\n            Answer: \"The capital of France is Lyon.\"\\n            Explanation: This answer demonstrates some understanding that the question is about a city in France, but it incorrectly identifies Lyon as the capital instead of Paris.\\n            Score 3: Partially Correct\\n            Answer: \"I think the capital of France is either Paris or Marseille.\"\\n            Explanation: This answer shows partial knowledge but includes a significant inaccuracy by suggesting Marseille might be the capital. Paris is correct, but the inclusion of Marseille indicates a lack of certainty or complete understanding.\\n            Score 4: Mostly Correct\\n            Answer: \"The capital of France is Paris, the largest city in the country.\"\\n            Explanation: This answer is mostly correct and identifies Paris as the capital. The addition of \"the largest city in the country\" is accurate but not directly relevant to the capital status, introducing a slight deviation from the question\\'s focus.\\n            Score 5: Completely Correct and Thorough\\n            Answer: \"The capital of France is Paris, which is not only the country\\'s largest city but also its cultural and political center, hosting major institutions like the President\\'s residence, the ElysÃ©e Palace.\"\\n            Explanation: This answer is completely correct, providing a thorough explanation that adds relevant context about Paris\\'s role as the cultural and political center of France, directly addressing the question with depth and precision.\\n                     '}</div><div class=\"dictlist\">tokenizer_judge_config={'use_fast': True}</div></td>\n",
       "      <td></td>\n",
       "      <td><div title=\"/User/functions_old/llm_judge/llm-judge-llm-judge/0/single_result.parquet\">single_result</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result1bebee7a-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result1bebee7a-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result1bebee7a\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result1bebee7a-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods  or <a href=\"https://dashboard.default-tenant.app.llm5.iguazio-cd1.com/mlprojects/llm-judge-pengwei/jobs/monitor/caa85cbd0c884d23a97ff39d13aae9f4/overview\" target=\"_blank\">click here</a> to open in UI</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:10:15,418 [info] Run execution finished: {'status': 'completed', 'name': 'llm-judge-llm-judge'}\n"
     ]
    }
   ],
   "source": [
    "single_grading_run = llm_judge_fn.run(\n",
    "    handler=\"llm_judge\",\n",
    "    params={\n",
    "        \"input_path\": \"data/qa.csv\",\n",
    "        \"metric_type\": \"LLMJudgeSingleGrading\",\n",
    "        \"name\": \"accuracy_metrics\",\n",
    "        \"model_judge\": JUDGE_MODEL,\n",
    "        \"model_judge_config\": JUDGE_CONFIG,\n",
    "        \"model_judge_infer_config\": JUDGE_INFER_CONFIG,\n",
    "        \"prompt_config\" :prompt_config,\n",
    "        \"tokenizer_judge_config\": TOKENIZER_JUDGE_CONFIG,\n",
    "    },\n",
    "    returns=[\n",
    "        \"single_result: dataset\",\n",
    "    ],\n",
    "    local=True,\n",
    "    artifact_path=\"./\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231f9afc-d489-4377-9d7e-7c4f36dbcb6c",
   "metadata": {},
   "source": [
    "### OPENAI model as Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4d46c91-0c74-4fe0-a07d-63696ba968ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b537c0-5efc-4c8c-9b4f-f8ec84b6eaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:10:15,440 [warning] artifact/output path is not defined or is local and relative, artifacts will not be visible in the UI: {'output_path': './'}\n",
      "> 2024-02-21 00:10:15,440 [info] Storing function: {'name': 'llm-judge-llm-judge', 'uid': '63442e4c9912481db5c898393089c973', 'db': None}\n",
      "> 2024-02-21 00:10:16,446 [info] Prepare the openAI model as judge\n",
      "> 2024-02-21 00:10:16,469 [info] Computing the metrics over all data\n",
      "> 2024-02-21 00:10:16,470 [info] Compute the metrics over one data point using openAI's model\n",
      "> 2024-02-21 00:10:16,470 [info] Filling the prompt template with the prompt config\n",
      "> 2024-02-21 00:10:18,021 [info] Extracting the score and explanation from - score: 1\n",
      "- explanation: The response provided is completely incorrect as the capital of China is not Kongfu. The answer demonstrates a fundamental misunderstanding of the topic, therefore warranting a score of 1 according to the grading rubric.\n",
      "> 2024-02-21 00:10:18,025 [info] Compute the metrics over one data point using openAI's model\n",
      "> 2024-02-21 00:10:18,025 [info] Filling the prompt template with the prompt config\n",
      "> 2024-02-21 00:10:19,413 [info] Extracting the score and explanation from - score: 5\n",
      "- explanation: The response provided by the AI assistant correctly identifies the capital of France as Paris without any inaccuracies or omissions. It directly addresses the question with precision and accuracy, making it a completely correct and thorough response. The answer demonstrates a deep understanding of the topic and provides the necessary information without any errors, leading to a score of 5.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>llm-judge-pengwei</td>\n",
       "      <td><div title=\"63442e4c9912481db5c898393089c973\"><a href=\"https://dashboard.default-tenant.app.llm5.iguazio-cd1.com/mlprojects/llm-judge-pengwei/jobs/monitor/63442e4c9912481db5c898393089c973/overview\" target=\"_blank\" >...3089c973</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Feb 21 00:10:15</td>\n",
       "      <td>completed</td>\n",
       "      <td>llm-judge-llm-judge</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=pengwei</div><div class=\"dictlist\">kind=local</div><div class=\"dictlist\">owner=pengwei</div><div class=\"dictlist\">host=jupyter-pengwei-gpu-7777658756-jcs45</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">input_path=data/qa.csv</div><div class=\"dictlist\">metric_type=OPENAIJudgeSingleGrading</div><div class=\"dictlist\">name=accuracy_metrics</div><div class=\"dictlist\">model_judge=gpt-3.5-turbo</div><div class=\"dictlist\">prompt_config={'name': 'accuracy', 'definition': 'The accuracy of the provided answer.', 'rubric': 'Accuracy: This rubric assesses the accuracy of the provided answer. The details for different scores are as follows:\\n            - Score 1: The answer is completely incorrect or irrelevant to the question. It demonstrates a fundamental\\n              misunderstanding of the topic or question.\\n            - Score 2: The answer contains significant inaccuracies, though it shows some understanding of the topic. Key\\n              elements of the question are addressed incorrectly.\\n            - Score 3: The answer is partially correct but has noticeable inaccuracies or omissions. It addresses the\\n              question but lacks depth or precision.\\n            - Score 4: The answer is mostly correct, with only minor inaccuracies or omissions. It provides a generally\\n              accurate response to the question.\\n            - Score 5: The answer is completely correct and thorough. It demonstrates a deep and accurate understanding of\\n              the topic, addressing all elements of the question effectively.', 'examples': '\\n            Question: What is the capital of France?\\n            Score 1: Completely Incorrect\\n            Answer: \"The capital of France is Berlin.\"\\n            Explanation: This answer is entirely incorrect and irrelevant, as Berlin is the capital of Germany, not France.\\n            Score 2: Significantly Inaccurate\\n            Answer: \"The capital of France is Lyon.\"\\n            Explanation: This answer demonstrates some understanding that the question is about a city in France, but it incorrectly identifies Lyon as the capital instead of Paris.\\n            Score 3: Partially Correct\\n            Answer: \"I think the capital of France is either Paris or Marseille.\"\\n            Explanation: This answer shows partial knowledge but includes a significant inaccuracy by suggesting Marseille might be the capital. Paris is correct, but the inclusion of Marseille indicates a lack of certainty or complete understanding.\\n            Score 4: Mostly Correct\\n            Answer: \"The capital of France is Paris, the largest city in the country.\"\\n            Explanation: This answer is mostly correct and identifies Paris as the capital. The addition of \"the largest city in the country\" is accurate but not directly relevant to the capital status, introducing a slight deviation from the question\\'s focus.\\n            Score 5: Completely Correct and Thorough\\n            Answer: \"The capital of France is Paris, which is not only the country\\'s largest city but also its cultural and political center, hosting major institutions like the President\\'s residence, the ElysÃ©e Palace.\"\\n            Explanation: This answer is completely correct, providing a thorough explanation that adds relevant context about Paris\\'s role as the cultural and political center of France, directly addressing the question with depth and precision.\\n                     '}</div></td>\n",
       "      <td></td>\n",
       "      <td><div title=\"/User/functions_old/llm_judge/llm-judge-llm-judge/0/openai_single_result.parquet\">openai_single_result</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result14d9007e-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result14d9007e-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result14d9007e\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result14d9007e-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods  or <a href=\"https://dashboard.default-tenant.app.llm5.iguazio-cd1.com/mlprojects/llm-judge-pengwei/jobs/monitor/63442e4c9912481db5c898393089c973/overview\" target=\"_blank\">click here</a> to open in UI</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:10:19,584 [info] Run execution finished: {'status': 'completed', 'name': 'llm-judge-llm-judge'}\n"
     ]
    }
   ],
   "source": [
    "openai_single_grading_run = llm_judge_fn.run(\n",
    "    handler=\"llm_judge\",\n",
    "    params={\n",
    "        \"input_path\": \"data/qa.csv\",\n",
    "        \"metric_type\": \"OPENAIJudgeSingleGrading\",\n",
    "        \"name\": \"accuracy_metrics\",\n",
    "        \"model_judge\": OPENAI_MODEL,\n",
    "        \"prompt_config\" :prompt_config,\n",
    "    },\n",
    "    returns=[\n",
    "        \"openai_single_result: dataset\",\n",
    "    ],\n",
    "    local=True,\n",
    "    artifact_path=\"./\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34ae427-acb4-4417-9e6f-65f373e3cf01",
   "metadata": {},
   "source": [
    "<a id=\"chapter2\"></a>\n",
    "## 2. Pairwise grading metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4208be-86eb-4735-8a28-220cd0140638",
   "metadata": {},
   "source": [
    "Pairwise grading metrics will use a smaller model as the benchmark model. It will ask the Judge to give two scores to the customized model and benchmark model to understand how well the model performs comparing with the benchmark model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c3753-5d95-42fe-850e-00b98d97d204",
   "metadata": {},
   "source": [
    "### HF model as Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "812dc712-1a42-4b03-af0a-c3ef4a4cbd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_MODEL = \"microsoft/phi-2\"\n",
    "BENCHMARK_CONFIG = {\n",
    "    \"max_length\": 1500,\n",
    "    \"device_map\": \"auto\",\n",
    "    \"revision\": \"main\",\n",
    "    \"trust_remote_code\": True,\n",
    "    \"torch_dtype\": \"auto\",\n",
    "}\n",
    "TOKENIZER_BENCHMARK_CONFIG = {\"trust_remote_code\": True}\n",
    "BENCHMARK_INFER_CONFIG = {\"max_length\": 1500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95bff55c-e315-400e-a935-c04f34e78cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:10:19,602 [warning] artifact/output path is not defined or is local and relative, artifacts will not be visible in the UI: {'output_path': './'}\n",
      "> 2024-02-21 00:10:19,603 [info] Storing function: {'name': 'llm-judge-llm-judge', 'uid': '8e8d2c31987542878218f68a40068efc', 'db': None}\n",
      "> 2024-02-21 00:10:19,824 [info] Preparing the judge model TheBloke/Mistral-7B-OpenOrca-GPTQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:11:08,867 [info] Preparing the bench mark model microsoft/phi-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e825048e48f74d399abfa10a00fd1b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:11:51,373 [info] Computing the metrics over What is the capital of China? and The capital of China is Kongfu\n",
      "> 2024-02-21 00:11:51,374 [info] Computing the bench mark response for What is the capital of China?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:11:52,703 [info] Response of the bench mark model is What is the capital of China?\n",
      "A) Beijing\n",
      "B) Shanghai\n",
      "C) Hong Kong\n",
      "D) Tokyo\n",
      "\n",
      "Answer: A) Beijing\n",
      "\n",
      "> 2024-02-21 00:11:52,704 [info] Filling the prompt template with the prompt config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:13:32,039 [info] Response of the judge model is \n",
      "Task:\n",
      "Your task is to determine two numerical score of accuracy for the responses from two AI assistants. You must use the grading rubric to determine your scores. You must also give a explanation about how did you determine the scores step-by-step. Please using chain of thinking.\n",
      "Examples could be included beblow for your reference. Make sure you understand the grading rubric and use the examples before completing the task.\n",
      "[Examples]:\n",
      "\n",
      "            Question: What is the capital of France?\n",
      "            Score 1: Completely Incorrect\n",
      "            Answer: \"The capital of France is Berlin.\"\n",
      "            Explanation: This answer is entirely incorrect and irrelevant, as Berlin is the capital of Germany, not France.\n",
      "            Score 2: Significantly Inaccurate\n",
      "            Answer: \"The capital of France is Lyon.\"\n",
      "            Explanation: This answer demonstrates some understanding that the question is about a city in France, but it incorrectly identifies Lyon as the capital instead of Paris.\n",
      "            Score 3: Partially Correct\n",
      "            Answer: \"I think the capital of France is either Paris or Marseille.\"\n",
      "            Explanation: This answer shows partial knowledge but includes a significant inaccuracy by suggesting Marseille might be the capital. Paris is correct, but the inclusion of Marseille indicates a lack of certainty or complete understanding.\n",
      "            Score 4: Mostly Correct\n",
      "            Answer: \"The capital of France is Paris, the largest city in the country.\"\n",
      "            Explanation: This answer is mostly correct and identifies Paris as the capital. The addition of \"the largest city in the country\" is accurate but not directly relevant to the capital status, introducing a slight deviation from the question's focus.\n",
      "            Score 5: Completely Correct and Thorough\n",
      "            Answer: \"The capital of France is Paris, which is not only the country's largest city but also its cultural and political center, hosting major institutions like the President's residence, the ElysÃ©e Palace.\"\n",
      "            Explanation: This answer is completely correct, providing a thorough explanation that adds relevant context about Paris's role as the cultural and political center of France, directly addressing the question with depth and precision.\n",
      "                     \n",
      "[User Question]:\n",
      "What is the capital of China?\n",
      "[Response of assistant A]:\n",
      "The capital of China is Kongfu\n",
      "[Response of assistant B]:\n",
      "What is the capital of China?\n",
      "A) Beijing\n",
      "B) Shanghai\n",
      "C) Hong Kong\n",
      "D) Tokyo\n",
      "\n",
      "Answer: A) Beijing\n",
      "\n",
      "[Definition of accuracy]:\n",
      "The accuracy of the provided answer.\n",
      "[Grading Rubric]:\n",
      "Accuracy: This rubric assesses the accuracy of the provided answer. The details for different scores are as follows:\n",
      "            - Score 1: The answer is completely incorrect or irrelevant to the question. It demonstrates a fundamental\n",
      "              misunderstanding of the topic or question.\n",
      "            - Score 2: The answer contains significant inaccuracies, though it shows some understanding of the topic. Key\n",
      "              elements of the question are addressed incorrectly.\n",
      "            - Score 3: The answer is partially correct but has noticeable inaccuracies or omissions. It addresses the\n",
      "              question but lacks depth or precision.\n",
      "            - Score 4: The answer is mostly correct, with only minor inaccuracies or omissions. It provides a generally\n",
      "              accurate response to the question.\n",
      "            - Score 5: The answer is completely correct and thorough. It demonstrates a deep and accurate understanding of\n",
      "              the topic, addressing all elements of the question effectively.\n",
      "You must return the following fields in your output:\n",
      "- score of assistant a: a numerical score of accuracy for the response\n",
      "- explanation of assistant a: a explanation about how did you determine the score step-by-step\n",
      "- score of assistant b: a numerical score of accuracy for the response\n",
      "- explanation of assistant b: a explanation about how did you determine the score step-by-step\n",
      "[Output]:\n",
      "- score of assistant a: 1\n",
      "- explanation of assistant a: The answer is completely incorrect and irrelevant to the question. It demonstrates a fundamental misunderstanding of the topic or question.\n",
      "- score of assistant b: 5\n",
      "- explanation of assistant b: The answer is completely correct and thorough. It demonstrates a deep and accurate understanding of the topic, addressing all elements of the question effectively.\n",
      "> 2024-02-21 00:13:32,043 [info] Computing the metrics over What is the capital of France? and The capital of France is Paris\n",
      "> 2024-02-21 00:13:32,043 [info] Computing the bench mark response for What is the capital of France?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:13:33,056 [info] Response of the bench mark model is What is the capital of France?\n",
      "A) London\n",
      "B) Paris\n",
      "C) Rome\n",
      "D) Berlin\n",
      "Answer: B) Paris\n",
      "\n",
      "> 2024-02-21 00:13:33,057 [info] Filling the prompt template with the prompt config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:15:51,290 [info] Response of the judge model is \n",
      "Task:\n",
      "Your task is to determine two numerical score of accuracy for the responses from two AI assistants. You must use the grading rubric to determine your scores. You must also give a explanation about how did you determine the scores step-by-step. Please using chain of thinking.\n",
      "Examples could be included beblow for your reference. Make sure you understand the grading rubric and use the examples before completing the task.\n",
      "[Examples]:\n",
      "\n",
      "            Question: What is the capital of France?\n",
      "            Score 1: Completely Incorrect\n",
      "            Answer: \"The capital of France is Berlin.\"\n",
      "            Explanation: This answer is entirely incorrect and irrelevant, as Berlin is the capital of Germany, not France.\n",
      "            Score 2: Significantly Inaccurate\n",
      "            Answer: \"The capital of France is Lyon.\"\n",
      "            Explanation: This answer demonstrates some understanding that the question is about a city in France, but it incorrectly identifies Lyon as the capital instead of Paris.\n",
      "            Score 3: Partially Correct\n",
      "            Answer: \"I think the capital of France is either Paris or Marseille.\"\n",
      "            Explanation: This answer shows partial knowledge but includes a significant inaccuracy by suggesting Marseille might be the capital. Paris is correct, but the inclusion of Marseille indicates a lack of certainty or complete understanding.\n",
      "            Score 4: Mostly Correct\n",
      "            Answer: \"The capital of France is Paris, the largest city in the country.\"\n",
      "            Explanation: This answer is mostly correct and identifies Paris as the capital. The addition of \"the largest city in the country\" is accurate but not directly relevant to the capital status, introducing a slight deviation from the question's focus.\n",
      "            Score 5: Completely Correct and Thorough\n",
      "            Answer: \"The capital of France is Paris, which is not only the country's largest city but also its cultural and political center, hosting major institutions like the President's residence, the ElysÃ©e Palace.\"\n",
      "            Explanation: This answer is completely correct, providing a thorough explanation that adds relevant context about Paris's role as the cultural and political center of France, directly addressing the question with depth and precision.\n",
      "                     \n",
      "[User Question]:\n",
      "What is the capital of France?\n",
      "[Response of assistant A]:\n",
      "The capital of France is Paris\n",
      "[Response of assistant B]:\n",
      "What is the capital of France?\n",
      "A) London\n",
      "B) Paris\n",
      "C) Rome\n",
      "D) Berlin\n",
      "Answer: B) Paris\n",
      "\n",
      "[Definition of accuracy]:\n",
      "The accuracy of the provided answer.\n",
      "[Grading Rubric]:\n",
      "Accuracy: This rubric assesses the accuracy of the provided answer. The details for different scores are as follows:\n",
      "            - Score 1: The answer is completely incorrect or irrelevant to the question. It demonstrates a fundamental\n",
      "              misunderstanding of the topic or question.\n",
      "            - Score 2: The answer contains significant inaccuracies, though it shows some understanding of the topic. Key\n",
      "              elements of the question are addressed incorrectly.\n",
      "            - Score 3: The answer is partially correct but has noticeable inaccuracies or omissions. It addresses the\n",
      "              question but lacks depth or precision.\n",
      "            - Score 4: The answer is mostly correct, with only minor inaccuracies or omissions. It provides a generally\n",
      "              accurate response to the question.\n",
      "            - Score 5: The answer is completely correct and thorough. It demonstrates a deep and accurate understanding of\n",
      "              the topic, addressing all elements of the question effectively.\n",
      "You must return the following fields in your output:\n",
      "- score of assistant a: a numerical score of accuracy for the response\n",
      "- explanation of assistant a: a explanation about how did you determine the score step-by-step\n",
      "- score of assistant b: a numerical score of accuracy for the response\n",
      "- explanation of assistant b: a explanation about how did you determine the score step-by-step\n",
      "[Output]:\n",
      "- score of assistant A: 5\n",
      "- explanation of assistant A: Assistant A provided a completely correct and thorough answer to the question, identifying Paris as the capital of France and adding relevant context about its role as the country's political and cultural center.\n",
      "- score of assistant B: 4\n",
      "- explanation of assistant B: Assistant B provided a mostly correct answer to the question, identifying Paris as the capital of France. However, the response format was not ideal, as it was presented as a multiple-choice question with options that were not relevant to the question.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>llm-judge-pengwei</td>\n",
       "      <td><div title=\"8e8d2c31987542878218f68a40068efc\"><a href=\"https://dashboard.default-tenant.app.llm5.iguazio-cd1.com/mlprojects/llm-judge-pengwei/jobs/monitor/8e8d2c31987542878218f68a40068efc/overview\" target=\"_blank\" >...40068efc</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Feb 21 00:10:19</td>\n",
       "      <td>completed</td>\n",
       "      <td>llm-judge-llm-judge</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=pengwei</div><div class=\"dictlist\">kind=local</div><div class=\"dictlist\">owner=pengwei</div><div class=\"dictlist\">host=jupyter-pengwei-gpu-7777658756-jcs45</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">input_path=data/qa.csv</div><div class=\"dictlist\">metric_type=LLMJudgePairwiseGrading</div><div class=\"dictlist\">name=accuracy_metrics</div><div class=\"dictlist\">model_judge=TheBloke/Mistral-7B-OpenOrca-GPTQ</div><div class=\"dictlist\">model_judge_config={'device_map': 'auto', 'revision': 'gptq-8bit-128g-actorder_True', 'trust_remote_code': False}</div><div class=\"dictlist\">model_judge_infer_config={'max_length': 1500}</div><div class=\"dictlist\">model_bench_mark=microsoft/phi-2</div><div class=\"dictlist\">model_bench_mark_config={'max_length': 1500, 'device_map': 'auto', 'revision': 'main', 'trust_remote_code': True, 'torch_dtype': 'auto'}</div><div class=\"dictlist\">model_bench_mark_infer_config={'max_length': 1500}</div><div class=\"dictlist\">tokenizer_bench_mark_config={'trust_remote_code': True}</div><div class=\"dictlist\">prompt_config={'name': 'accuracy', 'definition': 'The accuracy of the provided answer.', 'rubric': 'Accuracy: This rubric assesses the accuracy of the provided answer. The details for different scores are as follows:\\n            - Score 1: The answer is completely incorrect or irrelevant to the question. It demonstrates a fundamental\\n              misunderstanding of the topic or question.\\n            - Score 2: The answer contains significant inaccuracies, though it shows some understanding of the topic. Key\\n              elements of the question are addressed incorrectly.\\n            - Score 3: The answer is partially correct but has noticeable inaccuracies or omissions. It addresses the\\n              question but lacks depth or precision.\\n            - Score 4: The answer is mostly correct, with only minor inaccuracies or omissions. It provides a generally\\n              accurate response to the question.\\n            - Score 5: The answer is completely correct and thorough. It demonstrates a deep and accurate understanding of\\n              the topic, addressing all elements of the question effectively.', 'examples': '\\n            Question: What is the capital of France?\\n            Score 1: Completely Incorrect\\n            Answer: \"The capital of France is Berlin.\"\\n            Explanation: This answer is entirely incorrect and irrelevant, as Berlin is the capital of Germany, not France.\\n            Score 2: Significantly Inaccurate\\n            Answer: \"The capital of France is Lyon.\"\\n            Explanation: This answer demonstrates some understanding that the question is about a city in France, but it incorrectly identifies Lyon as the capital instead of Paris.\\n            Score 3: Partially Correct\\n            Answer: \"I think the capital of France is either Paris or Marseille.\"\\n            Explanation: This answer shows partial knowledge but includes a significant inaccuracy by suggesting Marseille might be the capital. Paris is correct, but the inclusion of Marseille indicates a lack of certainty or complete understanding.\\n            Score 4: Mostly Correct\\n            Answer: \"The capital of France is Paris, the largest city in the country.\"\\n            Explanation: This answer is mostly correct and identifies Paris as the capital. The addition of \"the largest city in the country\" is accurate but not directly relevant to the capital status, introducing a slight deviation from the question\\'s focus.\\n            Score 5: Completely Correct and Thorough\\n            Answer: \"The capital of France is Paris, which is not only the country\\'s largest city but also its cultural and political center, hosting major institutions like the President\\'s residence, the ElysÃ©e Palace.\"\\n            Explanation: This answer is completely correct, providing a thorough explanation that adds relevant context about Paris\\'s role as the cultural and political center of France, directly addressing the question with depth and precision.\\n                     '}</div><div class=\"dictlist\">tokenizer_judge_config={'use_fast': True}</div></td>\n",
       "      <td></td>\n",
       "      <td><div title=\"/User/functions_old/llm_judge/llm-judge-llm-judge/0/pairwise_result.parquet\">pairwise_result</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result8ca6afec-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result8ca6afec-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result8ca6afec\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result8ca6afec-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods  or <a href=\"https://dashboard.default-tenant.app.llm5.iguazio-cd1.com/mlprojects/llm-judge-pengwei/jobs/monitor/8e8d2c31987542878218f68a40068efc/overview\" target=\"_blank\">click here</a> to open in UI</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:15:51,502 [info] Run execution finished: {'status': 'completed', 'name': 'llm-judge-llm-judge'}\n"
     ]
    }
   ],
   "source": [
    "pairwise_grading_run = llm_judge_fn.run(\n",
    "    handler=\"llm_judge\",\n",
    "    params={\n",
    "        \"input_path\": \"data/qa.csv\",\n",
    "        \"metric_type\": \"LLMJudgePairwiseGrading\",\n",
    "        \"name\": \"accuracy_metrics\",\n",
    "        \"model_judge\": JUDGE_MODEL,\n",
    "        \"model_judge_config\": JUDGE_CONFIG,\n",
    "        \"model_judge_infer_config\": JUDGE_INFER_CONFIG,\n",
    "        \"model_bench_mark\":BENCHMARK_MODEL,\n",
    "        \"model_bench_mark_config\": BENCHMARK_CONFIG,\n",
    "        \"model_bench_mark_infer_config\": BENCHMARK_INFER_CONFIG,\n",
    "        \"tokenizer_bench_mark_config\": TOKENIZER_BENCHMARK_CONFIG,\n",
    "        \"prompt_config\" :prompt_config,\n",
    "        \"tokenizer_judge_config\": TOKENIZER_JUDGE_CONFIG,\n",
    "    },\n",
    "    returns=[\n",
    "        \"pairwise_result: dataset\",\n",
    "    ],\n",
    "    local=True,\n",
    "    artifact_path=\"./\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1275fce-1b3e-4da1-9be8-7f0153301008",
   "metadata": {},
   "source": [
    "### OPENAI model as Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "309c80f3-b297-4b37-85ad-59fc88853666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:15:51,514 [warning] artifact/output path is not defined or is local and relative, artifacts will not be visible in the UI: {'output_path': './'}\n",
      "> 2024-02-21 00:15:51,514 [info] Storing function: {'name': 'llm-judge-llm-judge', 'uid': '294038f7ddfb4a0b93b913ef44a82bc1', 'db': None}\n",
      "> 2024-02-21 00:15:51,757 [info] Prepare the openAI model as judge\n",
      "> 2024-02-21 00:15:51,782 [info] Preparing the bench mark model microsoft/phi-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25a24dd641748efbf0f22229efbcd74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:15:54,867 [info] Computing the metrics over What is the capital of China? and The capital of China is Kongfu\n",
      "> 2024-02-21 00:15:54,868 [info] Computing the bench mark response for What is the capital of China?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:15:55,972 [info] Response of the bench mark model is What is the capital of China?\n",
      "A) Beijing\n",
      "B) Shanghai\n",
      "C) Hong Kong\n",
      "D) Tokyo\n",
      "\n",
      "Answer: A) Beijing\n",
      "\n",
      "> 2024-02-21 00:15:55,972 [info] Filling the prompt template with the prompt config\n",
      "> 2024-02-21 00:15:58,090 [info] Extract the score and the explanation from the - Score of Assistant A: 1\n",
      "- Explanation of Assistant A: The response \"The capital of China is Kongfu\" is completely incorrect and irrelevant to the question. It demonstrates a fundamental misunderstanding of the topic, as Kongfu is not the capital of China, leading to a score of 1.\n",
      "\n",
      "- Score of Assistant B: 4\n",
      "- Explanation of Assistant B: The response \"A) Beijing\" is mostly correct with only minor inaccuracies. It correctly identifies Beijing as the capital of China, which is the accurate answer. While other options are provided, the correct answer is included, leading to a score of 4.\n",
      "> 2024-02-21 00:15:58,092 [info] Computing the metrics over What is the capital of France? and The capital of France is Paris\n",
      "> 2024-02-21 00:15:58,093 [info] Computing the bench mark response for What is the capital of France?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:15:59,111 [info] Response of the bench mark model is What is the capital of France?\n",
      "A) London\n",
      "B) Paris\n",
      "C) Rome\n",
      "D) Berlin\n",
      "Answer: B) Paris\n",
      "\n",
      "> 2024-02-21 00:15:59,111 [info] Filling the prompt template with the prompt config\n",
      "> 2024-02-21 00:16:02,294 [info] Extract the score and the explanation from the - Score of assistant A: 5\n",
      "- Explanation of assistant A: The response from assistant A, \"The capital of France is Paris,\" is completely correct and directly answers the question without any inaccuracies or omissions. This demonstrates a deep and accurate understanding of the topic and earns a score of 5 according to the grading rubric.\n",
      "\n",
      "- Score of assistant B: 2\n",
      "- Explanation of assistant B: The response from assistant B, \"B) Paris,\" is somewhat accurate as it correctly identifies Paris as the capital of France. However, the answer is presented in a multiple-choice format which may imply uncertainty or lack of depth in understanding. Additionally, the other options mentioned (A) London, C) Rome, D) Berlin) are clearly incorrect and irrelevant, suggesting a lack of complete understanding. Therefore, it contains significant inaccuracies in terms of how the answer is presented, resulting in a score of 2.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>llm-judge-pengwei</td>\n",
       "      <td><div title=\"294038f7ddfb4a0b93b913ef44a82bc1\"><a href=\"https://dashboard.default-tenant.app.llm5.iguazio-cd1.com/mlprojects/llm-judge-pengwei/jobs/monitor/294038f7ddfb4a0b93b913ef44a82bc1/overview\" target=\"_blank\" >...44a82bc1</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Feb 21 00:15:51</td>\n",
       "      <td>completed</td>\n",
       "      <td>llm-judge-llm-judge</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=pengwei</div><div class=\"dictlist\">kind=local</div><div class=\"dictlist\">owner=pengwei</div><div class=\"dictlist\">host=jupyter-pengwei-gpu-7777658756-jcs45</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">input_path=data/qa.csv</div><div class=\"dictlist\">metric_type=OPENAIJudgePairwiseGrading</div><div class=\"dictlist\">name=accuracy_metrics</div><div class=\"dictlist\">model_judge=gpt-3.5-turbo</div><div class=\"dictlist\">prompt_config={'name': 'accuracy', 'definition': 'The accuracy of the provided answer.', 'rubric': 'Accuracy: This rubric assesses the accuracy of the provided answer. The details for different scores are as follows:\\n            - Score 1: The answer is completely incorrect or irrelevant to the question. It demonstrates a fundamental\\n              misunderstanding of the topic or question.\\n            - Score 2: The answer contains significant inaccuracies, though it shows some understanding of the topic. Key\\n              elements of the question are addressed incorrectly.\\n            - Score 3: The answer is partially correct but has noticeable inaccuracies or omissions. It addresses the\\n              question but lacks depth or precision.\\n            - Score 4: The answer is mostly correct, with only minor inaccuracies or omissions. It provides a generally\\n              accurate response to the question.\\n            - Score 5: The answer is completely correct and thorough. It demonstrates a deep and accurate understanding of\\n              the topic, addressing all elements of the question effectively.', 'examples': '\\n            Question: What is the capital of France?\\n            Score 1: Completely Incorrect\\n            Answer: \"The capital of France is Berlin.\"\\n            Explanation: This answer is entirely incorrect and irrelevant, as Berlin is the capital of Germany, not France.\\n            Score 2: Significantly Inaccurate\\n            Answer: \"The capital of France is Lyon.\"\\n            Explanation: This answer demonstrates some understanding that the question is about a city in France, but it incorrectly identifies Lyon as the capital instead of Paris.\\n            Score 3: Partially Correct\\n            Answer: \"I think the capital of France is either Paris or Marseille.\"\\n            Explanation: This answer shows partial knowledge but includes a significant inaccuracy by suggesting Marseille might be the capital. Paris is correct, but the inclusion of Marseille indicates a lack of certainty or complete understanding.\\n            Score 4: Mostly Correct\\n            Answer: \"The capital of France is Paris, the largest city in the country.\"\\n            Explanation: This answer is mostly correct and identifies Paris as the capital. The addition of \"the largest city in the country\" is accurate but not directly relevant to the capital status, introducing a slight deviation from the question\\'s focus.\\n            Score 5: Completely Correct and Thorough\\n            Answer: \"The capital of France is Paris, which is not only the country\\'s largest city but also its cultural and political center, hosting major institutions like the President\\'s residence, the ElysÃ©e Palace.\"\\n            Explanation: This answer is completely correct, providing a thorough explanation that adds relevant context about Paris\\'s role as the cultural and political center of France, directly addressing the question with depth and precision.\\n                     '}</div><div class=\"dictlist\">model_bench_mark=microsoft/phi-2</div><div class=\"dictlist\">model_bench_mark_config={'max_length': 1500, 'device_map': 'auto', 'revision': 'main', 'trust_remote_code': True, 'torch_dtype': 'auto'}</div><div class=\"dictlist\">model_bench_mark_infer_config={'max_length': 1500}</div><div class=\"dictlist\">tokenizer_bench_mark_config={'trust_remote_code': True}</div></td>\n",
       "      <td></td>\n",
       "      <td><div title=\"/User/functions_old/llm_judge/llm-judge-llm-judge/0/openai_pairwise_result.parquet\">openai_pairwise_result</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result99f209a2-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result99f209a2-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result99f209a2\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result99f209a2-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods  or <a href=\"https://dashboard.default-tenant.app.llm5.iguazio-cd1.com/mlprojects/llm-judge-pengwei/jobs/monitor/294038f7ddfb4a0b93b913ef44a82bc1/overview\" target=\"_blank\">click here</a> to open in UI</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:16:02,488 [info] Run execution finished: {'status': 'completed', 'name': 'llm-judge-llm-judge'}\n"
     ]
    }
   ],
   "source": [
    "openai_pairwise_grading_run = llm_judge_fn.run(\n",
    "    handler=\"llm_judge\",\n",
    "    params={\n",
    "        \"input_path\": \"data/qa.csv\",\n",
    "        \"metric_type\": \"OPENAIJudgePairwiseGrading\",\n",
    "        \"name\": \"accuracy_metrics\",\n",
    "        \"model_judge\": OPENAI_MODEL,\n",
    "        \"prompt_config\" :prompt_config,\n",
    "        \"model_bench_mark\":BENCHMARK_MODEL,\n",
    "        \"model_bench_mark_config\": BENCHMARK_CONFIG,\n",
    "        \"model_bench_mark_infer_config\": BENCHMARK_INFER_CONFIG,\n",
    "        \"tokenizer_bench_mark_config\": TOKENIZER_BENCHMARK_CONFIG,\n",
    "    },\n",
    "    returns=[\n",
    "        \"openai_pairwise_result: dataset\",\n",
    "    ],\n",
    "    local=True,\n",
    "    artifact_path=\"./\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184c5186-86f0-4741-946d-017bf2aed7f2",
   "metadata": {},
   "source": [
    "<a id=\"chapter3\"></a>\n",
    "## 3. Pairwise grading metrics with reference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a59059-7861-4232-aacf-11839f6e4e60",
   "metadata": {},
   "source": [
    "This type of metrics will use a benchmark model and the ground truth of the question to give the grading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9f4501-1a93-4adb-840e-f1e74160bb81",
   "metadata": {},
   "source": [
    "### HF model as Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5c555df-afe6-4962-9552-eb6fbc146698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:16:02,500 [warning] artifact/output path is not defined or is local and relative, artifacts will not be visible in the UI: {'output_path': './'}\n",
      "> 2024-02-21 00:16:02,501 [info] Storing function: {'name': 'llm-judge-llm-judge', 'uid': 'c4aee4056c6d45e5ab2dfc7362ffdb31', 'db': None}\n",
      "> 2024-02-21 00:16:02,717 [info] Preparing the judge model TheBloke/Mistral-7B-OpenOrca-GPTQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:17:03,088 [info] Preparing the bench mark model microsoft/phi-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a0b472c1f84b7e8b4c67635a36628a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:17:45,346 [info] Computing the metrics over What is the capital of China? and The capital of China is Kongfu\n",
      "> 2024-02-21 00:17:45,347 [info] Computing the bench mark response for What is the capital of China?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:17:46,529 [info] Response of the bench mark model is What is the capital of China?\n",
      "A) Beijing\n",
      "B) Shanghai\n",
      "C) Hong Kong\n",
      "D) Tokyo\n",
      "\n",
      "Answer: A) Beijing\n",
      "\n",
      "> 2024-02-21 00:17:46,530 [info] Filling the prompt template with the prompt config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:19:26,011 [info] Response of the judge model is \n",
      "Task:\n",
      "Your task is to determine two numerical score of accuracy for the responses from two AI assistants with the ground truth of the response. You must use the grading rubric to determine your scores. You must use the ground truth of the response. You need to give a explanation about how did you compare with the ground truth of the response to determine the scores step-by-step. Please using chain of thinking.\n",
      "Examples could be included beblow for your reference. Make sure you understand the grading rubric and use the examples before completing the task.\n",
      "[Examples]:\n",
      "\n",
      "            Question: What is the capital of France?\n",
      "            Score 1: Completely Incorrect\n",
      "            Answer: \"The capital of France is Berlin.\"\n",
      "            Explanation: This answer is entirely incorrect and irrelevant, as Berlin is the capital of Germany, not France.\n",
      "            Score 2: Significantly Inaccurate\n",
      "            Answer: \"The capital of France is Lyon.\"\n",
      "            Explanation: This answer demonstrates some understanding that the question is about a city in France, but it incorrectly identifies Lyon as the capital instead of Paris.\n",
      "            Score 3: Partially Correct\n",
      "            Answer: \"I think the capital of France is either Paris or Marseille.\"\n",
      "            Explanation: This answer shows partial knowledge but includes a significant inaccuracy by suggesting Marseille might be the capital. Paris is correct, but the inclusion of Marseille indicates a lack of certainty or complete understanding.\n",
      "            Score 4: Mostly Correct\n",
      "            Answer: \"The capital of France is Paris, the largest city in the country.\"\n",
      "            Explanation: This answer is mostly correct and identifies Paris as the capital. The addition of \"the largest city in the country\" is accurate but not directly relevant to the capital status, introducing a slight deviation from the question's focus.\n",
      "            Score 5: Completely Correct and Thorough\n",
      "            Answer: \"The capital of France is Paris, which is not only the country's largest city but also its cultural and political center, hosting major institutions like the President's residence, the ElysÃ©e Palace.\"\n",
      "            Explanation: This answer is completely correct, providing a thorough explanation that adds relevant context about Paris's role as the cultural and political center of France, directly addressing the question with depth and precision.\n",
      "                     \n",
      "[User Question]:\n",
      "What is the capital of China?\n",
      "[Response of assistant A]:\n",
      "The capital of China is Kongfu\n",
      "[Response of assistant B]:\n",
      "What is the capital of China?\n",
      "A) Beijing\n",
      "B) Shanghai\n",
      "C) Hong Kong\n",
      "D) Tokyo\n",
      "\n",
      "Answer: A) Beijing\n",
      "\n",
      "[Ground truth of the response]:\n",
      "Beijing\n",
      "[Definition of accuracy]:\n",
      "The accuracy of the provided answer.\n",
      "[Grading Rubric]:\n",
      "Accuracy: This rubric assesses the accuracy of the provided answer. The details for different scores are as follows:\n",
      "            - Score 1: The answer is completely incorrect or irrelevant to the question. It demonstrates a fundamental\n",
      "              misunderstanding of the topic or question.\n",
      "            - Score 2: The answer contains significant inaccuracies, though it shows some understanding of the topic. Key\n",
      "              elements of the question are addressed incorrectly.\n",
      "            - Score 3: The answer is partially correct but has noticeable inaccuracies or omissions. It addresses the\n",
      "              question but lacks depth or precision.\n",
      "            - Score 4: The answer is mostly correct, with only minor inaccuracies or omissions. It provides a generally\n",
      "              accurate response to the question.\n",
      "            - Score 5: The answer is completely correct and thorough. It demonstrates a deep and accurate understanding of\n",
      "              the topic, addressing all elements of the question effectively.\n",
      "You must return the following fields in your output:\n",
      "- score of assistant a: a numerical score of accuracy for the response\n",
      "- explanation of assistant a: a explanation about how did you compare with the ground truth of the response to determine the score step-by-step\n",
      "- score of assistant b: a numerical score of accuracy for the response\n",
      "- explanation of assistant b: a explanation about how did you compare with the ground truth of the response to determine the score step-by-step\n",
      "[Output]:\n",
      "- score of assistant a: 1\n",
      "- explanation of assistant a: The answer is completely incorrect and irrelevant to the question. It demonstrates a fundamental misunderstanding of the topic or question.\n",
      "- score of assistant b: 5\n",
      "- explanation of assistant b: The answer is completely correct and thorough. It demonstrates a deep and accurate understanding of the topic, addressing all elements of the question effectively.\n",
      "> 2024-02-21 00:19:26,015 [info] Computing the metrics over What is the capital of France? and The capital of France is Seattle\n",
      "> 2024-02-21 00:19:26,015 [info] Computing the bench mark response for What is the capital of France?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:19:27,032 [info] Response of the bench mark model is What is the capital of France?\n",
      "A) London\n",
      "B) Paris\n",
      "C) Rome\n",
      "D) Berlin\n",
      "Answer: B) Paris\n",
      "\n",
      "> 2024-02-21 00:19:27,032 [info] Filling the prompt template with the prompt config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:21:06,516 [info] Response of the judge model is \n",
      "Task:\n",
      "Your task is to determine two numerical score of accuracy for the responses from two AI assistants with the ground truth of the response. You must use the grading rubric to determine your scores. You must use the ground truth of the response. You need to give a explanation about how did you compare with the ground truth of the response to determine the scores step-by-step. Please using chain of thinking.\n",
      "Examples could be included beblow for your reference. Make sure you understand the grading rubric and use the examples before completing the task.\n",
      "[Examples]:\n",
      "\n",
      "            Question: What is the capital of France?\n",
      "            Score 1: Completely Incorrect\n",
      "            Answer: \"The capital of France is Berlin.\"\n",
      "            Explanation: This answer is entirely incorrect and irrelevant, as Berlin is the capital of Germany, not France.\n",
      "            Score 2: Significantly Inaccurate\n",
      "            Answer: \"The capital of France is Lyon.\"\n",
      "            Explanation: This answer demonstrates some understanding that the question is about a city in France, but it incorrectly identifies Lyon as the capital instead of Paris.\n",
      "            Score 3: Partially Correct\n",
      "            Answer: \"I think the capital of France is either Paris or Marseille.\"\n",
      "            Explanation: This answer shows partial knowledge but includes a significant inaccuracy by suggesting Marseille might be the capital. Paris is correct, but the inclusion of Marseille indicates a lack of certainty or complete understanding.\n",
      "            Score 4: Mostly Correct\n",
      "            Answer: \"The capital of France is Paris, the largest city in the country.\"\n",
      "            Explanation: This answer is mostly correct and identifies Paris as the capital. The addition of \"the largest city in the country\" is accurate but not directly relevant to the capital status, introducing a slight deviation from the question's focus.\n",
      "            Score 5: Completely Correct and Thorough\n",
      "            Answer: \"The capital of France is Paris, which is not only the country's largest city but also its cultural and political center, hosting major institutions like the President's residence, the ElysÃ©e Palace.\"\n",
      "            Explanation: This answer is completely correct, providing a thorough explanation that adds relevant context about Paris's role as the cultural and political center of France, directly addressing the question with depth and precision.\n",
      "                     \n",
      "[User Question]:\n",
      "What is the capital of France?\n",
      "[Response of assistant A]:\n",
      "The capital of France is Seattle\n",
      "[Response of assistant B]:\n",
      "What is the capital of France?\n",
      "A) London\n",
      "B) Paris\n",
      "C) Rome\n",
      "D) Berlin\n",
      "Answer: B) Paris\n",
      "\n",
      "[Ground truth of the response]:\n",
      "Paris\n",
      "[Definition of accuracy]:\n",
      "The accuracy of the provided answer.\n",
      "[Grading Rubric]:\n",
      "Accuracy: This rubric assesses the accuracy of the provided answer. The details for different scores are as follows:\n",
      "            - Score 1: The answer is completely incorrect or irrelevant to the question. It demonstrates a fundamental\n",
      "              misunderstanding of the topic or question.\n",
      "            - Score 2: The answer contains significant inaccuracies, though it shows some understanding of the topic. Key\n",
      "              elements of the question are addressed incorrectly.\n",
      "            - Score 3: The answer is partially correct but has noticeable inaccuracies or omissions. It addresses the\n",
      "              question but lacks depth or precision.\n",
      "            - Score 4: The answer is mostly correct, with only minor inaccuracies or omissions. It provides a generally\n",
      "              accurate response to the question.\n",
      "            - Score 5: The answer is completely correct and thorough. It demonstrates a deep and accurate understanding of\n",
      "              the topic, addressing all elements of the question effectively.\n",
      "You must return the following fields in your output:\n",
      "- score of assistant a: a numerical score of accuracy for the response\n",
      "- explanation of assistant a: a explanation about how did you compare with the ground truth of the response to determine the score step-by-step\n",
      "- score of assistant b: a numerical score of accuracy for the response\n",
      "- explanation of assistant b: a explanation about how did you compare with the ground truth of the response to determine the score step-by-step\n",
      "[Output]:\n",
      "- score of assistant a: 1\n",
      "- explanation of assistant a: The answer is completely incorrect and irrelevant to the question. It demonstrates a fundamental misunderstanding of the topic or question.\n",
      "- score of assistant b: 5\n",
      "- explanation of assistant b: The answer is completely correct and thorough. It demonstrates a deep and accurate understanding of the topic, addressing all elements of the question effectively.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>llm-judge-pengwei</td>\n",
       "      <td><div title=\"c4aee4056c6d45e5ab2dfc7362ffdb31\"><a href=\"https://dashboard.default-tenant.app.llm5.iguazio-cd1.com/mlprojects/llm-judge-pengwei/jobs/monitor/c4aee4056c6d45e5ab2dfc7362ffdb31/overview\" target=\"_blank\" >...62ffdb31</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Feb 21 00:16:02</td>\n",
       "      <td>completed</td>\n",
       "      <td>llm-judge-llm-judge</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=pengwei</div><div class=\"dictlist\">kind=local</div><div class=\"dictlist\">owner=pengwei</div><div class=\"dictlist\">host=jupyter-pengwei-gpu-7777658756-jcs45</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">input_path=data/ref.csv</div><div class=\"dictlist\">metric_type=LLMJudgeReferenceGrading</div><div class=\"dictlist\">name=accuracy_metrics</div><div class=\"dictlist\">model_judge=TheBloke/Mistral-7B-OpenOrca-GPTQ</div><div class=\"dictlist\">model_judge_config={'device_map': 'auto', 'revision': 'gptq-8bit-128g-actorder_True', 'trust_remote_code': False}</div><div class=\"dictlist\">model_judge_infer_config={'max_length': 1500}</div><div class=\"dictlist\">model_bench_mark=microsoft/phi-2</div><div class=\"dictlist\">model_bench_mark_config={'max_length': 1500, 'device_map': 'auto', 'revision': 'main', 'trust_remote_code': True, 'torch_dtype': 'auto'}</div><div class=\"dictlist\">model_bench_mark_infer_config={'max_length': 1500}</div><div class=\"dictlist\">tokenizer_bench_mark_config={'trust_remote_code': True}</div><div class=\"dictlist\">prompt_config={'name': 'accuracy', 'definition': 'The accuracy of the provided answer.', 'rubric': 'Accuracy: This rubric assesses the accuracy of the provided answer. The details for different scores are as follows:\\n            - Score 1: The answer is completely incorrect or irrelevant to the question. It demonstrates a fundamental\\n              misunderstanding of the topic or question.\\n            - Score 2: The answer contains significant inaccuracies, though it shows some understanding of the topic. Key\\n              elements of the question are addressed incorrectly.\\n            - Score 3: The answer is partially correct but has noticeable inaccuracies or omissions. It addresses the\\n              question but lacks depth or precision.\\n            - Score 4: The answer is mostly correct, with only minor inaccuracies or omissions. It provides a generally\\n              accurate response to the question.\\n            - Score 5: The answer is completely correct and thorough. It demonstrates a deep and accurate understanding of\\n              the topic, addressing all elements of the question effectively.', 'examples': '\\n            Question: What is the capital of France?\\n            Score 1: Completely Incorrect\\n            Answer: \"The capital of France is Berlin.\"\\n            Explanation: This answer is entirely incorrect and irrelevant, as Berlin is the capital of Germany, not France.\\n            Score 2: Significantly Inaccurate\\n            Answer: \"The capital of France is Lyon.\"\\n            Explanation: This answer demonstrates some understanding that the question is about a city in France, but it incorrectly identifies Lyon as the capital instead of Paris.\\n            Score 3: Partially Correct\\n            Answer: \"I think the capital of France is either Paris or Marseille.\"\\n            Explanation: This answer shows partial knowledge but includes a significant inaccuracy by suggesting Marseille might be the capital. Paris is correct, but the inclusion of Marseille indicates a lack of certainty or complete understanding.\\n            Score 4: Mostly Correct\\n            Answer: \"The capital of France is Paris, the largest city in the country.\"\\n            Explanation: This answer is mostly correct and identifies Paris as the capital. The addition of \"the largest city in the country\" is accurate but not directly relevant to the capital status, introducing a slight deviation from the question\\'s focus.\\n            Score 5: Completely Correct and Thorough\\n            Answer: \"The capital of France is Paris, which is not only the country\\'s largest city but also its cultural and political center, hosting major institutions like the President\\'s residence, the ElysÃ©e Palace.\"\\n            Explanation: This answer is completely correct, providing a thorough explanation that adds relevant context about Paris\\'s role as the cultural and political center of France, directly addressing the question with depth and precision.\\n                     '}</div><div class=\"dictlist\">tokenizer_judge_config={'use_fast': True}</div></td>\n",
       "      <td></td>\n",
       "      <td><div title=\"/User/functions_old/llm_judge/llm-judge-llm-judge/0/reference_result.parquet\">reference_result</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"resulte88fe55b-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"resulte88fe55b-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"resulte88fe55b\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"resulte88fe55b-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods  or <a href=\"https://dashboard.default-tenant.app.llm5.iguazio-cd1.com/mlprojects/llm-judge-pengwei/jobs/monitor/c4aee4056c6d45e5ab2dfc7362ffdb31/overview\" target=\"_blank\">click here</a> to open in UI</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:21:06,753 [info] Run execution finished: {'status': 'completed', 'name': 'llm-judge-llm-judge'}\n"
     ]
    }
   ],
   "source": [
    "pairwise_grading_run = llm_judge_fn.run(\n",
    "    handler=\"llm_judge\",\n",
    "    params={\n",
    "        \"input_path\": \"data/ref.csv\",\n",
    "        \"metric_type\": \"LLMJudgeReferenceGrading\",\n",
    "        \"name\": \"accuracy_metrics\",\n",
    "        \"model_judge\": JUDGE_MODEL,\n",
    "        \"model_judge_config\": JUDGE_CONFIG,\n",
    "        \"model_judge_infer_config\": JUDGE_INFER_CONFIG,\n",
    "        \"model_bench_mark\":BENCHMARK_MODEL,\n",
    "        \"model_bench_mark_config\": BENCHMARK_CONFIG,\n",
    "        \"model_bench_mark_infer_config\": BENCHMARK_INFER_CONFIG,\n",
    "        \"tokenizer_bench_mark_config\": TOKENIZER_BENCHMARK_CONFIG,\n",
    "        \"prompt_config\" :prompt_config,\n",
    "        \"tokenizer_judge_config\": TOKENIZER_JUDGE_CONFIG,\n",
    "    },\n",
    "    returns=[\n",
    "        \"reference_result: dataset\",\n",
    "    ],\n",
    "    local=True,\n",
    "    artifact_path=\"./\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5dd9e8-630b-4d6c-beec-20082550b60a",
   "metadata": {},
   "source": [
    "### OPENAI model as Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28031255-3f0b-4419-b921-62de1ad5f731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:21:06,765 [warning] artifact/output path is not defined or is local and relative, artifacts will not be visible in the UI: {'output_path': './'}\n",
      "> 2024-02-21 00:21:06,765 [info] Storing function: {'name': 'llm-judge-llm-judge', 'uid': '1f506415dfa3473484e270faeacdc498', 'db': None}\n",
      "> 2024-02-21 00:21:06,977 [info] Prepare the openAI model as judge\n",
      "> 2024-02-21 00:21:07,003 [info] Preparing the bench mark model microsoft/phi-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34683203adb41fbaf719bbbdd5ee08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:21:10,182 [info] Computing the metrics over What is the capital of China? and The capital of China is Kongfu\n",
      "> 2024-02-21 00:21:10,182 [info] Computing the bench mark response for What is the capital of China?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:21:11,299 [info] Response of the bench mark model is What is the capital of China?\n",
      "A) Beijing\n",
      "B) Shanghai\n",
      "C) Hong Kong\n",
      "D) Tokyo\n",
      "\n",
      "Answer: A) Beijing\n",
      "\n",
      "> 2024-02-21 00:21:11,300 [info] Filling the prompt template with the prompt config\n",
      "> 2024-02-21 00:21:13,910 [info] Extract the score and the explanation from the - score of assistant a: 1\n",
      "- explanation of assistant a: The response from assistant A is completely incorrect as the capital of China is Beijing, not Kongfu. This answer demonstrates a fundamental misunderstanding of the question.\n",
      "- score of assistant b: 5\n",
      "- explanation of assistant b: The response from assistant B is completely correct as it identifies Beijing as the capital of China, which aligns perfectly with the ground truth. This answer shows a deep and accurate understanding of the topic.\n",
      "> 2024-02-21 00:21:13,912 [info] Computing the metrics over What is the capital of France? and The capital of France is Seattle\n",
      "> 2024-02-21 00:21:13,912 [info] Computing the bench mark response for What is the capital of France?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:21:14,948 [info] Response of the bench mark model is What is the capital of France?\n",
      "A) London\n",
      "B) Paris\n",
      "C) Rome\n",
      "D) Berlin\n",
      "Answer: B) Paris\n",
      "\n",
      "> 2024-02-21 00:21:14,948 [info] Filling the prompt template with the prompt config\n",
      "> 2024-02-21 00:21:17,446 [info] Extract the score and the explanation from the - score of assistant a: 1\n",
      "- explanation of assistant a: The response from assistant A incorrectly states that the capital of France is Seattle, which is completely irrelevant and incorrect. This answer demonstrates a fundamental misunderstanding of the topic, resulting in a score of 1.\n",
      "\n",
      "- score of assistant b: 4\n",
      "- explanation of assistant b: The response from assistant B correctly identifies Paris as the capital of France, which aligns with the ground truth. The multiple-choice options provided help confirm that option B (Paris) is the correct answer. While there are other cities listed, the response correctly selects Paris, resulting in a score of 4 based on the grading rubric.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>llm-judge-pengwei</td>\n",
       "      <td><div title=\"1f506415dfa3473484e270faeacdc498\"><a href=\"https://dashboard.default-tenant.app.llm5.iguazio-cd1.com/mlprojects/llm-judge-pengwei/jobs/monitor/1f506415dfa3473484e270faeacdc498/overview\" target=\"_blank\" >...eacdc498</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Feb 21 00:21:06</td>\n",
       "      <td>completed</td>\n",
       "      <td>llm-judge-llm-judge</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=pengwei</div><div class=\"dictlist\">kind=local</div><div class=\"dictlist\">owner=pengwei</div><div class=\"dictlist\">host=jupyter-pengwei-gpu-7777658756-jcs45</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">input_path=data/ref.csv</div><div class=\"dictlist\">metric_type=OPENAIJudgeReferenceGrading</div><div class=\"dictlist\">name=accuracy_metrics</div><div class=\"dictlist\">model_judge=gpt-3.5-turbo</div><div class=\"dictlist\">prompt_config={'name': 'accuracy', 'definition': 'The accuracy of the provided answer.', 'rubric': 'Accuracy: This rubric assesses the accuracy of the provided answer. The details for different scores are as follows:\\n            - Score 1: The answer is completely incorrect or irrelevant to the question. It demonstrates a fundamental\\n              misunderstanding of the topic or question.\\n            - Score 2: The answer contains significant inaccuracies, though it shows some understanding of the topic. Key\\n              elements of the question are addressed incorrectly.\\n            - Score 3: The answer is partially correct but has noticeable inaccuracies or omissions. It addresses the\\n              question but lacks depth or precision.\\n            - Score 4: The answer is mostly correct, with only minor inaccuracies or omissions. It provides a generally\\n              accurate response to the question.\\n            - Score 5: The answer is completely correct and thorough. It demonstrates a deep and accurate understanding of\\n              the topic, addressing all elements of the question effectively.', 'examples': '\\n            Question: What is the capital of France?\\n            Score 1: Completely Incorrect\\n            Answer: \"The capital of France is Berlin.\"\\n            Explanation: This answer is entirely incorrect and irrelevant, as Berlin is the capital of Germany, not France.\\n            Score 2: Significantly Inaccurate\\n            Answer: \"The capital of France is Lyon.\"\\n            Explanation: This answer demonstrates some understanding that the question is about a city in France, but it incorrectly identifies Lyon as the capital instead of Paris.\\n            Score 3: Partially Correct\\n            Answer: \"I think the capital of France is either Paris or Marseille.\"\\n            Explanation: This answer shows partial knowledge but includes a significant inaccuracy by suggesting Marseille might be the capital. Paris is correct, but the inclusion of Marseille indicates a lack of certainty or complete understanding.\\n            Score 4: Mostly Correct\\n            Answer: \"The capital of France is Paris, the largest city in the country.\"\\n            Explanation: This answer is mostly correct and identifies Paris as the capital. The addition of \"the largest city in the country\" is accurate but not directly relevant to the capital status, introducing a slight deviation from the question\\'s focus.\\n            Score 5: Completely Correct and Thorough\\n            Answer: \"The capital of France is Paris, which is not only the country\\'s largest city but also its cultural and political center, hosting major institutions like the President\\'s residence, the ElysÃ©e Palace.\"\\n            Explanation: This answer is completely correct, providing a thorough explanation that adds relevant context about Paris\\'s role as the cultural and political center of France, directly addressing the question with depth and precision.\\n                     '}</div><div class=\"dictlist\">model_bench_mark=microsoft/phi-2</div><div class=\"dictlist\">model_bench_mark_config={'max_length': 1500, 'device_map': 'auto', 'revision': 'main', 'trust_remote_code': True, 'torch_dtype': 'auto'}</div><div class=\"dictlist\">model_bench_mark_infer_config={'max_length': 1500}</div><div class=\"dictlist\">tokenizer_bench_mark_config={'trust_remote_code': True}</div></td>\n",
       "      <td></td>\n",
       "      <td><div title=\"/User/functions_old/llm_judge/llm-judge-llm-judge/0/openai_reference_result.parquet\">openai_reference_result</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"resultc7da6c24-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"resultc7da6c24-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"resultc7da6c24\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"resultc7da6c24-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods  or <a href=\"https://dashboard.default-tenant.app.llm5.iguazio-cd1.com/mlprojects/llm-judge-pengwei/jobs/monitor/1f506415dfa3473484e270faeacdc498/overview\" target=\"_blank\">click here</a> to open in UI</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-02-21 00:21:17,650 [info] Run execution finished: {'status': 'completed', 'name': 'llm-judge-llm-judge'}\n"
     ]
    }
   ],
   "source": [
    "openai_ref_grading_run = llm_judge_fn.run(\n",
    "    handler=\"llm_judge\",\n",
    "    params={\n",
    "        \"input_path\": \"data/ref.csv\",\n",
    "        \"metric_type\": \"OPENAIJudgeReferenceGrading\",\n",
    "        \"name\": \"accuracy_metrics\",\n",
    "        \"model_judge\": OPENAI_MODEL,\n",
    "        \"prompt_config\" :prompt_config,\n",
    "        \"model_bench_mark\":BENCHMARK_MODEL,\n",
    "        \"model_bench_mark_config\": BENCHMARK_CONFIG,\n",
    "        \"model_bench_mark_infer_config\": BENCHMARK_INFER_CONFIG,\n",
    "        \"tokenizer_bench_mark_config\": TOKENIZER_BENCHMARK_CONFIG,\n",
    "    },\n",
    "    returns=[\n",
    "        \"openai_reference_result: dataset\",\n",
    "    ],\n",
    "    local=True,\n",
    "    artifact_path=\"./\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd2d46e-9f4e-433d-b0f7-1d107e5d003b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_judge",
   "language": "python",
   "name": "conda-env-.conda-llm_judge-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
