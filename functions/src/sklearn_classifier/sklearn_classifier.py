# Copyright 2019 Iguazio
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Generated by nuclio.export.NuclioExporter

import warnings

warnings.simplefilter(action="ignore", category=FutureWarning)


from cloudpickle import dumps
import pandas as pd
import numpy as np
from typing import List, Tuple
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from mlrun.execution import MLClientCtx
from mlrun.datastore import DataItem
from mlrun.utils.helpers import create_class


def get_sample(dataset: DataItem, sample: int, label_column: str) -> Tuple[pd.DataFrame, pd.Series, list]:
    """Get a sample of the dataset with labels separated.

    :param dataset: DataItem containing the dataset
    :param sample: Number of samples to take. If -1, use all. If < -1, take random sample.
    :param label_column: Name of the label column
    """
    df = dataset.as_df()

    if sample == -1:
        sampled_df = df
    elif sample < -1:
        sampled_df = df.sample(n=abs(sample), random_state=1)
    else:
        sampled_df = df.head(sample)

    labels = sampled_df[label_column]
    features = sampled_df.drop(label_column, axis=1)
    header = list(features.columns)

    return features, labels, header


def get_splits(
    features: pd.DataFrame,
    labels: pd.Series,
    num_splits: int,
    test_size: float,
    val_size: float,
    random_state: int = 1
) -> List[Tuple[pd.DataFrame, pd.Series]]:
    """Split data into train, validation, and test sets.

    :param features: Feature DataFrame
    :param labels: Labels Series
    :param num_splits: Number of splits (3 for train/val/test)
    :param test_size: Proportion for test set
    :param val_size: Proportion of remaining data for validation
    :param random_state: Random seed
    """
    # First split: separate test set
    X_temp, X_test, y_temp, y_test = train_test_split(
        features, labels, test_size=test_size, random_state=random_state
    )

    # Second split: separate train and validation from remaining data
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=val_size, random_state=random_state
    )

    return [(X_train, y_train), (X_val, y_val), (X_test, y_test)]


def gen_sklearn_model(model_pkg_class: str, parameters: list) -> dict:
    """Generate sklearn model configuration from class name and parameters.

    :param model_pkg_class: Full class path (e.g., "sklearn.ensemble.RandomForestClassifier")
    :param parameters: List of (key, value) parameter tuples
    """
    config = {
        "META": {"class": model_pkg_class},
        "CLASS": {},
        "FIT": {}
    }

    # Parameters that should not be passed to sklearn model
    excluded_params = {
        'model_pkg_class', 'dataset', 'label_column', 'encode_cols',
        'sample', 'test_size', 'train_val_split', 'test_set_key',
        'model_evaluator', 'models_dest', 'plots_dest', 'file_ext',
        'model_pkg_file', 'context'
    }

    # Separate parameters into model init params and fit params
    for key, value in parameters:
        if key in ['X', 'y', 'sample_weight']:
            config["FIT"][key] = value
        elif key not in excluded_params:
            # Only add parameters that are not function-specific
            config["CLASS"][key] = value

    return config


def eval_model_v2(
    context: MLClientCtx,
    xvalid: pd.DataFrame,
    yvalid: pd.Series,
    model,
    plots_artifact_path: str = None
) -> dict:
    """Evaluate a sklearn classifier model.

    :param context: MLRun context
    :param xvalid: Validation features
    :param yvalid: Validation labels
    :param model: Trained sklearn model
    :param plots_artifact_path: Path for plots (not used in this simplified version)
    """
    y_pred = model.predict(xvalid)

    metrics = {
        "accuracy": accuracy_score(yvalid, y_pred),
        "precision": precision_score(yvalid, y_pred, average='weighted', zero_division=0),
        "recall": recall_score(yvalid, y_pred, average='weighted', zero_division=0),
        "f1_score": f1_score(yvalid, y_pred, average='weighted', zero_division=0)
    }

    # Log metrics to context
    for key, value in metrics.items():
        context.log_result(key, value)

    return {}


def train_model(
    context: MLClientCtx,
    model_pkg_class: str,
    dataset: DataItem,
    label_column: str = "labels",
    encode_cols: List[str] = [],
    sample: int = -1,
    test_size: float = 0.30,
    train_val_split: float = 0.70,
    test_set_key: str = "test_set",
    model_evaluator=None,
    models_dest: str = "",
    plots_dest: str = "plots",
    file_ext: str = "parquet",
    model_pkg_file: str = "",
    random_state: int = 1,
) -> None:
    """train a classifier

    An optional cutom model evaluator can be supplied that should have the signature:
    `my_custom_evaluator(context, xvalid, yvalid, model)` and return a dictionary of
    scalar "results", a "plots" keys with a list of PlotArtifacts, and
    and "tables" key containing a returned list of TableArtifacts.

    :param context:           the function context
    :param model_pkg_class:   the model to train, e.g, "sklearn.neural_networks.MLPClassifier",
                              or json model config
    :param dataset:           ("data") name of raw data file
    :param label_column:      ground-truth (y) labels
    :param encode_cols:       dictionary of names and prefixes for columns that are
                              to hot be encoded.
    :param sample:            Selects the first n rows, or select a sample
                              starting from the first. If negative <-1, select
                              a random sample
    :param test_size:         (0.05) test set size
    :param train_val_split:   (0.75) Once the test set has been removed the
                              training set gets this proportion.
    :param test_set_key:      key of held out data in artifact store
    :param model_evaluator:   (None) a custom model evaluator can be specified
    :param models_dest:       ("") models subfolder on artifact path
    :param plots_dest:        plot subfolder on artifact path
    :param file_ext:          ("parquet") format for test_set_key hold out data
    :param random_state:      (1) sklearn rng seed

    """
    models_dest = models_dest or "model"

    raw, labels, header = get_sample(dataset, sample, label_column)

    if encode_cols:
        raw = pd.get_dummies(
            raw,
            columns=list(encode_cols.keys()),
            prefix=list(encode_cols.values()),
            drop_first=True,
        )

    (xtrain, ytrain), (xvalid, yvalid), (xtest, ytest) = get_splits(
        raw, labels, 3, test_size, 1 - train_val_split, random_state
    )

    test_set = pd.concat([xtest, ytest.to_frame()], axis=1)
    context.log_dataset(
        test_set_key,
        df=test_set,
        format=file_ext,
        index=False,
        labels={"data-type": "held-out"},
        artifact_path=context.artifact_subpath("data"),
    )

    model_config = gen_sklearn_model(model_pkg_class, context.parameters.items())

    model_config["FIT"].update({"X": xtrain, "y": ytrain.values})

    ClassifierClass = create_class(model_config["META"]["class"])

    model = ClassifierClass(**model_config["CLASS"])

    model.fit(**model_config["FIT"])

    artifact_path = context.artifact_subpath(models_dest)
    plots_path = context.artifact_subpath(models_dest, plots_dest)
    if model_evaluator:
        eval_metrics = model_evaluator(
            context, xvalid, yvalid, model, plots_artifact_path=plots_path
        )
    else:
        eval_metrics = eval_model_v2(
            context, xvalid, yvalid, model, plots_artifact_path=plots_path
        )

    kwargs = {"training_set": test_set, "label_column": label_column}
    split = model_pkg_class.rsplit(".", 1)
    if split and len(split) == 2:
        kwargs["algorithm"] = split[1]

    if dataset.meta and dataset.meta.kind == "FeatureVector":
        kwargs["feature_vector"] = dataset.meta.uri

    context.set_label("class", model_pkg_class)
    context.log_model(
        "model",
        body=dumps(model),
        artifact_path=artifact_path,
        extra_data=eval_metrics,
        model_file="model.pkl",
        metrics=context.results,
        labels={"class": model_pkg_class},
        framework="sklearn",
        **kwargs
    )
