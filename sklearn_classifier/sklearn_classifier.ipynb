{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from cloudpickle import dump, load\n",
    "\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from typing import List\n",
    "from mlrun.execution import MLClientCtx\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun.artifacts import PlotArtifact\n",
    "\n",
    "from mlutils.models import get_model_configs, create_class\n",
    "from mlutils.plots import plot_roc, plot_importance, gcf_clear\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "def train_model(\n",
    "    context: MLClientCtx,\n",
    "    model_pkg_class: str = \"\",\n",
    "    data_key: str = \"data\",\n",
    "    sample: int = -1,\n",
    "    label_column: str = \"labels\",\n",
    "    model_key: str = \"model\",\n",
    "    test_size: float = 0.05,\n",
    "    train_val_split: float = 0.75,\n",
    "    test_set_key: str = \"test_set\",\n",
    "    rng: int = 1,\n",
    "    models_dest: str = \"models\",\n",
    "    plots_dest: str = \"plots\",\n",
    "    score_method: str = \"micro\",\n",
    "    model_pkg_file: str = \"\",\n",
    "    file_ext: str = \"parquet\"\n",
    ") -> None:\n",
    "    \"\"\"train a classifier.\n",
    "\n",
    "    :param context:           the function context\n",
    "    :param model_pkg_class:   the model to train, e.g, \"sklearn.neural_networks.MLPClassifier\", \n",
    "                              or json model config\n",
    "    :param data_key:          (\"data\") name of raw data file\n",
    "    :param sample:            Selects the first n rows, or select a sample\n",
    "                              starting from the first. If negative <-1, select\n",
    "                              a random sample\n",
    "    :param label_column:      ground-truth (y) labels\n",
    "    :param model_key:         (\"model\") name of model in artifact store,\n",
    "                              points to a directory\n",
    "    :param test_size:         (0.05) test set size\n",
    "    :param train_val_split:   (0.75) Once the test set has been removed the\n",
    "                              training set gets this proportion.\n",
    "    :param test_set_key:      store the test data set under this key in the\n",
    "                              artifact store\n",
    "    :param rng:               (1) sklearn rng seed\n",
    "    :param models_dest:       models subfolder on artifact path\n",
    "    :param plots_dest:        plot subfolder on artifact path\n",
    "    :param score_method:      for multiclass classification\n",
    "    :param model_pkg_file:    json model config file\n",
    "    :param file_ext:          format for test_set_key hold out data\n",
    "    \"\"\"\n",
    "    # extract file name from DataItem\n",
    "    srcfilepath = str(data_key)\n",
    "    \n",
    "    # TODO: this should be part of data\"s metadata dealt with in another step get a data set, sample, etc...\n",
    "    # get all data or a sample\n",
    "    if (sample == -1) or (sample >= 1):\n",
    "        # get all rows, or contiguous sample starting at row 1.\n",
    "        if srcfilepath.endswith(\".csv\"):\n",
    "            raw = pd.read_csv(srcfilepath).dropna()\n",
    "        if srcfilepath.endswith(\"parquet\") or srcfilepath.endswith(\"pq\"):\n",
    "            raw = pd.read_parquet(srcfilepath).dropna()\n",
    "        else:\n",
    "            raise Exception(\"file type unhandled\")\n",
    "        labels = raw.pop(label_column)\n",
    "        raw = raw.iloc[:sample, :]\n",
    "        labels = labels.iloc[:sample]\n",
    "    else:\n",
    "        # grab a random sample\n",
    "        raw = pq.read_table(srcfilepath).to_pandas().dropna().sample(sample * -1)\n",
    "        labels = raw.pop(label_column)\n",
    "\n",
    "    # TODO: this should be part of data\"s metadata dealt with in another step\n",
    "    context.header = raw.columns.values\n",
    "    \n",
    "    # TODO: all of this should be part of a spitter component that does cv too, dealt with in another step\n",
    "    # make a hot encode copy of labels before the split\n",
    "    yb = label_binarize(labels, classes=labels.unique()) # if binary 0/1 labels, will return labels as is\n",
    "    \n",
    "    # double split to generate 3 data sets: train, validation and test\n",
    "    # with xtest,ytest set aside\n",
    "    # here we hide the binary encoded labels inside the X matrix so that when splitting we preserve order in both the encoded\n",
    "    # and non-encoded labels:\n",
    "    x, xtest, y, ytest = train_test_split(np.concatenate([raw, yb], axis=1), labels, test_size=test_size, random_state=rng)\n",
    "    xtrain, xvalid, ytrain, yvalid = train_test_split(x, y, train_size=train_val_split, random_state=rng)\n",
    "    # now extract the hot_encoded labels\n",
    "    ytrainb = xtrain[:, -yb.shape[1]:].copy()\n",
    "    xtrain = xtrain[:, :-yb.shape[1]].copy()\n",
    "    # extract the hot_encoded labels\n",
    "    yvalidb = xvalid[:, -yb.shape[1]:].copy()\n",
    "    xvalid = xvalid[:, :-yb.shape[1]].copy()\n",
    "    # extract the hot_encoded labels\n",
    "    ytestb = xtest[:, -yb.shape[1]:].copy()\n",
    "    xtest = xtest[:, :-yb.shape[1]].copy()                                      \n",
    "    \n",
    "    # set-aside test_set\n",
    "    test_set = pd.concat(\n",
    "        [pd.DataFrame(data=xtest, columns=context.header),\n",
    "         pd.DataFrame(data=ytest.values, columns=[label_column])],\n",
    "        axis=1,)\n",
    "    context.log_dataset(test_set_key, df=test_set, format=file_ext, index=False)\n",
    "\n",
    "    if model_pkg_file:\n",
    "        try:\n",
    "            model_config = json.loads(model_pkg_file.get())\n",
    "        except:\n",
    "            model_config = json.loads(open(model_pkg_file, \"r\"))\n",
    "    elif model_pkg_class:\n",
    "        model_config = get_model_configs(model_pkg_class)\n",
    "    else:\n",
    "        raise ValueError('model_pkg_file or model_pkg_class must be provided')\n",
    "    \n",
    "    for k, v in context.parameters.items():\n",
    "        if k.startswith('CLASS_'):\n",
    "            model_config['CLASS'][k[6:]] = v\n",
    "        if k.startswith('FIT_'):\n",
    "            model_config['FIT'][k[4:]] = v\n",
    "\n",
    "    model_config[\"FIT\"].update({\"X\": xtrain,\"y\": ytrain.values})\n",
    "    \n",
    "    # create class and fit\n",
    "    ClassifierClass = create_class(model_config[\"META\"][\"class\"])\n",
    "    model = ClassifierClass(**model_config[\"CLASS\"])\n",
    "    model.fit(**model_config[\"FIT\"])\n",
    "\n",
    "    # save model\n",
    "    filepath = os.path.join(context.artifact_path, f\"{models_dest}/{model_key}.pkl\")\n",
    "    os.makedirs(os.path.join(context.artifact_path, models_dest), exist_ok=True)\n",
    "    try:\n",
    "        dump(model, open(filepath, \"wb\"))\n",
    "        context.log_artifact(model_key, local_path=models_dest)\n",
    "    except Exception as e:\n",
    "        print(\"SERIALIZE MODEL ERROR:\", str(e))\n",
    "\n",
    "    # compute validation metrics\n",
    "    ypred = model.predict(xvalid)\n",
    "    y_score = model.predict_proba(xvalid)\n",
    "    context.logger.info(f\"y_score.shape {y_score.shape}\")\n",
    "    context.logger.info(f\"yvalidb.shape {yvalidb.shape}\")\n",
    "    if yvalidb.shape[1] > 1:\n",
    "        # label encoding was applied:\n",
    "        average_precision = metrics.average_precision_score(yvalidb,\n",
    "                                                            y_score,\n",
    "                                                            average=score_method)\n",
    "        context.log_result(f\"rocauc\", metrics.roc_auc_score(yvalidb, y_score))\n",
    "    else:\n",
    "        average_precision = metrics.average_precision_score(yvalidb,\n",
    "                                                            y_score[:, 1],\n",
    "                                                            average=score_method)\n",
    "        context.log_result(f\"rocauc\", metrics.roc_auc_score(yvalidb, y_score[:, 1]))\n",
    "        \n",
    "    context.log_result(f\"avg_precscore\", average_precision)\n",
    "    context.log_result(f\"accuracy\", float(model.score(xvalid, yvalid)))\n",
    "    context.log_result(f\"f1_score\", metrics.f1_score(yvalid, ypred,\n",
    "                                             average=score_method))\n",
    "\n",
    "    # TODO: missing validation plots, callbacks need to reintroduced\n",
    "    \n",
    "    plot_roc(context, yvalidb, y_score)\n",
    "    gcf_clear(plt)\n",
    "    # use sklearn >= v0.22 built in:\n",
    "    metrics.plot_confusion_matrix(model, xvalid, yvalid, labels=labels.unique(), normalize='true') \n",
    "    context.log_artifact(PlotArtifact(\"confusion\", body=plt.gcf()), local_path=f\"{plots_dest}/confusion.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function \n",
    "# create job function object from notebook code\n",
    "fn = code_to_function(\"sklearn_classifier\", kind=\"job\", with_doc=True,\n",
    "                      handler=train_model, image=\"mlrun/ml-models:0.4.6\")\n",
    "\n",
    "# add metadata (for templates and reuse)\n",
    "fn.spec.default_handler = \"train_model\"\n",
    "fn.spec.description = \"train any classifier using scikit-learn's API\"\n",
    "fn.metadata.categories = [\"models\", \"classifier\"]\n",
    "fn.spec.image_pull_policy = \"Always\"\n",
    "fn.metadata.labels = {\"author\": \"yjb\"}\n",
    "\n",
    "fn.save()\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import import_function, mount_v3io\n",
    "\n",
    "func = import_function(\"hub://sklearn_classifier\").apply(mount_v3io())\n",
    "# func = import_function(\"function.yaml\").apply(mlrun.mount_v3io())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_params = {\n",
    "    \"name\" : \"tasks train a classifier\",\n",
    "    \"params\" : {\n",
    "        \n",
    "        # CHOOSE YOUR MODEL AND CHNAGE SOME DEFAULT PARAMETERS\n",
    "        \"model_pkg_class\"    : \"sklearn.linear_model.LogisticRegression\",\n",
    "        \"CLASS_random_state\" : 1,\n",
    "        \"CLASS_solver\"       : \"liblinear\",\n",
    "    \n",
    "        # POINT THIS TO YOUR DATA\n",
    "        #\"data_key\"        : \"/User/artifacts/iris.parquet\",\n",
    "        #\"data_key\"        : \"/User/artifacts/wine.parquet\",\n",
    "        \"data_key\"        : \"/User/artifacts/breast_cancer.parquet\",\n",
    "        \"sample\"          : -1,\n",
    "        \"label_column\"    : \"labels\",\n",
    "        \"test_size\"       : 0.10,\n",
    "        \"train_val_split\" : 0.75,\n",
    "        \"rng\"             : 1}}\n",
    "    \n",
    "\n",
    "from mlrun import NewTask\n",
    "run = func.run(NewTask(**task_params), artifact_path=\"/User/artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_params = {\n",
    "    \"name\" : \"tasks train a classifier\",\n",
    "    \"params\" : {\n",
    "        \n",
    "        # CHOOSE YOUR MODEL AND CHNAGE SOME DEFAULT PARAMETERS\n",
    "        \"model_pkg_file\"     : \"sample-configs/XGBCLassifier.json\",\n",
    "        \"CLASS_random_state\" : 1,\n",
    "        \"CLASS_num_class\"    : 2,\n",
    "    \n",
    "        # POINT THIS TO YOUR DATA\n",
    "        \"data_key\"        : \"/User/artifacts/iris.parquet\",\n",
    "        #\"data_key\"        : \"/User/artifacts/wine.parquet\",\n",
    "        #\"data_key\"        : \"/User/artifacts/breast_cancer.parquet\",\n",
    "        \"sample\"          : -1,\n",
    "        \"label_column\"    : \"labels\",\n",
    "        \"test_size\"       : 0.10,\n",
    "        \"train_val_split\" : 0.75,\n",
    "        \"rng\"             : 1}}\n",
    "    \n",
    "\n",
    "from mlrun import NewTask\n",
    "run = func.run(NewTask(**task_params), artifact_path=\"/User/artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-v0.4.6",
   "language": "python",
   "name": "mlrun-v0.4.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
