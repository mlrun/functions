{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX Utils\n",
    "\n",
    "A collection of ONNX utils in one MLRun function. The function includes the following handlers:\n",
    "\n",
    "1. [to_onnx](#handler1) - Convert your model into `onnx` format.\n",
    "2. [optimize](#handler2) - Perform ONNX optimizations using `onnxmodeloptimizer` on a given ONNX model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"handler1\"></a>\n",
    "\n",
    "## 1. to_onnx\n",
    "\n",
    "### 1.1. Docs\n",
    "Convert the given model to an ONNX model.\n",
    "\n",
    "#### Parameters:\n",
    "* **`context`**: `mlrun.MLClientCtx` - The MLRun function execution context\n",
    "* **`model_path`**: `str` - The model path store object.\n",
    "* **`onnx_model_name`**: `str = None` - The name to use to log the converted ONNX model. If not given, the given `model_name` will be used with an additional suffix `_onnx`. Defaulted to None.\n",
    "* **`optimize_model`**: `bool = True` - Whether to optimize the ONNX model using 'onnxoptimizer' before saving the model. Defaulted to True.\n",
    "* **`framework`**: `str = None` - The model's framework. If None, it will be read from the 'framework' label of the model artifact provided. Defaulted to None.\n",
    "* **`framework_kwargs`**: `Dict[str, Any] = None` - Additional arguments each framework may require in order to convert to ONNX. *To get the doc string of the desired framework onnx conversion function, **pass \"help\"**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Supported keyword arguments (`framework_kwargs`) per framework:\n",
    "`tensorflow.keras`:\n",
    "* **`input_signature`**: `List[Tuple[Tuple[int], str]] = None` - A list of the input layers shape and data type properties. Expected to receive a list where each element is an input layer tuple. An input layer tuple is a tuple of:\n",
    "  * [0] = Layer's shape, a tuple of integers.\n",
    "  * [1] = Layer's data type, a mlrun.data_types.ValueType string.\n",
    "\n",
    "  If None, the input signature will be tried to be read automatically before converting to ONNX or from the model artifact if available. Defaulted to None.\n",
    "\n",
    "`torch`:\n",
    "* **`input_signature`**: `List[Tuple[Tuple[int], str]] = None` - A list of the input layers shape and data type properties. Expected to receive a list where each element is an input layer tuple. An input layer tuple is a tuple of:\n",
    "  * [0] = Layer's shape, a tuple of integers.\n",
    "  * [1] = Layer's data type, a mlrun.data_types.ValueType string.\n",
    "\n",
    "  If None, the input signature will be read from the model artifact if available. Defaulted to None.\n",
    "* **`input_layers_names`**: `List[str] = None` - List of names to assign to the input nodes of the graph in order. All of the other parameters (inner layers) can be set as well by passing additional names in the list. The order is by the order of the parameters in the model. If None, the inputs will be read from the handler's inputs. If its also None, it is defaulted to: \"input_0\", \"input_1\", ...\n",
    "* **`output_layers_names`**: `List[str] = None` - List of names to assign to the output nodes of the graph in order. If None, the outputs will be read from the handler's outputs. If its also None, it is defaulted to: \"output_0\" (for multiple outputs, this parameter must be provided).\n",
    "* **`param dynamic_axes`**: `Dict[str, Dict[int, str]] = None` - If part of the input / output shape is dynamic, like (batch_size, 3, 32, 32) you can specify it by giving a dynamic axis to the input / output layer by its name as follows:\n",
    "```python\n",
    "{\n",
    "    \"input layer name\": {0: \"batch_size\"},\n",
    "    \"output layer name\": {0: \"batch_size\"},\n",
    "}\n",
    "```\n",
    "If provided, the 'is_batched' flag will be ignored. Defaulted to None.\n",
    "* **`is_batched`**: `bool = True` - Whether to include a batch size as the first axis in every input and output layer. Defaulted to True. Will be ignored if 'dynamic_axes' is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2. Demo\n",
    "\n",
    "We will use the `TF.Keras` framework, a `MobileNetV2` as our model and we will convert it to ONNX using the `to_onnx` handler.\n",
    "\n",
    "1.2.1. First we will set a temporary artifact path for our model to be saved in and choose the models names:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"true\"\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "# Create a temporary directory for the model artifact:\n",
    "ARTIFACT_PATH = TemporaryDirectory().name\n",
    "os.makedirs(ARTIFACT_PATH)\n",
    "\n",
    "# Choose our model's name:\n",
    "MODEL_NAME = \"mobilenetv2\"\n",
    "\n",
    "# Choose our ONNX version model's name:\n",
    "ONNX_MODEL_NAME = \"onnx_mobilenetv2\"\n",
    "\n",
    "# Choose our optimized ONNX version model's name:\n",
    "OPTIMIZED_ONNX_MODEL_NAME = \"optimized_onnx_mobilenetv2\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.2.2. Download the model from `keras.applications` and log it with MLRun's `TFKerasModelHandler`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# mlrun: start-code"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "import mlrun\n",
    "import mlrun.frameworks.tf_keras as mlrun_tf_keras\n",
    "\n",
    "\n",
    "def get_model(context: mlrun.MLClientCtx, model_name: str):\n",
    "    # Download the MobileNetV2 model:\n",
    "    model = keras.applications.mobilenet_v2.MobileNetV2()\n",
    "\n",
    "    # Initialize a model handler for logging the model:\n",
    "    model_handler = mlrun_tf_keras.TFKerasModelHandler(\n",
    "        model_name=model_name,\n",
    "        model=model,\n",
    "        context=context\n",
    "    )\n",
    "\n",
    "    # Log the model:\n",
    "    model_handler.log()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# mlrun: end-code"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.2.3. Create the function using MLRun's `code_to_function` and run it:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import mlrun\n",
    "\n",
    "\n",
    "# Create the function parsing this notebook's code using 'code_to_function':\n",
    "get_model_function = mlrun.code_to_function(\n",
    "    name=\"get_mobilenetv2\",\n",
    "    kind=\"job\",\n",
    "    image=\"mlrun/ml-models\"\n",
    ")\n",
    "\n",
    "# Run the function to log the model:\n",
    "get_model_run = get_model_function.run(\n",
    "    handler=\"get_model\",\n",
    "    artifact_path=ARTIFACT_PATH,\n",
    "    params={\n",
    "        \"model_name\": MODEL_NAME\n",
    "    },\n",
    "    local=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.4. Import the `onnx_utils` MLRun function and run it:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Import the ONNX function from the marketplace:\n",
    "onnx_utils_function = mlrun.import_function(\"hub://onnx_utils\")\n",
    "\n",
    "# Run the function to convert our model to ONNX:\n",
    "to_onnx_run = onnx_utils_function.run(\n",
    "    handler=\"to_onnx\",\n",
    "    artifact_path=ARTIFACT_PATH,\n",
    "    params={\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"model_path\": get_model_run.outputs[MODEL_NAME],  # <- Take the logged model from the previous function.\n",
    "        \"onnx_model_name\": ONNX_MODEL_NAME,\n",
    "        \"optimize_model\": False  # <- For optimizing it later in the demo, we mark the flag as False\n",
    "    },\n",
    "    local=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.5. Now, listing the artifact directory we will see both our `tf.keras` model and the `onnx` model:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "\n",
    "print(os.listdir(ARTIFACT_PATH))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"handler2\"></a>\n",
    "\n",
    "## 2. optimize\n",
    "\n",
    "### 2.1. Docs\n",
    "Optimize the given ONNX model.\n",
    "\n",
    "#### Parameters:\n",
    "* **`context`**: `mlrun.MLClientCtx` - The MLRun function execution context\n",
    "* **`model_path`**: `str` - The model path store object.\n",
    "* **`optimizations`**: `List[str] = None` - List of possible optimizations. *To see what optimizations are available, **pass \"help\"**.* If None, all of the optimizations will be used. Defaulted to None.\n",
    "* **`fixed_point`**: `bool = False` - Optimize the weights using fixed point. Defaulted to False.\n",
    "* **`optimized_model_name`**: `str = None` - The name of the optimized model. If None, the original model will be overridden. Defaulted to None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Demo\n",
    "\n",
    "We will use our converted model from the last example and optimize it.\n",
    "\n",
    "2.2.1. We will call now the `optimize` handler:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "onnx_utils_function.run(\n",
    "    handler=\"optimize\",\n",
    "    artifact_path=ARTIFACT_PATH,\n",
    "    params={\n",
    "        \"model_name\": ONNX_MODEL_NAME,\n",
    "        \"model_path\": to_onnx_run.output(ONNX_MODEL_NAME),  # <- Take the logged model from the previous function.\n",
    "        \"optimized_model_name\": OPTIMIZED_ONNX_MODEL_NAME,\n",
    "    },\n",
    "    local=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2. And now our model was optimized and can be seen under the `ARTIFACT_PATH`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "print(os.listdir(ARTIFACT_PATH))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lastly, run this code to clean up the models:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "shutil.rmtree(ARTIFACT_PATH)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
