{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio config kind = \"job\"\n",
    "%nuclio config spec.image = \"mlrun/ml-models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 Iguazio\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlrun.execution import MLClientCtx\n",
    "\n",
    "\n",
    "def load_dataset(\n",
    "        context: MLClientCtx,\n",
    "        dataset: str,\n",
    "        name: str = '',\n",
    "        file_ext: str = 'parquet',\n",
    "        params: dict = {}\n",
    ") -> None:\n",
    "    \"\"\"Loads a scikit-learn toy dataset for classification or regression\n",
    "\n",
    "    The following datasets are available ('name' : desription):\n",
    "\n",
    "        'boston'          : boston house-prices dataset (regression)\n",
    "        'iris'            : iris dataset (classification)\n",
    "        'diabetes'        : diabetes dataset (regression)\n",
    "        'digits'          : digits dataset (classification)\n",
    "        'linnerud'        : linnerud dataset (multivariate regression)\n",
    "        'wine'            : wine dataset (classification)\n",
    "        'breast_cancer'   : breast cancer wisconsin dataset (classification)\n",
    "\n",
    "    The scikit-learn functions return a data bunch including the following items:\n",
    "    - data              the features matrix\n",
    "    - target            the ground truth labels\n",
    "    - DESCR             a description of the dataset\n",
    "    - feature_names     header for data\n",
    "\n",
    "    The features (and their names) are stored with the target labels in a DataFrame.\n",
    "\n",
    "    For further details see https://scikit-learn.org/stable/datasets/index.html#toy-datasets\n",
    "\n",
    "    :param context:    function execution context\n",
    "    :param dataset:    name of the dataset to load\n",
    "    :param name:       artifact name (defaults to dataset)\n",
    "    :param file_ext:   output file_ext: parquet or csv\n",
    "    :param params:     params of the sklearn load_data method\n",
    "    \"\"\"\n",
    "    dataset = str(dataset)\n",
    "    # reach into module and import the appropriate load_xxx function\n",
    "    pkg_module = 'sklearn.datasets'\n",
    "    fname = f'load_{dataset}'\n",
    "\n",
    "    pkg_module = __import__(pkg_module, fromlist=[fname])\n",
    "    load_data_fn = getattr(pkg_module, fname)\n",
    "\n",
    "    data = load_data_fn(**params)\n",
    "    feature_names = data['feature_names']\n",
    "\n",
    "    # create the toy dataset\n",
    "    xy = np.concatenate([data['data'], data['target'].reshape(-1, 1)], axis=1)\n",
    "    if hasattr(feature_names, 'append'):\n",
    "        # its a list\n",
    "        feature_names.append('labels')\n",
    "    else:\n",
    "        # its an array\n",
    "        feature_names = np.append(feature_names, 'labels')\n",
    "    df = pd.DataFrame(data=xy, columns=feature_names)\n",
    "\n",
    "    # log and upload the dataset\n",
    "    context.log_dataset(name or dataset, df=df, format=file_ext, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import mlconf\n",
    "import os\n",
    "\n",
    "mlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'\n",
    "mlconf.artifact_path = mlconf.artifact_path or f'{os.environ[\"HOME\"]}/artifacts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function \n",
    "# create job function object from notebook code\n",
    "fn = code_to_function(\"load_dataset\")\n",
    "\n",
    "# add metadata (for templates and reuse)\n",
    "fn.spec.default_handler = \"load_dataset\"\n",
    "fn.spec.description = \"load a toy dataset from scikit-learn\"\n",
    "fn.metadata.categories = [\"data-source\", \"ml\"]\n",
    "fn.metadata.labels = {\"author\": \"yjb\", \"framework\": \"sklearn\"}\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load function from marketplacen\n",
    "from mlrun import import_function\n",
    "\n",
    "# vcs_branch = 'development'\n",
    "# base_vcs = f'https://raw.githubusercontent.com/mlrun/functions/{vcs_branch}/'\n",
    "# mlconf.hub_url = mlconf.hub_url or base_vcs + f'{name}/function.yaml'\n",
    "# fn = import_function(\"hub://load_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"V3IO_HOME\" in list(os.environ):\n",
    "    from mlrun import mount_v3io\n",
    "    fn.apply(mount_v3io())\n",
    "else:\n",
    "    # is you set up mlrun using the instructions at https://github.com/mlrun/mlrun/blob/master/hack/local/README.md\n",
    "    from mlrun.platforms import mount_pvc\n",
    "    fn.apply(mount_pvc('nfsvol', 'nfsvol', '/home/joyan/data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import NewTask    \n",
    "\n",
    "task_params = {\n",
    "    \"name\"   : \"tasks load toy dataset\", \n",
    "    \"params\" : {\"dataset\"     : \"wine\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = fn.run(NewTask(**task_params), artifact_path=mlconf.artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### or locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import run_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [\"wine\", \"iris\", \"breast_cancer\"]:\n",
    "    run_local(handler=load_dataset,\n",
    "              inputs={\"dataset\": dataset}, artifact_path=mlconf.artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
