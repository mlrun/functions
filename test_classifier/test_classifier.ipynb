{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import importlib\n",
    "from cloudpickle import load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "import seaborn as sns\n",
    "\n",
    "from mlrun.execution import MLClientCtx\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun.artifacts import TableArtifact, PlotArtifact\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "def _gcf_clear(plt):\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()        \n",
    "\n",
    "def test_classifier(\n",
    "    context: MLClientCtx,\n",
    "    models_dir: str, \n",
    "    test_set: str,\n",
    "    label_column: str,\n",
    "    score_method: str = 'micro',\n",
    "    key: str = \"\",\n",
    "    plots_dir: str = \"plots\"\n",
    ") -> None:\n",
    "    \"\"\"Test one or more classifier models against held-out dataset\n",
    "    \n",
    "    Using held-out test features, evaluates the peformance of the estimated model\n",
    "    \n",
    "    Can be part of a kubeflow pipeline as a test step that is run post EDA and \n",
    "    training/validation cycles\n",
    "    \n",
    "    :param context:         the function context\n",
    "    :param models_dir:      artifact models representing a folder or a folder\n",
    "    :param test_set:        test features and labels\n",
    "    :param label_column:    column name for ground truth labels\n",
    "    :param score_method:    for multiclass classification\n",
    "    :param key:             key for results artifact (maybe just a dir of artifacts for test like plots_dir)\n",
    "    :param plots_dir:       dir for test plots\n",
    "    \"\"\"\n",
    "    xtest = pd.read_parquet(str(test_set))\n",
    "    ytest = xtest.pop(label_column)\n",
    "    \n",
    "    context.header = list(xtest.columns.values)\n",
    "    \n",
    "    def _eval_model(model):\n",
    "        # enclose all except model\n",
    "        ytestb = label_binarize(ytest, classes=ytest.unique())\n",
    "        clf = load(open(os.path.join(str(models_dir), model), \"rb\"))\n",
    "        if callable(getattr(clf, \"predict_proba\")):\n",
    "            y_score = clf.predict_proba(xtest.values)\n",
    "            ypred = clf.predict(xtest.values)\n",
    "            context.logger.info(f\"y_score.shape {y_score.shape}\")\n",
    "            context.logger.info(f\"ytestb.shape {ytestb.shape}\")\n",
    "            plot_roc(context, ytestb, y_score, key=f\"roc\", plots_dir=plots_dir)\n",
    "        else:\n",
    "            ypred = clf.predict(xtest.values) # refactor\n",
    "            y_score = None\n",
    "        plot_confusion_matrix(context, ytest, ypred, key=\"confusion\", fmt=\"png\")\n",
    "        if hasattr(clf, \"feature_importances_\"):\n",
    "            plot_importance(context, clf, key=f\"featimp\")\n",
    "        average_precision = metrics.average_precision_score(ytestb[:,:-1], y_score, average=score_method)\n",
    "        context.log_result(f\"accuracy\", float(clf.score(xtest.values, ytest.values)))\n",
    "        context.log_result(f\"rocauc\", metrics.roc_auc_score(ytestb, y_score))\n",
    "        context.log_result(f\"f1_score\", metrics.f1_score(ytest.values, ypred, average=score_method))\n",
    "        context.log_result(f\"avg_precscore\", average_precision)\n",
    "    \n",
    "    best_model = None\n",
    "    for model in os.listdir(str(models_dir)):\n",
    "        if model.endswith('.pkl'):\n",
    "            _eval_model(model)\n",
    "            # HACK: there is only one model here\n",
    "            best_model = model\n",
    "\n",
    "    # log 'best model' as artifact\n",
    "    context.log_artifact('TODAYS-MODELS-TEST-REPORT', local_path=best_model)\n",
    "    context.log_artifact('DEPLOY', body=b'true', local_path='DEPLOY')\n",
    "    \n",
    "def plot_roc(\n",
    "    context,\n",
    "    y_labels,\n",
    "    y_probs,\n",
    "    key=\"roc\",\n",
    "    plots_dir: str = \"plots\",\n",
    "    fmt=\"png\",\n",
    "    fpr_label: str = \"false positive rate\",\n",
    "    tpr_label: str =  \"true positive rate\",\n",
    "    title: str = \"roc curve\",\n",
    "    legend_loc: str = \"best\"\n",
    "):\n",
    "    \"\"\"plot roc curves\n",
    "    \n",
    "    TODO:  add averaging method (as string) that was used to create probs, \n",
    "    display in legend\n",
    "    \n",
    "    :param context:      the function context\n",
    "    :param y_labels:     ground truth labels, hot encoded for multiclass  \n",
    "    :param y_probs:      model prediction probabilities\n",
    "    :param key:          (\"roc\") key of plot in artifact store\n",
    "    :param plots_dir:    (\"plots\") destination folder relative path to artifact path\n",
    "    :param fmt:          (\"png\") plot format\n",
    "    :param fpr_label:    (\"false positive rate\") x-axis labels\n",
    "    :param tpr_label:    (\"true positive rate\") y-axis labels\n",
    "    :param title:        (\"roc curve\") title of plot\n",
    "    :param legend_loc:   (\"best\") location of plot legend\n",
    "    \"\"\"\n",
    "       # clear matplotlib current figure\n",
    "    _gcf_clear(plt)\n",
    "    \n",
    "    # draw 45 degree line\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    \n",
    "    # labelling\n",
    "    plt.xlabel(fpr_label)\n",
    "    plt.ylabel(tpr_label)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=legend_loc)\n",
    "    \n",
    "    # single ROC or mutliple\n",
    "    if y_labels.shape[1] > 1:\n",
    "        # data accummulators by class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(y_labels[:,:-1].shape[1]):\n",
    "            fpr[i], tpr[i], _ = metrics.roc_curve(y_labels[:, i], y_probs[:, i], pos_label=1)\n",
    "            roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "            plt.plot(fpr[i], tpr[i], label=f\"class {i}\")\n",
    "    else:\n",
    "        fpr, tpr, _ = metrics.roc_curve(y_labels, y_probs[:, 1], pos_label=1)\n",
    "        plt.plot(fpr, tpr, label=f\"positive class\")\n",
    "        \n",
    "    fname = f\"{plots_dir}/{key}.{fmt}\"\n",
    "    plt.savefig(os.path.join(context.artifact_path, fname))\n",
    "    context.log_artifact(PlotArtifact(key, body=plt.gcf()), local_path=fname)\n",
    "    \n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    context: MLClientCtx,\n",
    "    labels,\n",
    "    predictions,\n",
    "    key: str = \"confusion_matrix\",\n",
    "    plots_dir: str = \"plots\",\n",
    "    colormap: str = \"Blues\",\n",
    "    fiel_ext: str = \"html\",\n",
    "    sample_weight=None\n",
    "):\n",
    "    \"\"\"Create a confusion matrix.\n",
    "    Plot and save a confusion matrix using test data from a\n",
    "    modelline step.\n",
    "    \n",
    "    See https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "    \n",
    "    TODO: fix label alignment\n",
    "    TODO: consider using another packaged version\n",
    "    TODO: refactor to take params dict for plot options\n",
    "\n",
    "    :param context:         function context\n",
    "    :param labels:          validation data ground-truth labels\n",
    "    :param predictions:     validation data predictions\n",
    "    :param key:             str\n",
    "    :param plots_dir:       relative path of plots in artifact store\n",
    "    :param colormap:        colourmap for confusion matrix\n",
    "    :param fmt:             plot format\n",
    "    :param sample_weight:   sample weights\n",
    "    \"\"\"\n",
    "    _gcf_clear(plt)\n",
    "    \n",
    "    cm = metrics.confusion_matrix(labels, predictions, sample_weight=None)\n",
    "    sns.heatmap(cm, annot=True, cmap=colormap, square=True)\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fname = f\"{plots_dir}/{key}.{fmt}\"\n",
    "    context.log_artifact(PlotArtifact(key, body=fig), local_path=fname)\n",
    "\n",
    "def plot_importance(\n",
    "    context,\n",
    "    model,\n",
    "    key: str = \"feature-importances\",\n",
    "    file_ext = \"html\"\n",
    "):\n",
    "    \"\"\"Display estimated feature importances.\n",
    "    :param context:     function context\n",
    "    :param model:       fitted lightgbm model\n",
    "    \"\"\"\n",
    "    _gcf_clear(plt)\n",
    "    \n",
    "    # create a feature importance table with desired labels\n",
    "    zipped = zip(model.feature_importances_, context.header)\n",
    "\n",
    "    feature_imp = pd.DataFrame(sorted(zipped), columns=[\"freq\",\"feature\"]\n",
    "                                ).sort_values(by=\"freq\", ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.barplot(x=\"freq\", y=\"feature\", data=feature_imp)\n",
    "    plt.title(\"features\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    context.log_artifact(PlotArtifact(f\"{key}.{fmt}\", body=fig), local_path=f\"plots/{key}.{fmt}\")\n",
    "\n",
    "    # feature importances are also saved as a table:\n",
    "    feature_imp.to_csv(os.path.join(context.artifact_path, key+\".csv\"))\n",
    "    \n",
    "    context.log_artifact(key+\".csv\", local_path=key+\".csv\")\n",
    "\n",
    "def _plot_confusion_matrix(y_true, y_pred, \n",
    "                           classes=[\"neg\", \"pos\"], \n",
    "                           normalize=True,\n",
    "                           title=None,\n",
    "                           cmap=plt.cm.Blues):\n",
    "    \"\"\"This can be deprecated once intel python upgrades scikit-learn to >0.22\n",
    "    \n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \n",
    "    https://scikit-learn.org/0.21/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = \"Normalized confusion matrix\"\n",
    "        else:\n",
    "            title = \"Confusion matrix, without normalization\"\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel=\"True label\",\n",
    "           xlabel=\"Predicted label\")\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function \n",
    "# create job function object from notebook code\n",
    "fn = code_to_function(\"test_classifier\", kind=\"job\", with_doc=True,\n",
    "                      handler=test_classifier, image=\"mlrun/ml-models\")\n",
    "\n",
    "# add metadata (for templates and reuse)\n",
    "fn.spec.default_handler = \"test_classifier\"\n",
    "fn.spec.description = \"test a classifier using held-out or new data\"\n",
    "fn.metadata.categories = [\"models\", \"testing\"]\n",
    "fn.spec.image_pull_policy = \"Always\"\n",
    "fn.metadata.labels = {\"author\": \"yjb\"}\n",
    "\n",
    "fn.save()\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import import_function, mount_v3io\n",
    "\n",
    "func = import_function(\"hub://test_classifier\").apply(mount_v3io())\n",
    "# func = import_function(\"function.yaml\").apply(mlrun.mount_v3io())\n",
    "\n",
    "task_params = {\n",
    "    \"name\" : \"tasks - test classifier\"\n",
    "    \"params\": {\n",
    "        # Ina pipeline setting, the models_dir parameter would be the output of a training step\n",
    "        \"models_dir\"    : \"/User/artifacts/models\",\n",
    "        \"test_set\"      : \"/User/artifacts/test_set.parquet\",\n",
    "        \"label_column\"  : \"labels\"}}\n",
    "\n",
    "from mlrun import NewTask\n",
    "run = func.run(NewTask(**task_params), artifact_path=\"/User/artifacts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-v0.4.6",
   "language": "python",
   "name": "mlrun-v0.4.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
