{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7955da79-02cc-42fe-aee0-5456d3e386fd",
   "metadata": {},
   "source": [
    "# LangChain ✕ MLRun Integration\n",
    "\n",
    "`langchain_mlrun` is a hub module that implements LangChain integration with MLRun. Using the module allows MLRun to orchestrate LangChain and LangGraph code, enabling tracing and monitoring batch workflows and realtime deployments.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8392a3e1-d0a1-409a-ae68-fcc36858d30a",
   "metadata": {},
   "source": [
    "## Main Components\n",
    "\n",
    "This is a short brief of the components available to import from the `langchain_mlrun` module. For full docs, see the documentation page.\n",
    "\n",
    "### Settings\n",
    "\n",
    "The module uses Pydantic settings classes that can be configured programmatically or via environment variables. The main class is `MLRunTracerSettings`. It contains two sub-settings:\n",
    "* `MLRunTracerClientSettings` - Connection settings (stream path, container, endpoint info). Env prefix: `\"LC_MLRUN_TRACER_CLIENT_\"`\n",
    "* `MLRunTracerMonitorSettings` - Controls what/how runs are captured (filters, labels, debug mode). Env prefix: `\"LC_MLRUN_TRACER_MONITOR_\"`\n",
    "\n",
    "For more information about each setting, see the class docstrings.\n",
    "\n",
    "#### Example - via code configuration\n",
    "\n",
    "```python\n",
    "from langchain_mlrun import MLRunTracerSettings, MLRunTracerClientSettings, MLRunTracerMonitorSettings\n",
    "\n",
    "settings = MLRunTracerSettings(\n",
    "    client=MLRunTracerClientSettings(\n",
    "        stream_path=\"my-project/model-endpoints/stream-v1\",\n",
    "        container=\"projects\",\n",
    "        model_endpoint_name=\"my_endpoint\",\n",
    "        model_endpoint_uid=\"abc123\",\n",
    "        serving_function=\"my_function\",\n",
    "    ),\n",
    "    monitor=MLRunTracerMonitorSettings(\n",
    "        label=\"production\",\n",
    "        root_run_only=True,  # Only monitor root runs, not child runs\n",
    "        tags_filter=[\"important\"],  # Only monitor runs with this tag\n",
    "    ),\n",
    ")\n",
    "```\n",
    "\n",
    "#### Example - environment variable configuration\n",
    "\n",
    "```bash\n",
    "export LC_MLRUN_TRACER_CLIENT_STREAM_PATH=\"my-project/model-endpoints/stream-v1\"\n",
    "export LC_MLRUN_TRACER_CLIENT_CONTAINER=\"projects\"\n",
    "export LC_MLRUN_TRACER_MONITOR_LABEL=\"production\"\n",
    "export LC_MLRUN_TRACER_MONITOR_ROOT_RUN_ONLY=\"true\"\n",
    "```\n",
    "\n",
    "### MLRun Tracer\n",
    "\n",
    "`MLRunTracer` is a LangChain-compatible tracer that converts LangChain `Run` objects into MLRun monitoring events and publishes them to a V3IO stream. \n",
    "\n",
    "Key points:\n",
    "* **No inheritance required** - use it directly without subclassing.\n",
    "* **Fully customizable via settings** - control filtering, summarization, and output format.\n",
    "* **Custom summarizer support** - pass your own `run_summarizer_function` via settings to customize how runs are converted to events.\n",
    "\n",
    "### Monitoring Setup Utility Function\n",
    "\n",
    "`setup_langchain_monitoring()` is a utility function that creates the necessary MLRun infrastructure for LangChain monitoring. This is a **temporary workaround** until custom endpoint creation support is added to MLRun.\n",
    "\n",
    "The function returns a dictionary of environment variables to configure auto-tracing. See how to use it in the tutorial section below.\n",
    "\n",
    "### LangChain Monitoring Application\n",
    "\n",
    "`LangChainMonitoringApp` is a base class (inheriting from MLRun's `ModelMonitoringApplicationBase`) for building monitoring applications that process events from the MLRun Tracer.\n",
    "\n",
    "It offers several built-in helper methods and metrics for analyzing LangChain runs:\n",
    "\n",
    "* Helper methods:\n",
    "  * `get_structured_runs()` - Parse raw monitoring samples into structured run dictionaries with filtering options\n",
    "  * `iterate_structured_runs()` - Iterate over all runs including nested child runs\n",
    "* Metric methods:\n",
    "  * `calculate_average_latency()` - Average latency across root runs\n",
    "  * `calculate_success_rate()` - Percentage of runs without errors\n",
    "  * `count_token_usage()` - Total input/output tokens from LLM runs\n",
    "  * `count_run_names()` - Count occurrences of each run name\n",
    "\n",
    "The base app can be used as-is, but it is recommended to extend it with your own custom monitoring logic.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e24e1a5-d80a-4b7e-9b94-57b24e8b39d7",
   "metadata": {},
   "source": [
    "## How to Apply MLRun?\n",
    "\n",
    "### Auto Tracing\n",
    "\n",
    "Auto tracing automatically instruments all LangChain code by setting the `LC_MLRUN_MONITORING_ENABLED` environment variable and importing the module:\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"LC_MLRUN_MONITORING_ENABLED\"] = \"1\"\n",
    "# Set other LC_MLRUN_TRACER_* environment variables as needed...\n",
    "\n",
    "# Import the module BEFORE any LangChain code\n",
    "langchain_mlrun = mlrun.import_module(\"hub://langchain_mlrun\")\n",
    "\n",
    "# All LangChain/LangGraph code below will be automatically traced\n",
    "chain.invoke(...)\n",
    "```\n",
    "\n",
    "### Manual Tracing\n",
    "\n",
    "For more control, use the `mlrun_monitoring()` context manager to trace specific code blocks:\n",
    "\n",
    "```python\n",
    "langchain_mlrun = mlrun.import_module(\"hub://langchain_mlrun\")\n",
    "mlrun_monitoring = langchain_mlrun.mlrun_monitoring\n",
    "MLRunTracerSettings = langchain_mlrun.MLRunTracerSettings\n",
    "\n",
    "# Optional: customize settings\n",
    "settings = MLRunTracerSettings(...)\n",
    "\n",
    "with mlrun_monitoring(settings=settings) as tracer:\n",
    "    # Only LangChain code within this block will be traced\n",
    "    result = chain.invoke({\"topic\": \"MLRun\"})\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b52d3d-a431-44fb-acd6-ea33fec37a49",
   "metadata": {},
   "source": [
    "## Tutorial\n",
    "\n",
    "In this tutorial we'll show how to orchestrate LangChain based code with MLRun using the `langchain_mlrun` hub module.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Install MLRun and the `langchain_mlrun` requirements."
   ]
  },
  {
   "cell_type": "code",
   "id": "caf72aa6-06e8-4a04-bfc4-409b39d255fe",
   "metadata": {},
   "source": "!pip install mlrun langchain pydantic-settings kafka-python",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "vprq8wj4iqh",
   "source": [
    "### Local Development Setup (Optional)\n",
    "\n",
    "> Skip this section if you're running inside a Jupyter instance deployed in the MLRun cluster.\n",
    "\n",
    "If you're running this notebook from your local machine, follow these steps:\n",
    "\n",
    "#### Step 1: Set Environment Variables\n",
    "\n",
    "Run the cell below to set up all required environment variables for local development."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9lc788zu3zi",
   "source": [
    "import os\n",
    "\n",
    "# MLRun API endpoint:\n",
    "# os.environ[\"MLRUN_DBPATH\"] = \"http://localhost:30070\"\n",
    "\n",
    "# Kafka Configuration:\n",
    "# os.environ[\"KAFKA_BROKER\"] = \"<kafka-broker-address>\"\n",
    "\n",
    "# TDEngine Configuration:\n",
    "# os.environ[\"TDENGINE_HOST\"] = \"<tdengine-host>\"\n",
    "# os.environ[\"TDENGINE_PORT\"] = \"<tdengine-port>\"\n",
    "# os.environ[\"TDENGINE_USER\"] = \"<tdengine-username>\"\n",
    "# os.environ[\"TDENGINE_PASSWORD\"] = \"<tdengine-password>\"\n",
    "\n",
    "# MinIO/S3 Configuration:\n",
    "# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"<your-minio-access-key>\"\n",
    "# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"<your-minio-secret-key>\"\n",
    "# os.environ[\"AWS_ENDPOINT_URL_S3\"] = \"<s3-endpoint-url>\""
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 2: Set Up Port Forwarding\n",
    "\n",
    "Set up port-forwarding to access cluster services. Run these commands in separate terminal windows:\n",
    "\n",
    "```bash\n",
    "# MLRun API\n",
    "kubectl port-forward -n mlrun svc/mlrun-api 30070:8080\n",
    "```\n",
    "\n",
    "```bash\n",
    "# MinIO (S3-compatible storage)\n",
    "kubectl port-forward -n mlrun svc/minio 9000:9000\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Kafka (for CE mode) - requires /etc/hosts entry: 127.0.0.1 kafka-stream\n",
    "kubectl port-forward -n mlrun svc/kafka-stream 9092:9092\n",
    "```\n",
    "\n",
    "```bash\n",
    "# TDEngine (for CE mode) - requires /etc/hosts entry: 127.0.0.1 tdengine-tsdb\n",
    "kubectl port-forward -n mlrun svc/tdengine-tsdb 6041:6041\n",
    "```"
   ],
   "id": "6d1d2d3c016ec62c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create Project\n",
    "\n",
    "We'll first create an MLRun project"
   ],
   "id": "4442f7ad1b0a8ee"
  },
  {
   "cell_type": "code",
   "id": "2664df3e-d9c6-40dd-a215-29d60e4b4208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T19:43:18.142870Z",
     "start_time": "2026-02-03T19:43:10.068758Z"
    }
   },
   "source": [
    "import time\n",
    "import datetime\n",
    "import mlrun\n",
    "\n",
    "print(f\"MLRun version: {mlrun.__version__}\")\n",
    "print(f\"CE Mode: {mlrun.mlconf.is_ce_mode()}\")\n",
    "\n",
    "project = mlrun.get_or_create_project(\"langchain-mlrun-tutorial\")\n",
    "print(f\"Project: {project.name}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLRun version: 1.10.0\n",
      "CE Mode: True\n",
      "> 2026-02-03 21:43:18,053 [info] Loading project from path: {\"path\":\"./\",\"project_name\":\"langchain-mlrun-tutorial\",\"user_project\":false}\n",
      "> 2026-02-03 21:43:18,141 [info] Project loaded successfully: {\"path\":\"./\",\"project_name\":\"langchain-mlrun-tutorial\",\"stored_in_db\":true}\n",
      "Project: langchain-mlrun-tutorial\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "33f28986-c158-47fd-97a6-74f69892b4eb",
   "metadata": {},
   "source": "### Enable Monitoring\n\nTo use MLRun's monitoring feature in our project we first need to set up the monitoring infrastructure.\n\n- **MLRun CE**: Uses Kafka for streaming (automatically detected)\n- **MLRun Enterprise**: Uses V3IO for streaming (automatically detected)\n\nThe cell below automatically detects your MLRun mode and sets up the appropriate streaming infrastructure."
  },
  {
   "cell_type": "code",
   "id": "d9d2fa66-0498-445d-ab4a-8370f46aec1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T19:49:22.700332Z",
     "start_time": "2026-02-03T19:43:18.148037Z"
    }
   },
   "source": [
    "# Create datastore profiles (based on CE or Enterprise):\n",
    "if mlrun.mlconf.is_ce_mode():\n",
    "    print(\"Setting up Kafka streaming for MLRun CE...\")\n",
    "    from mlrun.datastore.datastore_profile import DatastoreProfileKafkaStream, DatastoreProfileTDEngine\n",
    "    \n",
    "    stream_profile = DatastoreProfileKafkaStream(\n",
    "        name=\"kafka-stream-profile\",\n",
    "        brokers=os.environ[\"KAFKA_BROKER\"],\n",
    "        topics=[],\n",
    "    )\n",
    "    tsdb_profile = DatastoreProfileTDEngine(\n",
    "        name=\"tsdb-profile\",\n",
    "        user=os.environ[\"TDENGINE_USER\"],\n",
    "        password=os.environ[\"TDENGINE_PASSWORD\"],\n",
    "        host=os.environ[\"TDENGINE_HOST\"],\n",
    "        port=int(os.environ[\"TDENGINE_PORT\"]),\n",
    "    )\n",
    "    project.register_datastore_profile(stream_profile)\n",
    "    project.register_datastore_profile(tsdb_profile)\n",
    "else:  # Enterprise\n",
    "    print(\"Setting up V3IO streaming for MLRun Enterprise...\")\n",
    "    from mlrun.datastore import DatastoreProfileV3io\n",
    "    \n",
    "    stream_profile = DatastoreProfileV3io(name=\"v3io-ds\", v3io_access_key=os.environ[\"V3IO_ACCESS_KEY\"])\n",
    "    tsdb_profile = stream_profile\n",
    "    project.register_datastore_profile(stream_profile)\n",
    "\n",
    "# Enable monitoring in our project:\n",
    "project.set_model_monitoring_credentials(\n",
    "    stream_profile_name=stream_profile.name,\n",
    "    tsdb_profile_name=tsdb_profile.name,\n",
    ")\n",
    "project.enable_model_monitoring(\n",
    "    base_period=1,\n",
    "    wait_for_deployment=True,\n",
    ")\n",
    "\n",
    "print(\"Monitoring enabled successfully!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f23117fa-7b67-470c-80ca-976d14c2120e",
   "metadata": {},
   "source": [
    "### Import `langchain_mlrun`\n",
    "\n",
    "Now we'll import `langchain_mlrun` from the hub."
   ]
  },
  {
   "cell_type": "code",
   "id": "2360cd49-b260-4140-bd16-138349e000b3",
   "metadata": {},
   "source": [
    "# Import the module from the hub:\n",
    "langchain_mlrun = mlrun.import_module(\"hub://langchain_mlrun\")\n",
    "\n",
    "# Import the utility function and monitoring application from the module:\n",
    "setup_langchain_monitoring = langchain_mlrun.setup_langchain_monitoring\n",
    "LangChainMonitoringApp = langchain_mlrun.LangChainMonitoringApp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "de030131-ebaf-48f8-96ed-3c1013b5e260",
   "metadata": {},
   "source": [
    "### Create Monitorable Endpoint\n",
    "\n",
    "Endpoints are the entities being monitored by MLRun. We'll use the `setup_langchain_monitoring()` utility function to create the model monitoring endpoint.\n",
    "\n",
    "For MLRun CE mode, you must pass the `kafka_stream_profile_name` parameter with the name of the registered Kafka stream profile.\n",
    "\n",
    "By default, the endpoint name will be `\"langchain_mlrun_endpoint\"` but you can change it by using the `model_endpoint_name` parameter."
   ]
  },
  {
   "cell_type": "code",
   "id": "0e9baf78-3d38-46bd-89dd-6f83760eaeb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T19:49:23.861085Z",
     "start_time": "2026-02-03T19:49:23.412235Z"
    }
   },
   "source": [
    "# Pass kafka_stream_profile_name for CE mode (required)\n",
    "env_vars = setup_langchain_monitoring(\n",
    "    kafka_stream_profile_name=stream_profile.name if mlrun.mlconf.is_ce_mode() else None\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LangChain model endpoint\n",
      "\n",
      "  [✓] Loading Project......................... Done (0.00s)\u001B[K\n",
      "  [✓] Creating Model.......................... Done (0.31s)                            \u001B[K\n",
      "  [✓] Creating Function....................... Done (0.04s)                                  \u001B[K\n",
      "  [✓] Creating Model Endpoint................. Done (0.09s)                        \u001B[K\n",
      "\n",
      "✨ Done! LangChain monitoring model endpoint created successfully.\n",
      "You can now set the following environment variables to enable MLRun tracing in your LangChain code:\n",
      "\n",
      "{\n",
      "    \"MLRUN_MONITORING_ENABLED\": \"1\",\n",
      "    \"MLRUN_TRACER_CLIENT_PROJECT\": \"langchain-mlrun-tutorial\",\n",
      "    \"MLRUN_TRACER_CLIENT_MODEL_ENDPOINT_NAME\": \"langchain_mlrun_endpoint\",\n",
      "    \"MLRUN_TRACER_CLIENT_MODEL_ENDPOINT_UID\": \"d1d2b2686772441cacf687b45cd48ffa\",\n",
      "    \"MLRUN_TRACER_CLIENT_SERVING_FUNCTION\": \"langchain_mlrun_function\",\n",
      "    \"MLRUN_TRACER_CLIENT_KAFKA_STREAM_PROFILE_NAME\": \"kafka-stream-profile\"\n",
      "}\n",
      "\n",
      "To customize the monitoring behavior, you can also set additional environment variables prefixed with 'MLRUN_TRACER_MONITOR_'. Refer to the MLRun tracer documentation for more details.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "dd45c94b-ee05-449c-9336-0aa659e66bda",
   "metadata": {},
   "source": [
    "### Setup Environment Variables for Auto Tracing\n",
    "\n",
    "We'll use the environment variables returned from `setup_langchain_monitoring` to setup the environment for auto-tracing. Read the printed outputs for more information."
   ]
  },
  {
   "cell_type": "code",
   "id": "1c1988f8-c80a-4bf2-bfb1-d43523fc161f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T19:49:23.866556Z",
     "start_time": "2026-02-03T19:49:23.864805Z"
    }
   },
   "source": [
    "os.environ.update(env_vars)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "d3f3b8e5-3538-4153-95da-e6d8776be3ac",
   "metadata": {},
   "source": "### Run `langchain` or `langgraph` Code\n\nHere we have 3 functions, each using different method utilizing LLMs with `langchain` and `langgraph`:\n* `run_simple_chain` - Using `langchain`'s chains.\n* `run_simple_agent` - Using `langchain`'s `create_agent` function and `tool`s.\n* `run_langgraph_graph` - Using pure `langgraph`.\n\n> **Notice**: You don't need to set OpenAI API credentials, there is a mock `ChatModel` that will replace it if the credentials are not set in the environment. If you wish to use OpenAI models, make sure you `pip install langchain_openai` and set the `OPENAI_API_KEY` environment variable before continue to the next cell.\n\nBecause the auto-tracing environment is set, any run will be automatically traced and monitored!\n\nFeel free to adjust the code as you like.\n\n> **Remember**: To enable auto-tracing you do need to set the environment variables and import the `langchain_mlrun` module before any LangChain code. For batch jobs and realtime functions, make sure you set env vars in the MLRun function and add the import line `langchain_mlrun = mlrun.import_module(\"hub://langchain_mlrun\")` at the top of your code."
  },
  {
   "cell_type": "code",
   "id": "94b4d4b0-8d10-4ad3-8f16-7b1b7daeac11",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2026-02-03T19:49:24.899991Z",
     "start_time": "2026-02-03T19:49:23.869475Z"
    }
   },
   "source": [
    "import os\n",
    "from typing import Literal, TypedDict, Annotated, Sequence, Any, Callable\n",
    "from operator import add\n",
    "\n",
    "from langchain_core.language_models import LanguageModelInput\n",
    "from langchain_core.runnables import Runnable, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.language_models.fake_chat_models import FakeListChatModel, GenericFakeChatModel\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.tools import tool, BaseTool\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "def _check_openai_credentials() -> bool:\n",
    "    \"\"\"\n",
    "    Check if OpenAI API key is set in environment variables.\n",
    "\n",
    "    :return: True if OPENAI_API_KEY is set, False otherwise.\n",
    "    \"\"\"\n",
    "    return \"OPENAI_API_KEY\" in os.environ\n",
    "\n",
    "\n",
    "# Import ChatOpenAI only if OpenAI credentials are available (meaning `langchain-openai` must be installed).\n",
    "if _check_openai_credentials():\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    \n",
    "class _ToolEnabledFakeModel(GenericFakeChatModel):\n",
    "    \"\"\"\n",
    "    A fake chat model that supports tool binding for running agent tracing tests.\n",
    "    \"\"\"\n",
    "\n",
    "    def bind_tools(\n",
    "        self,\n",
    "        tools: Sequence[\n",
    "            dict[str, Any] | type | Callable | BaseTool  # noqa: UP006\n",
    "        ],\n",
    "        *,\n",
    "        tool_choice: str | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable[LanguageModelInput, AIMessage]:\n",
    "        return self\n",
    "\n",
    "\n",
    "#: Tag value for testing tag filtering.\n",
    "_dummy_tag = \"dummy_tag\"\n",
    "\n",
    "\n",
    "def run_simple_chain() -> str:\n",
    "    \"\"\"\n",
    "    Run a simple LangChain chain that gets a fact about a topic.\n",
    "    \"\"\"\n",
    "    # Build a simple chain: prompt -> llm -> str output parser\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        tags=[_dummy_tag]\n",
    "    ) if _check_openai_credentials() else (\n",
    "        FakeListChatModel(\n",
    "            responses=[\n",
    "                \"MLRun is an open-source orchestrator for machine learning pipelines.\"\n",
    "            ],\n",
    "            tags=[_dummy_tag]\n",
    "        )\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_template(\"Tell me a short fact about {topic}\")\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run the chain:\n",
    "    response = chain.invoke({\"topic\": \"MLRun\"})\n",
    "    return response\n",
    "\n",
    "\n",
    "def run_simple_agent():\n",
    "    \"\"\"\n",
    "    Run a simple LangChain agent that uses two tools to get weather and stock price.\n",
    "    \"\"\"\n",
    "    # Define the tools:\n",
    "    @tool\n",
    "    def get_weather(city: str) -> str:\n",
    "        \"\"\"Get the current weather for a specific city.\"\"\"\n",
    "        return f\"The weather in {city} is 22°C and sunny.\"\n",
    "\n",
    "    @tool\n",
    "    def get_stock_price(symbol: str) -> str:\n",
    "        \"\"\"Get the current stock price for a symbol.\"\"\"\n",
    "        return f\"The stock price for {symbol} is $150.25.\"\n",
    "\n",
    "    # Define the model:\n",
    "    model = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        tags=[_dummy_tag]\n",
    "    ) if _check_openai_credentials() else (\n",
    "        _ToolEnabledFakeModel(\n",
    "            messages=iter(\n",
    "                [\n",
    "                    AIMessage(\n",
    "                        content=\"\",\n",
    "                        tool_calls=[\n",
    "                            {\"name\": \"get_weather\", \"args\": {\"city\": \"London\"}, \"id\": \"call_abc123\"},\n",
    "                            {\"name\": \"get_stock_price\", \"args\": {\"symbol\": \"AAPL\"}, \"id\": \"call_def456\"}\n",
    "                        ]\n",
    "                    ),\n",
    "                    AIMessage(content=\"The weather in London is 22°C and AAPL is trading at $150.25.\")\n",
    "                ]\n",
    "            ),\n",
    "            tags=[_dummy_tag]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the agent:\n",
    "    agent = create_agent(\n",
    "        model=model,\n",
    "        tools=[get_weather, get_stock_price],\n",
    "        system_prompt=\"You are a helpful assistant with access to tools.\"\n",
    "    )\n",
    "\n",
    "    # Run the agent:\n",
    "    return agent.invoke({\"messages\": [\"What is the weather in London and the stock price of AAPL?\"]})\n",
    "\n",
    "\n",
    "def run_langgraph_graph():\n",
    "    \"\"\"\n",
    "    Run a LangGraph agent that uses reflection to correct its answer.\n",
    "    \"\"\"\n",
    "    # Define the graph state:\n",
    "    class AgentState(TypedDict):\n",
    "        messages: Annotated[list[BaseMessage], add]\n",
    "        attempts: int\n",
    "\n",
    "    # Define the model:\n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\") if _check_openai_credentials() else (\n",
    "        _ToolEnabledFakeModel(\n",
    "            messages=iter(\n",
    "                [\n",
    "                    AIMessage(content=\"There are 2 'r's in Strawberry.\"),  # Mocking the failure\n",
    "                    AIMessage(content=\"I stand corrected. S-t-r-a-w-b-e-r-r-y. There are 3 'r's.\"),  # Mocking the fix\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Define the graph nodes and router:\n",
    "    def call_model(state: AgentState):\n",
    "        response = model.invoke(state[\"messages\"])\n",
    "        return {\"messages\": [response], \"attempts\": state[\"attempts\"] + 1}\n",
    "\n",
    "    def reflect_node(state: AgentState):\n",
    "        prompt = \"Wait, count the 'r's again slowly, letter by letter. Are you sure?\"\n",
    "        return {\"messages\": [HumanMessage(content=prompt)]}\n",
    "\n",
    "    def router(state: AgentState) -> Literal[\"reflect\", END]:\n",
    "        # Make sure there are 2 attempts at least for an answer:\n",
    "        if state[\"attempts\"] == 1:\n",
    "            return \"reflect\"\n",
    "        return END\n",
    "\n",
    "    # Build the graph:\n",
    "    builder = StateGraph(AgentState)\n",
    "    builder.add_node(\"model\", call_model)\n",
    "    tagged_reflect_node = RunnableLambda(reflect_node).with_config(tags=[_dummy_tag])\n",
    "    builder.add_node(\"reflect\", tagged_reflect_node)\n",
    "    builder.add_edge(START, \"model\")\n",
    "    builder.add_conditional_edges(\"model\", router)\n",
    "    builder.add_edge(\"reflect\", \"model\")\n",
    "    graph = builder.compile()\n",
    "\n",
    "    # Run the graph:\n",
    "    return graph.invoke({\"messages\": [HumanMessage(content=\"How many 'r's in Strawberry?\")], \"attempts\": 0})"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "49964f96-89ba-4f61-8788-38290a877aa2",
   "metadata": {},
   "source": "Let's create some traffic, we'll run whatever function you want in a loop to get some events. We take timestamps in order to use them later to run the monitoring application on the data we'll send."
  },
  {
   "cell_type": "code",
   "id": "b7e6418d-76f4-4b18-9ef9-c5bb40b20545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T22:05:54.601563Z",
     "start_time": "2026-02-03T22:05:52.518385Z"
    }
   },
   "source": [
    "# Run LangChain code and now it should be tracked and monitored in MLRun:\n",
    "start_timestamp = datetime.datetime.now() - datetime.timedelta(minutes=1)\n",
    "for i in range(20):\n",
    "    run_simple_agent()\n",
    "end_timestamp = datetime.datetime.now() + datetime.timedelta(minutes=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2026-02-04 00:05:52,553 [info] Project loaded successfully: {\"project_name\":\"langchain-mlrun-tutorial\"}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "d9085765-91fd-4d31-84b4-927ecf9cc455",
   "metadata": {},
   "source": "> **Note**: Please wait a minute or two until the events are processed."
  },
  {
   "cell_type": "code",
   "id": "85fae3e4-5f1b-4f0c-ba71-81060f10804f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T19:50:26.655189Z",
     "start_time": "2026-02-03T19:49:26.648461Z"
    }
   },
   "source": [
    "time.sleep(60)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "2475ebec-fc32-4884-9723-3ca9cfde577f",
   "metadata": {},
   "source": [
    "### Test the LangChain Monitoring Application\n",
    "\n",
    "To test a monitoring application, we use the `evaluate` class method. We'll run an evaluation on the data we just sent. It is a small local job and should run fast.\n",
    "\n",
    "Keep an eye for the returned metrics from the monitoring application."
   ]
  },
  {
   "cell_type": "code",
   "id": "3d046755-9153-497a-a024-5d63316e1f91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T19:50:28.003195Z",
     "start_time": "2026-02-03T19:50:26.670024Z"
    }
   },
   "source": [
    "LangChainMonitoringApp.evaluate(\n",
    "    func_name=\"langchain-monitoring-app-test\",\n",
    "    func_path=\"langchain_mlrun.py\",\n",
    "    run_local=True,\n",
    "    endpoints=[env_vars[\"LC_MLRUN_TRACER_CLIENT_MODEL_ENDPOINT_NAME\"]],\n",
    "    start=start_timestamp.isoformat(),\n",
    "    end=end_timestamp.isoformat(),\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2026-02-03 21:50:26,671 [info] Changing function name - adding `\"-batch\"` suffix: {\"func_name\":\"langchain-monitoring-app-test-batch\"}\n",
      "> 2026-02-03 21:50:26,815 [warning] It is recommended to use k8s secret (specify secret_name), specifying aws_access_key/aws_secret_key directly is unsafe.\n",
      "> 2026-02-03 21:50:26,829 [info] Storing function: {\"db\":\"http://localhost:30070\",\"name\":\"langchain-monitoring-app-test-batch--handler\",\"uid\":\"f2c3c94681094915beb2c5c1ccc0dac8\"}\n",
      "> 2026-02-03 21:50:27,953 [warning] No data was found for any of the specified endpoints. No results were produced: {\"application_name\":\"langchain-monitoring-app-test-batch\",\"end\":\"2026-02-03T21:54:26.640556\",\"endpoints\":[\"langchain_mlrun_endpoint\"],\"start\":\"2026-02-03T21:48:24.904667\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "\n",
       "  // Get the base URL of the current notebook\n",
       "  var baseUrl = window.location.origin;\n",
       "\n",
       "  // Construct the full URL\n",
       "  var fullUrl = new URL(el.title, baseUrl).href;\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = fullUrl\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (fullUrl.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", fullUrl);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = fullUrl;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>state</th>\n",
       "      <th>kind</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>langchain-mlrun-tutorial</td>\n",
       "      <td><div title=\"f2c3c94681094915beb2c5c1ccc0dac8\">...c0dac8</div></td>\n",
       "      <td>0</td>\n",
       "      <td>Feb 03 19:50:26</td>\n",
       "      <td>NaT</td>\n",
       "      <td>completed</td>\n",
       "      <td>run</td>\n",
       "      <td>langchain-monitoring-app-test-batch--handler</td>\n",
       "      <td><div class=\"dictlist\">kind=local</div><div class=\"dictlist\">owner=Tomer_Weitzman</div><div class=\"dictlist\">host=M-QXN63PHMF9</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">endpoints=['langchain_mlrun_endpoint']</div><div class=\"dictlist\">start=2026-02-03T21:48:24.904667</div><div class=\"dictlist\">end=2026-02-03T21:54:26.640556</div><div class=\"dictlist\">base_period=None</div><div class=\"dictlist\">write_output=False</div><div class=\"dictlist\">existing_data_handling=fail_on_overlap</div><div class=\"dictlist\">stream_profile=None</div></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result54d8bae8-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result54d8bae8-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result54d8bae8\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result54d8bae8-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods </b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2026-02-03 21:50:28,001 [info] Run execution finished: {\"name\":\"langchain-monitoring-app-test-batch--handler\",\"status\":\"completed\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.model.RunObject at 0x132cfaa50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "eda724c3-27f3-4d28-a7ba-1e59b9be2a37",
   "metadata": {},
   "source": "### Deploy the Monitoring Application\n\nAll that's left to do now is to deploy our monitoring application!"
  },
  {
   "cell_type": "code",
   "id": "652b00d4-070d-4849-9784-4d461cb83eae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T19:52:29.502406Z",
     "start_time": "2026-02-03T19:50:28.009318Z"
    }
   },
   "source": "# Deploy the monitoring app:\nLangChainMonitoringApp.deploy(\n    func_name=\"langchain-monitoring-app\",\n    func_path=\"langchain_mlrun.py\",\n    image=\"mlrun/mlrun\",\n    requirements=[\n        \"langchain\",\n        \"pydantic-settings\",\n    ],\n)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c23bef7a-cbdb-4b22-a2d9-2edbfde5eb04",
   "metadata": {},
   "source": [
    "Once it is deployed, you can run events again and see the monitoring application in MLRun UI in action:\n",
    "\n",
    "![mlrun ui example](./notebook_images/mlrun_ui.png)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fc994d2114a89a25"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
