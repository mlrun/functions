{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio config kind = \"job\"\n",
    "%nuclio config spec.image = \"mlrun/ml-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn\"t verify HTTPS certificates by default\n",
    "        pass\n",
    "else:\n",
    "    # Handle target environment that doesn\"t support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from cloudpickle import dump, load\n",
    "\n",
    "from mlrun.execution import MLClientCtx\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun.artifacts import PlotArtifact, TableArtifact\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "def arc_to_parquet(\n",
    "    context: MLClientCtx,\n",
    "    archive_url: DataItem,\n",
    "    header: str = \"\",\n",
    "    chunksize: int = 10_000,\n",
    "    dtype=None,\n",
    "    encoding: str = \"latin-1\",\n",
    "    key: str = \"data\",\n",
    "    dataset: str = \"\",\n",
    "    part_cols = [],\n",
    "    file_ext: str = \"parquet\",\n",
    "    refresh_data: bool = False\n",
    ") -> None:\n",
    "    \"\"\"Open a file/object archive and save as a parquet file or dataset\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * partitioning requires precise specification of column types.\n",
    "    * the archive_url can be any file readable by pandas read_csv, which includes tar files\n",
    "    * if the `dataset` parameter is not empty, then a partitioned dataset will be created\n",
    "    instead of a single file in the folder `dataset`\n",
    "    * if a key exists already then it will not be re-acquired unless the `refresh_data` param\n",
    "    is set to `True`.  This is in case the original file is corrupt, or a refresh is \n",
    "    required.\n",
    "\n",
    "    :param context:      function context\n",
    "    :param archive_url:  MLRun data input (DataItem object)\n",
    "    :param header:       column names\n",
    "    :param chunksize:    (0) row size retrieved per iteration\n",
    "    :param dtypes        destination data type of columns, as dict(col, type), for example\n",
    "                         {‘a’: np.float64, ‘b’: np.int32, ‘c’: ‘Int64’}  \n",
    "    :param encoding      (\"latin-8\") file encoding\n",
    "    :param key:          key in artifact store\n",
    "    :param dataset:      (None) if not None then \"target_path/dataset\"\n",
    "                         is folder for partitioned files\n",
    "    :param file_ext:     (parquet) csv/parquet file extension\n",
    "    :param part_cols:    ([]) list of partitioning columns\n",
    "    :param refresh_data: (False) overwrite existing data at that location/kye\n",
    "    \"\"\"\n",
    "    base_path = context.artifact_path\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    archive_url = archive_url.local()\n",
    "    \n",
    "    if dataset:\n",
    "        dest_path = os.path.join(base_path, dataset)\n",
    "        exists = os.path.isdir(dest_path)\n",
    "    else:\n",
    "        dest_path = os.path.join(base_path, key+f\".{file_ext}\")\n",
    "        exists = os.path.isfile(dest_path)\n",
    "\n",
    "    # todo: more logic for header\n",
    "    if not exists:\n",
    "        context.logger.info(\"destination file does not exist, downloading\")\n",
    "        pqwriter = None\n",
    "        for i, df in enumerate(pd.read_csv(archive_url, \n",
    "                                           chunksize=chunksize, \n",
    "                                           names=header,\n",
    "                                           encoding=encoding, \n",
    "                                           dtype=dtypes)):\n",
    "            table = pa.Table.from_pandas(df)\n",
    "            if i == 0:\n",
    "                if dataset:\n",
    "                    # just write header here\n",
    "                    pq.ParquetWriter(os.path.join(base_path,f\"header-only.{file_ext}\"), table.schema)\n",
    "                else:\n",
    "                    # start writing file\n",
    "                    pqwriter = pq.ParquetWriter(dest_path, table.schema)\n",
    "                context.log_artifact(\"header\", local_path=f\"header-only.{file_ext}\")\n",
    "            if dataset:\n",
    "                pq.write_to_dataset(table, root_path=dest_path, partition_cols=partition_cols)\n",
    "            else:\n",
    "                pqwriter.write_table(table)\n",
    "        if pqwriter:\n",
    "            pqwriter.close()\n",
    "\n",
    "        context.logger.info(f\"saved table to {dest_path}\")\n",
    "    else:\n",
    "        context.logger.info(\"destination file already exists\")\n",
    "    context.log_artifact(key, local_path=key+f\".{file_ext}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import mlconf\n",
    "import os\n",
    "mlconf.dbpath = mlconf.dbpath or \"http://mlrun-api:8080\"\n",
    "mlconf.artifact_path = mlconf.artifact_path or f\"{os.environ['V3IO_HOME']}/artifacts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function \n",
    "# create job function object from notebook code\n",
    "fn = code_to_function(\"arc_to_parquet\")\n",
    "\n",
    "# add metadata (for templates and reuse)\n",
    "fn.spec.default_handler = \"arc_to_parquet\"\n",
    "fn.spec.description = \"retrieve remote archive, open and save as parquet\"\n",
    "fn.metadata.categories = ['data-movement', 'utils']\n",
    "fn.metadata.labels = {\"author\": \"yjb\"}\n",
    "\n",
    "fn.save()\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"V3IO_HOME\" in list(os.environ):\n",
    "    from mlrun import mount_v3io\n",
    "    fn.apply(mount_v3io())\n",
    "else:\n",
    "    # is you set up mlrun using the instructions at https://github.com/mlrun/mlrun/blob/master/hack/local/README.md\n",
    "    from mlrun.platforms import mount_pvc\n",
    "    fn.apply(mount_pvc(\"nfsvol\", \"nfsvol\", \"/home/joyan/data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import NewTask    \n",
    "\n",
    "task_params = {\n",
    "    \"name\" : \"tasks archive to parquet\",\n",
    "    \"params\" :  {\n",
    "        \"header\" : [\"labels\", \"lepton_pT\", \"lepton_eta\", \"lepton_phi\", \"missing_energy_magnitude\", \n",
    "                    \"missing_energy_phi\", \"jet_1_pt\", \"jet_1_eta\", \"jet_1_phi\", \"jet_1_b-tag\", \n",
    "                    \"jet_2_pt\", \"jet_2_eta\", \"jet_2_phi\", \"jet_2_b-tag\", \"jet_3_pt\", \"jet_3_eta\",\n",
    "                    \"jet_3_phi\", \"jet_3_b-tag\", \"jet_4_pt\", \"jet_4_eta\", \"jet_4_phi\", \"jet_4_b-tag\",\n",
    "                    \"m_jj\", \"m_jjj\", \"m_lv\", \"m_jlv\", \"m_bb\", \"m_wbb\", \"m_wwbb\"],\n",
    "        \n",
    "        \"key\"    : \"higgs\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import run_local\n",
    "\n",
    "run = run_local(NewTask(**task_params),\n",
    "          handler=arc_to_parquet,\n",
    "          inputs={\"archive_url\" : \"https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the following will run quickly if your artifact path hasn\"t changed, the large file will be detected and not downloaded a second time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import NewTask\n",
    "run = fn.run(NewTask(**task_params))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
