{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio config kind = \"job\"\n",
    "%nuclio config spec.image = \"mlrun/ml-models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun.artifacts import get_model\n",
    "from cloudpickle import load\n",
    "from mlrun.mlutils import eval_class_model\n",
    "\n",
    "def cox_test(\n",
    "    context,\n",
    "    models_path: DataItem, \n",
    "    test_set: DataItem,\n",
    "    label_column: str,\n",
    "    plots_dest: str = \"plots\",\n",
    "    model_evaluator = None\n",
    ") -> None:\n",
    "    \"\"\"Test one or more classifier models against held-out dataset\n",
    "    \n",
    "    Using held-out test features, evaluates the peformance of the estimated model\n",
    "    \n",
    "    Can be part of a kubeflow pipeline as a test step that is run post EDA and \n",
    "    training/validation cycles\n",
    "    \n",
    "    :param context:         the function context\n",
    "    :param model_file:      model artifact to be tested\n",
    "    :param test_set:        test features and labels\n",
    "    :param label_column:    column name for ground truth labels\n",
    "    :param score_method:    for multiclass classification\n",
    "    :param plots_dest:      dir for test plots\n",
    "    :param model_evaluator: WIP: specific method to generate eval, passed in as string\n",
    "                            or available in this folder\n",
    "    \"\"\"  \n",
    "    xtest = test_set.as_df()\n",
    "    ytest = xtest.pop(label_column)\n",
    "    \n",
    "    model_file, model_obj, _ = get_model(models_path.url, suffix='.pkl')\n",
    "    model_obj = load(open(str(model_file), \"rb\"))\n",
    "\n",
    "    try:\n",
    "        # there could be different eval_models, type of model (xgboost, tfv1, tfv2...)\n",
    "        if not model_evaluator:\n",
    "            # binary and multiclass\n",
    "            eval_metrics = eval_class_model(context, xtest, ytest, model_obj)\n",
    "\n",
    "        # just do this inside log_model?\n",
    "        model_plots = eval_metrics.pop(\"plots\")\n",
    "        model_tables = eval_metrics.pop(\"tables\")\n",
    "        for plot in model_plots:\n",
    "            context.log_artifact(plot, local_path=f\"{plots_dest}/{plot.key}.html\")\n",
    "        for tbl in model_tables:\n",
    "            context.log_artifact(tbl, local_path=f\"{plots_dest}/{plot.key}.csv\")\n",
    "\n",
    "        context.log_results(eval_metrics)\n",
    "    except:\n",
    "        #dummy log:\n",
    "        context.log_dataset(\"cox-test-summary\", df=model_obj.summary, index=True, format=\"csv\")\n",
    "        context.logger.info(\"cox tester not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import mlconf\n",
    "import os\n",
    "\n",
    "mlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'\n",
    "mlconf.artifact_path = mlconf.artifact_path or f'{os.environ[\"HOME\"]}/artifacts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function \n",
    "# create job function object from notebook code\n",
    "fn = code_to_function(\"cox_test\")\n",
    "\n",
    "# add metadata (for templates and reuse)\n",
    "fn.spec.default_handler = \"cox_test\"\n",
    "fn.spec.description = \"test a classifier using held-out or new data\"\n",
    "fn.metadata.categories = [\"ml\", \"test\"]\n",
    "fn.metadata.labels = {\"author\": \"yjb\", \"framework\": \"survival\"}\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"V3IO_HOME\" in list(os.environ):\n",
    "    from mlrun import mount_v3io\n",
    "    fn.apply(mount_v3io())\n",
    "else:\n",
    "    # is you set up mlrun using the instructions at https://github.com/mlrun/mlrun/blob/master/hack/local/README.md\n",
    "    from mlrun.platforms import mount_pvc\n",
    "    fn.apply(mount_pvc('nfsvol', 'nfsvol', '/home/jovyan/data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_params = {\n",
    "    \"name\" : \"tasks cox test\",\n",
    "    \"params\": {\n",
    "        \"label_column\"  : \"labels\",\n",
    "        \"plots_dest\"    : \"churn/test/plots\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://raw.githubusercontent.com/yjb-ds/testdata/master/data/churn-tests.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import run_local, NewTask\n",
    "\n",
    "run = run_local(NewTask(**task_params),\n",
    "                handler=cox_test,\n",
    "                inputs={\"test_set\": DATA_URL,\n",
    "                        \"models_path\"   : \"models/cox\"},\n",
    "               workdir=mlconf.artifact_path+\"/churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = fn.run(\n",
    "    NewTask(**task_params),\n",
    "    inputs={\n",
    "        \"test_set\": DATA_URL,\n",
    "        \"models_path\"   : \"models/cox\"},\n",
    "    workdir=os.path.join(mlconf.artifact_path, \"churn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
