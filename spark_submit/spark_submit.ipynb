{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submiting spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the MLRun environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting kind to 'job'\n",
      "%nuclio: setting spec.image to 'mlrun/mlrun'\n"
     ]
    }
   ],
   "source": [
    "%nuclio config kind = \"job\"\n",
    "%nuclio config spec.image = \"mlrun/mlrun\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import get_or_create_ctx\n",
    "from kubernetes import config, client\n",
    "from kubernetes.stream import stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K8SClient(object):\n",
    "\n",
    "    def __init__(self, logger, namespace='default-tenant', config_file=None):\n",
    "        self.namespace = namespace\n",
    "        self.logger = logger\n",
    "        self._init_k8s_config(config_file)\n",
    "        self.v1api = client.CoreV1Api()\n",
    "\n",
    "    def _init_k8s_config(self, config_file):\n",
    "        try:\n",
    "            config.load_incluster_config()\n",
    "            self.logger.info('using in-cluster config.')\n",
    "        except Exception:\n",
    "            try:\n",
    "                config.load_kube_config(config_file)\n",
    "                self.logger.info('using local kubernetes config.')\n",
    "            except Exception:\n",
    "                raise RuntimeError(\n",
    "                    'cannot find local kubernetes config file,'\n",
    "                    ' place it in ~/.kube/config or specify it in '\n",
    "                    'KUBECONFIG env var')\n",
    "\n",
    "    def get_shell_pod_name(self, pod_name='shell'):\n",
    "        shell_pod = self.v1api.list_namespaced_pod(namespace=self.namespace)\n",
    "        for i in shell_pod.items:\n",
    "            if pod_name in i.metadata.name:\n",
    "                self.logger.info(\"%s\\t%s\\t%s\" % (i.status.pod_ip, i.metadata.namespace, i.metadata.name))\n",
    "                shell_name = i.metadata.name\n",
    "                break\n",
    "        return shell_name\n",
    "\n",
    "    def exec_shell_cmd(self, cmd, shell_pod_name = 'shell'):\n",
    "        shell_name = self.get_shell_pod_name(shell_pod_name)\n",
    "        # Calling exec and waiting for response\n",
    "        exec_command = [\n",
    "            '/bin/bash',\n",
    "            '-c',\n",
    "            cmd]\n",
    "        resp = stream(self.v1api.connect_get_namespaced_pod_exec,\n",
    "                      shell_name,\n",
    "                      self.namespace,\n",
    "                      command=exec_command,\n",
    "                      stderr=True, stdin=False,\n",
    "                      stdout=True, tty=False)\n",
    "        self.logger.info(\"Response: \" + resp)\n",
    "\n",
    "\n",
    "def spark_command_builder(name, class_name, jars, packages, spark_options):\n",
    "    cmd = 'spark-submit'\n",
    "    if name:\n",
    "        cmd += ' --name ' + name\n",
    "\n",
    "    if class_name:\n",
    "        cmd += ' --class ' + class_name\n",
    "\n",
    "    if jars:\n",
    "        cmd += ' --jars ' + jars\n",
    "\n",
    "    if packages:\n",
    "        cmd += ' --packages ' + packages\n",
    "\n",
    "    if spark_options:\n",
    "        cmd += ' ' + spark_options\n",
    "\n",
    "    return cmd\n",
    "\n",
    "\n",
    "def spark_submit(context, v3io_access_key, name=None, class_name=None, jars=None, packages=None, spark_options=''):\n",
    "    \"\"\"spark_submit function\n",
    "    \n",
    "    submiting spark via shell\n",
    "    \n",
    "    :param name:        A name of your application.\n",
    "    :param class_name:  Your application's main class (for Java / Scala apps).\n",
    "                        * If relative will add to the {artifact_path}\n",
    "    :param jars:        Comma-separated list of jars to include on the driver\n",
    "                        and executor classpaths.\n",
    "    :param packages:    Comma-separated list of maven coordinates of jars to include\n",
    "                        on the driver and executor classpaths. Will search the local\n",
    "                        maven repo, then maven central and any additional remote\n",
    "                        repositories given by --repositories. The format for the\n",
    "    :param spark_options: spark parametes that are not included as function arguments\n",
    "    \"\"\"\n",
    "    cmd = spark_command_builder(name, class_name, jars, packages, spark_options)\n",
    "    context.logger.info(\"submiting :\" + cmd)\n",
    "    cli = K8SClient(context.logger)\n",
    "    cli.exec_shell_cmd(cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "> This test uses the metrics data, created by the [Generator function](https://github.com/mlrun/demo-network-operations/blob/master/notebooks/generator.ipynb) from MLRun's [Network Operations Demo](https://github.com/mlrun/demo-network-operations)  \n",
    "To test it yourself, please generate this dataset or use any of your available csv/parquet datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlrun import code_to_function, mount_v3io, NewTask, mlconf, run_local\n",
    "mlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'\n",
    "mlconf.artifact_path = mlconf.artifact_path or f'{os.environ[\"HOME\"]}/artifacts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the execute test task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_task = NewTask(name='spark-submit',\n",
    "                         project='submit-proj',\n",
    "                         params={'spark_options':\"/spark/examples/jars/spark-examples_2.11-2.4.4.jar 10\",'class_name':'org.apache.spark.examples.SparkPi'},                          \n",
    "                         handler=spark_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit_run = run_local(submit_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the code to an MLRun function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-06-17 14:47:58,078 function spec saved to path: function.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7f69a4117c50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = code_to_function('spark-submit', handler='spark_submit')\n",
    "fn.spec.service_account='mlrun-api'\n",
    "fn.apply(mount_v3io())\n",
    "fn.export('function.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-06-17 14:47:58,101 warning!, server (0.4.7) and client (0.4.9) ver dont match\n",
      "[mlrun] 2020-06-17 14:47:58,101 starting run spark-submit uid=27391b5c221e4409baf4ae896169768b  -> http://mlrun-api:8080\n",
      "[mlrun] 2020-06-17 14:47:58,204 Job is running in the background, pod: spark-submit-wrc48\n",
      "[mlrun] 2020-06-17 14:48:02,407 starting local run: main.py # spark_submit\n",
      "[mlrun] 2020-06-17 14:48:02,423 submiting :spark-submit --class org.apache.spark.examples.SparkPi /spark/examples/jars/spark-examples_2.11-2.4.4.jar 10\n",
      "[mlrun] 2020-06-17 14:48:02,423 using in-cluster config.\n",
      "[mlrun] 2020-06-17 14:48:02,516 10.200.0.53\tdefault-tenant\tshell-658dd9bb8f-8q5sr\n",
      "[mlrun] 2020-06-17 14:48:08,739 Response: 20/06/17 14:48:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "20/06/17 14:48:04 INFO spark.SparkContext: Running Spark version 2.4.4\n",
      "20/06/17 14:48:04 INFO spark.SparkContext: Submitted application: Spark Pi\n",
      "20/06/17 14:48:04 INFO spark.SecurityManager: Changing view acls to: iguazio\n",
      "20/06/17 14:48:04 INFO spark.SecurityManager: Changing modify acls to: iguazio\n",
      "20/06/17 14:48:04 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "20/06/17 14:48:04 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "20/06/17 14:48:04 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(iguazio); groups with view permissions: Set(); users  with modify permissions: Set(iguazio); groups with modify permissions: Set()\n",
      "20/06/17 14:48:04 INFO util.Utils: Successfully started service 'sparkDriver' on port 34054.\n",
      "20/06/17 14:48:04 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "20/06/17 14:48:04 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "20/06/17 14:48:04 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/06/17 14:48:04 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/06/17 14:48:04 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-b5993354-454e-47e8-b317-349b4268fe6a\n",
      "20/06/17 14:48:04 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "20/06/17 14:48:04 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "20/06/17 14:48:04 INFO util.log: Logging initialized @1657ms\n",
      "20/06/17 14:48:04 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "20/06/17 14:48:04 INFO server.Server: Started @1718ms\n",
      "20/06/17 14:48:04 INFO server.AbstractConnector: Started ServerConnector@7e97551f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "20/06/17 14:48:04 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1450078a{/jobs,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@534ca02b{/jobs/json,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a23c3d{/jobs/job,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fe46b62{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591fd34d{/stages,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61e45f87{/stages/json,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c9b78e3{/stages/stage,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5491f68b{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@736ac09a{/stages/pool,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ecd665{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@45394b31{/storage,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ec7d8b3{/storage/json,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3b0ca5e1{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5bb3131b{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54dcbb9f{/environment,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74fef3f7{/environment/json,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a037324{/executors,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69eb86b4{/executors/json,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@585ac855{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5bb8f9e2{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a933be2{/static,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@378bd86d{/,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2189e7a7{/api,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@644abb8f{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a411233{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:04 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.200.0.53:4040\n",
      "20/06/17 14:48:04 INFO spark.SparkContext: Added JAR file:///spark/v3io-libs/v3io-hcfs_2.11.jar at spark://10.200.0.53:34054/jars/v3io-hcfs_2.11.jar with timestamp 1592405284608\n",
      "20/06/17 14:48:04 INFO spark.SparkContext: Added JAR file:///spark/v3io-libs/v3io-spark2-object-dataframe_2.11.jar at spark://10.200.0.53:34054/jars/v3io-spark2-object-dataframe_2.11.jar with timestamp 1592405284609\n",
      "20/06/17 14:48:04 INFO spark.SparkContext: Added JAR file:///spark/v3io-libs/v3io-spark2-streaming_2.11.jar at spark://10.200.0.53:34054/jars/v3io-spark2-streaming_2.11.jar with timestamp 1592405284609\n",
      "20/06/17 14:48:04 INFO spark.SparkContext: Added JAR file:/spark/examples/jars/spark-examples_2.11-2.4.4.jar at spark://10.200.0.53:34054/jars/spark-examples_2.11-2.4.4.jar with timestamp 1592405284609\n",
      "20/06/17 14:48:04 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "20/06/17 14:48:04 INFO client.TransportClientFactory: Successfully created connection to spark-master/10.194.174.4:7077 after 29 ms (0 ms spent in bootstraps)\n",
      "20/06/17 14:48:04 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20200617144804-0002\n",
      "20/06/17 14:48:04 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20200617144804-0002/0 on worker-20200617131947-10.200.0.51-38368 (10.200.0.51:38368) with 1 core(s)\n",
      "20/06/17 14:48:04 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20200617144804-0002/0 on hostPort 10.200.0.51:38368 with 1 core(s), 2.0 GB RAM\n",
      "20/06/17 14:48:04 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20200617144804-0002/1 on worker-20200617131947-10.200.0.51-38368 (10.200.0.51:38368) with 1 core(s)\n",
      "20/06/17 14:48:04 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20200617144804-0002/1 on hostPort 10.200.0.51:38368 with 1 core(s), 2.0 GB RAM\n",
      "20/06/17 14:48:04 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20200617144804-0002/2 on worker-20200617131947-10.200.0.51-38368 (10.200.0.51:38368) with 1 core(s)\n",
      "20/06/17 14:48:04 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20200617144804-0002/2 on hostPort 10.200.0.51:38368 with 1 core(s), 2.0 GB RAM\n",
      "20/06/17 14:48:04 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20200617144804-0002/3 on worker-20200617131947-10.200.0.51-38368 (10.200.0.51:38368) with 1 core(s)\n",
      "20/06/17 14:48:04 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20200617144804-0002/3 on hostPort 10.200.0.51:38368 with 1 core(s), 2.0 GB RAM\n",
      "20/06/17 14:48:04 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45909.\n",
      "20/06/17 14:48:04 INFO netty.NettyBlockTransferService: Server created on 10.200.0.53:45909\n",
      "20/06/17 14:48:04 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/06/17 14:48:04 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20200617144804-0002/0 is now RUNNING\n",
      "20/06/17 14:48:04 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20200617144804-0002/1 is now RUNNING\n",
      "20/06/17 14:48:04 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20200617144804-0002/2 is now RUNNING\n",
      "20/06/17 14:48:04 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20200617144804-0002/3 is now RUNNING\n",
      "20/06/17 14:48:04 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.200.0.53, 45909, None)\n",
      "20/06/17 14:48:04 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.0.53:45909 with 366.3 MB RAM, BlockManagerId(driver, 10.200.0.53, 45909, None)\n",
      "20/06/17 14:48:04 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.200.0.53, 45909, None)\n",
      "20/06/17 14:48:04 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.200.0.53, 45909, None)\n",
      "20/06/17 14:48:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@67440de6{/metrics/json,null,AVAILABLE,@Spark}\n",
      "20/06/17 14:48:05 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "20/06/17 14:48:05 INFO spark.SparkContext: Starting job: reduce at SparkPi.scala:38\n",
      "20/06/17 14:48:05 INFO scheduler.DAGScheduler: Got job 0 (reduce at SparkPi.scala:38) with 10 output partitions\n",
      "20/06/17 14:48:05 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:38)\n",
      "20/06/17 14:48:05 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "20/06/17 14:48:05 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "20/06/17 14:48:05 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents\n",
      "20/06/17 14:48:05 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1936.0 B, free 366.3 MB)\n",
      "20/06/17 14:48:05 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1256.0 B, free 366.3 MB)\n",
      "20/06/17 14:48:05 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.200.0.53:45909 (size: 1256.0 B, free: 366.3 MB)\n",
      "20/06/17 14:48:05 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161\n",
      "20/06/17 14:48:05 INFO scheduler.DAGScheduler: Submitting 10 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "20/06/17 14:48:05 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 10 tasks\n",
      "20/06/17 14:48:07 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.0.51:46482) with ID 1\n",
      "20/06/17 14:48:07 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.200.0.51, executor 1, partition 0, PROCESS_LOCAL, 7870 bytes)\n",
      "20/06/17 14:48:07 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.0.51:46490) with ID 0\n",
      "20/06/17 14:48:07 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 10.200.0.51, executor 0, partition 1, PROCESS_LOCAL, 7870 bytes)\n",
      "20/06/17 14:48:07 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.0.51:44195 with 912.3 MB RAM, BlockManagerId(1, 10.200.0.51, 44195, None)\n",
      "20/06/17 14:48:07 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.0.51:46498) with ID 2\n",
      "20/06/17 14:48:07 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, 10.200.0.51, executor 2, partition 2, PROCESS_LOCAL, 7870 bytes)\n",
      "20/06/17 14:48:07 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.0.51:46504) with ID 3\n",
      "20/06/17 14:48:07 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, 10.200.0.51, executor 3, partition 3, PROCESS_LOCAL, 7870 bytes)\n",
      "20/06/17 14:48:07 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.0.51:45765 with 912.3 MB RAM, BlockManagerId(0, 10.200.0.51, 45765, None)\n",
      "20/06/17 14:48:07 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.0.51:38749 with 912.3 MB RAM, BlockManagerId(2, 10.200.0.51, 38749, None)\n",
      "20/06/17 14:48:07 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.0.51:37838 with 912.3 MB RAM, BlockManagerId(3, 10.200.0.51, 37838, None)\n",
      "20/06/17 14:48:07 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.200.0.51:44195 (size: 1256.0 B, free: 912.3 MB)\n",
      "20/06/17 14:48:08 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.200.0.51:45765 (size: 1256.0 B, free: 912.3 MB)\n",
      "20/06/17 14:48:08 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.200.0.51:37838 (size: 1256.0 B, free: 912.3 MB)\n",
      "20/06/17 14:48:08 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.200.0.51:38749 (size: 1256.0 B, free: 912.3 MB)\n",
      "20/06/17 14:48:08 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, 10.200.0.51, executor 1, partition 4, PROCESS_LOCAL, 7870 bytes)\n",
      "20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1092 ms on 10.200.0.51 (executor 1) (1/10)\n",
      "20/06/17 14:48:08 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, 10.200.0.51, executor 0, partition 5, PROCESS_LOCAL, 7870 bytes)\n",
      "20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1028 ms on 10.200.0.51 (executor 0) (2/10)\n",
      "20/06/17 14:48:08 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, 10.200.0.51, executor 1, partition 6, PROCESS_LOCAL, 7870 bytes)\n",
      "20/06/17 14:48:08 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, 10.200.0.51, executor 0, partition 7, PROCESS_LOCAL, 7870 bytes)\n",
      "20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 57 ms on 10.200.0.51 (executor 0) (3/10)\n",
      "20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 76 ms on 10.200.0.51 (executor 1) (4/10)\n",
      "20/06/17 14:48:08 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, 10.200.0.51, executor 3, partition 8, PROCESS_LOCAL, 7870 bytes)\n",
      "20/06/17 14:48:08 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, 10.200.0.51, executor 0, partition 9, PROCESS_LOCAL, 7870 bytes)\n",
      "20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 938 ms on 10.200.0.51 (executor 3) (5/10)\n",
      "20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 32 ms on 10.200.0.51 (executor 0) (6/10)\n",
      "20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 46 ms on 10.200.0.51 (executor 1) (7/10)\n",
      "20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 21 ms on 10.200.0.51 (executor 0) (8/10)\n",
      "20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 973 ms on 10.200.0.51 (executor 2) (9/10)\n",
      "20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 30 ms on 10.200.0.51 (executor 3) (10/10)\n",
      "20/06/17 14:48:08 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "20/06/17 14:48:08 INFO scheduler.DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 2.688 s\n",
      "20/06/17 14:48:08 INFO scheduler.DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 2.754248 s\n",
      "Pi is roughly 3.1406551406551406\n",
      "20/06/17 14:48:08 INFO server.AbstractConnector: Stopped Spark@7e97551f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "20/06/17 14:48:08 INFO ui.SparkUI: Stopped Spark web UI at http://10.200.0.53:4040\n",
      "20/06/17 14:48:08 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors\n",
      "20/06/17 14:48:08 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
      "20/06/17 14:48:08 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "20/06/17 14:48:08 INFO memory.MemoryStore: MemoryStore cleared\n",
      "20/06/17 14:48:08 INFO storage.BlockManager: BlockManager stopped\n",
      "20/06/17 14:48:08 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "20/06/17 14:48:08 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "20/06/17 14:48:08 INFO spark.SparkContext: Successfully stopped SparkContext\n",
      "20/06/17 14:48:08 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "20/06/17 14:48:08 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f4b281e0-b737-4f49-bb84-45cc1eeffcbc\n",
      "20/06/17 14:48:08 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-1f552705-8218-4bf4-9f8b-90afe47c4638\n",
      "\n",
      "\n",
      "[mlrun] 2020-06-17 14:48:08,752 run executed, status=completed\n",
      "final state: succeeded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style> \n",
       ".dictlist {\n",
       "  background-color: #b3edff; \n",
       "  text-align: center; \n",
       "  margin: 4px; \n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer; \n",
       "  background-color: #ffe6cc; \n",
       "  text-align: left; \n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #ffe6cc;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "  \n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "  \n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }  \n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "  \n",
       "  \n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>submit-proj</td>\n",
       "      <td><div title=\"27391b5c221e4409baf4ae896169768b\"><a href=\"https://mlrun-ui.default-tenant.app.app-lab-dev.iguazio-cd0.com/projects/submit-proj/jobs/27391b5c221e4409baf4ae896169768b/info\" target=\"_blank\" >...6169768b</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Jun 17 14:48:02</td>\n",
       "      <td>completed</td>\n",
       "      <td>spark-submit</td>\n",
       "      <td><div class=\"dictlist\">host=spark-submit-wrc48</div><div class=\"dictlist\">kind=job</div><div class=\"dictlist\">owner=admin</div><div class=\"dictlist\">v3io_user=admin</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">class_name=org.apache.spark.examples.SparkPi</div><div class=\"dictlist\">spark_options=/spark/examples/jars/spark-examples_2.11-2.4.4.jar 10</div></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result3a99abc3-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result3a99abc3-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result3a99abc3\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result3a99abc3-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to track results use .show() or .logs() or in CLI: \n",
      "!mlrun get run 27391b5c221e4409baf4ae896169768b --project submit-proj , !mlrun logs 27391b5c221e4409baf4ae896169768b --project submit-proj\n",
      "[mlrun] 2020-06-17 14:48:10,433 run executed, status=completed\n"
     ]
    }
   ],
   "source": [
    "execute_run = fn.run(execute_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
